{"id": "2506.05352", "pdf": "https://arxiv.org/pdf/2506.05352", "abs": "https://arxiv.org/abs/2506.05352", "authors": ["John Beverley", "Regina Hurley"], "title": "A Path to Loving", "categories": ["cs.AI"], "comment": null, "summary": "This work lays the foundations for a rigorous ontological characterization of\nlove, addressing its philosophical complexity and scientific relevance, with\nparticular emphasis on psychology and sociology, as well as highlighting ways\nin which such characterization enhances relevant AI based applications. The\nposition defended here is that love is best understood as a concatenation of\npassive sensations (e.g., emotional arousal) and active evaluative judgments\n(e.g., perceiving the beloved as valuable), in the interest of balancing the\ninvoluntary aspects of love with its rational accountability. To provide a\nstructured foundation, the paper draws on Basic Formal Ontology (BFO) and other\napplied ontological methods to differentiate various senses of love. This work\nengages with objections to the understanding of love as concatenation,\nparticularly concerning the relationship between sensation and judgment. A\ncausal correlation model is defended, ensuring that the affective and cognitive\ncomponents are linked. By offering a precise and scalable ontological account,\nthis work lays the foundation for future interdisciplinary applications, making\nlove a subject of formal inquiry in ontology engineering, artificial\nintelligence, and the sciences.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4e3a\u7231\u7684\u672c\u4f53\u8bba\u7279\u5f81\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u57fa\u7840\uff0c\u7ed3\u5408\u54f2\u5b66\u548c\u79d1\u5b66\u89c6\u89d2\uff0c\u5f3a\u8c03\u5fc3\u7406\u5b66\u548c\u793e\u4f1a\u5b66\uff0c\u5e76\u63a2\u8ba8\u5176\u5bf9AI\u5e94\u7528\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "motivation": "\u7231\u7684\u590d\u6742\u6027\u548c\u79d1\u5b66\u76f8\u5173\u6027\u9700\u8981\u4e00\u79cd\u4e25\u8c28\u7684\u672c\u4f53\u8bba\u63cf\u8ff0\uff0c\u4ee5\u5e73\u8861\u5176\u975e\u7406\u6027\u4e0e\u7406\u6027\u7279\u5f81\u3002", "method": "\u91c7\u7528\u57fa\u672c\u5f62\u5f0f\u672c\u4f53\u8bba\uff08BFO\uff09\u548c\u5176\u4ed6\u5e94\u7528\u672c\u4f53\u65b9\u6cd5\uff0c\u533a\u5206\u7231\u7684\u4e0d\u540c\u542b\u4e49\uff0c\u5e76\u5efa\u7acb\u56e0\u679c\u5173\u8054\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u7231\u662f\u88ab\u52a8\u611f\u89c9\u4e0e\u4e3b\u52a8\u8bc4\u4ef7\u5224\u65ad\u7684\u4e32\u8054\uff0c\u4e3a\u8de8\u5b66\u79d1\u5e94\u7528\uff08\u5982AI\uff09\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u672c\u4f53\u8bba\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u672c\u4f53\u8bba\u65b9\u6cd5\uff0c\u7231\u6210\u4e3a\u5f62\u5f0f\u5316\u7814\u7a76\u7684\u5bf9\u8c61\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u6280\u672f\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.05370", "pdf": "https://arxiv.org/pdf/2506.05370", "abs": "https://arxiv.org/abs/2506.05370", "authors": ["Kristy Wedel"], "title": "Contextual Memory Intelligence -- A Foundational Paradigm for Human-AI Collaboration and Reflective Generative AI Systems", "categories": ["cs.AI", "cs.ET"], "comment": "32 pages, 9 tables, 1 figure", "summary": "A critical challenge remains unresolved as generative AI systems are quickly\nimplemented in various organizational settings. Despite significant advances in\nmemory components such as RAG, vector stores, and LLM agents, these systems\nstill have substantial memory limitations. Gen AI workflows rarely store or\nreflect on the full context in which decisions are made. This leads to repeated\nerrors and a general lack of clarity. This paper introduces Contextual Memory\nIntelligence (CMI) as a new foundational paradigm for building intelligent\nsystems. It repositions memory as an adaptive infrastructure necessary for\nlongitudinal coherence, explainability, and responsible decision-making rather\nthan passive data. Drawing on cognitive science, organizational theory,\nhuman-computer interaction, and AI governance, CMI formalizes the structured\ncapture, inference, and regeneration of context as a fundamental system\ncapability. The Insight Layer is presented in this paper to operationalize this\nvision. This modular architecture uses human-in-the-loop reflection, drift\ndetection, and rationale preservation to incorporate contextual memory into\nsystems. The paper argues that CMI allows systems to reason with data, history,\njudgment, and changing context, thereby addressing a foundational blind spot in\ncurrent AI architectures and governance efforts. A framework for creating\nintelligent systems that are effective, reflective, auditable, and socially\nresponsible is presented through CMI. This enhances human-AI collaboration,\ngenerative AI design, and the resilience of the institutions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u7840\u8303\u5f0f\u2014\u2014\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u667a\u80fd\uff08CMI\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u751f\u6210\u5f0fAI\u7cfb\u7edf\u4e2d\u7684\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6355\u6349\u3001\u63a8\u7406\u548c\u518d\u751f\u4e0a\u4e0b\u6587\u6765\u63d0\u5347\u7cfb\u7edf\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0fAI\u7cfb\u7edf\u5728\u7ec4\u7ec7\u73af\u5883\u4e2d\u5feb\u901f\u90e8\u7f72\uff0c\u4f46\u5176\u8bb0\u5fc6\u7ec4\u4ef6\uff08\u5982RAG\u3001\u5411\u91cf\u5b58\u50a8\u7b49\uff09\u4ecd\u5b58\u5728\u663e\u8457\u9650\u5236\uff0c\u5bfc\u81f4\u91cd\u590d\u9519\u8bef\u548c\u51b3\u7b56\u4e0d\u900f\u660e\u3002", "method": "CMI\u5c06\u8bb0\u5fc6\u91cd\u65b0\u5b9a\u4f4d\u4e3a\u81ea\u9002\u5e94\u57fa\u7840\u8bbe\u65bd\uff0c\u7ed3\u5408\u8ba4\u77e5\u79d1\u5b66\u3001\u7ec4\u7ec7\u7406\u8bba\u7b49\uff0c\u63d0\u51faInsight Layer\u67b6\u6784\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u3001\u6f02\u79fb\u68c0\u6d4b\u7b49\u65b9\u6cd5\u5b9e\u73b0\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u3002", "result": "CMI\u80fd\u591f\u5e2e\u52a9\u7cfb\u7edf\u57fa\u4e8e\u6570\u636e\u3001\u5386\u53f2\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u8fdb\u884c\u63a8\u7406\uff0c\u586b\u8865\u5f53\u524dAI\u67b6\u6784\u548c\u6cbb\u7406\u7684\u76f2\u70b9\u3002", "conclusion": "CMI\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u53ef\u5ba1\u8ba1\u4e14\u793e\u4f1a\u8d23\u4efb\u7684\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u4eba\u673a\u534f\u4f5c\u548c\u751f\u6210\u5f0fAI\u8bbe\u8ba1\u7684\u97e7\u6027\u3002"}}
{"id": "2506.05422", "pdf": "https://arxiv.org/pdf/2506.05422", "abs": "https://arxiv.org/abs/2506.05422", "authors": ["Andrei T. Patrascu"], "title": "Constructive Symbolic Reinforcement Learning via Intuitionistic Logic and Goal-Chaining Inference", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "We introduce a novel learning and planning framework that replaces\ntraditional reward-based optimisation with constructive logical inference. In\nour model, actions, transitions, and goals are represented as logical\npropositions, and decision-making proceeds by building constructive proofs\nunder intuitionistic logic. This method ensures that state transitions and\npolicies are accepted only when supported by verifiable preconditions --\neschewing probabilistic trial-and-error in favour of guaranteed logical\nvalidity. We implement a symbolic agent operating in a structured gridworld,\nwhere reaching a goal requires satisfying a chain of intermediate subgoals\n(e.g., collecting keys to open doors), each governed by logical constraints.\nUnlike conventional reinforcement learning agents, which require extensive\nexploration and suffer from unsafe or invalid transitions, our constructive\nagent builds a provably correct plan through goal chaining, condition tracking,\nand knowledge accumulation. Empirical comparison with Q-learning demonstrates\nthat our method achieves perfect safety, interpretable behaviour, and efficient\nconvergence with no invalid actions, highlighting its potential for safe\nplanning, symbolic cognition, and trustworthy AI. This work presents a new\ndirection for reinforcement learning grounded not in numeric optimisation, but\nin constructive logic and proof theory.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6784\u9020\u6027\u903b\u8f91\u63a8\u7406\u7684\u5b66\u4e60\u4e0e\u89c4\u5212\u6846\u67b6\uff0c\u53d6\u4ee3\u4f20\u7edf\u7684\u57fa\u4e8e\u5956\u52b1\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u786e\u4fdd\u51b3\u7b56\u5177\u6709\u903b\u8f91\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u6982\u7387\u8bd5\u9519\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5b89\u5168\u6216\u65e0\u6548\u7684\u8fc7\u6e21\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6784\u9020\u6027\u903b\u8f91\u63a8\u7406\u5b9e\u73b0\u5b89\u5168\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u89c4\u5212\u3002", "method": "\u5c06\u52a8\u4f5c\u3001\u8fc7\u6e21\u548c\u76ee\u6807\u8868\u793a\u4e3a\u903b\u8f91\u547d\u9898\uff0c\u901a\u8fc7\u6784\u9020\u6027\u8bc1\u660e\u8fdb\u884c\u51b3\u7b56\uff0c\u786e\u4fdd\u6bcf\u4e00\u6b65\u90fd\u6709\u53ef\u9a8c\u8bc1\u7684\u524d\u63d0\u6761\u4ef6\u3002", "result": "\u5728\u7ed3\u6784\u5316\u7f51\u683c\u4e16\u754c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86100%\u7684\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u884c\u4e3a\u548c\u65e0\u65e0\u6548\u52a8\u4f5c\u7684\u9ad8\u6548\u6536\u655b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u57fa\u4e8e\u6784\u9020\u6027\u903b\u8f91\u548c\u8bc1\u660e\u7406\u8bba\u7684\u65b0\u65b9\u5411\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u89c4\u5212\u548c\u53ef\u4fe1AI\u3002"}}
{"id": "2506.05520", "pdf": "https://arxiv.org/pdf/2506.05520", "abs": "https://arxiv.org/abs/2506.05520", "authors": ["Cecil Pang"], "title": "Towards Data Systems That Are Business Semantic-Centric and AI Agents-Assisted", "categories": ["cs.AI", "cs.MA"], "comment": "Being peer reviewed by a journal", "summary": "Contemporary businesses operate in dynamic environments requiring rapid\nadaptation to achieve goals and maintain competitiveness. Existing data\nplatforms often fall short by emphasizing tools over alignment with business\nneeds, resulting in inefficiencies and delays. To address this gap, I propose\nthe Business Semantics Centric, AI Agents Assisted Data System (BSDS), a\nholistic system that integrates architecture, workflows, and team organization\nto ensure data systems are tailored to business priorities rather than dictated\nby technical constraints. BSDS redefines data systems as dynamic enablers of\nbusiness success, transforming them from passive tools into active drivers of\norganizational growth. BSDS has a modular architecture that comprises curated\ndata linked to business entities, a knowledge base for context-aware AI agents,\nand efficient data pipelines. AI agents play a pivotal role in assisting with\ndata access and system management, reducing human effort, and improving\nscalability. Complementing this architecture, BSDS incorporates workflows\noptimized for both exploratory data analysis and production requirements,\nbalancing speed of delivery with quality assurance. A key innovation of BSDS is\nits incorporation of the human factor. By aligning data team expertise with\nbusiness semantics, BSDS bridges the gap between technical capabilities and\nbusiness needs. Validated through real-world implementation, BSDS accelerates\ntime-to-market for data-driven initiatives, enhances cross-functional\ncollaboration, and provides a scalable blueprint for businesses of all sizes.\nFuture research can build on BSDS to explore optimization strategies using\ncomplex systems and adaptive network theories, as well as developing autonomous\ndata systems leveraging AI agents.", "AI": {"tldr": "BSDS\u662f\u4e00\u79cd\u4ee5\u4e1a\u52a1\u8bed\u4e49\u4e3a\u4e2d\u5fc3\u3001AI\u4ee3\u7406\u8f85\u52a9\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u6574\u5408\u67b6\u6784\u3001\u5de5\u4f5c\u6d41\u548c\u56e2\u961f\u7ec4\u7ec7\uff0c\u4f7f\u6570\u636e\u7cfb\u7edf\u66f4\u8d34\u5408\u4e1a\u52a1\u9700\u6c42\uff0c\u800c\u975e\u53d7\u9650\u4e8e\u6280\u672f\u7ea6\u675f\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5e73\u53f0\u8fc7\u4e8e\u5173\u6ce8\u5de5\u5177\u800c\u975e\u4e1a\u52a1\u9700\u6c42\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u5ef6\u8fdf\u3002BSDS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5c06\u6570\u636e\u7cfb\u7edf\u8f6c\u53d8\u4e3a\u4e1a\u52a1\u6210\u529f\u7684\u52a8\u6001\u63a8\u52a8\u8005\u3002", "method": "BSDS\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5305\u62ec\u4e0e\u4e1a\u52a1\u5b9e\u4f53\u5173\u8054\u7684\u7cbe\u9009\u6570\u636e\u3001\u652f\u6301\u4e0a\u4e0b\u6587\u611f\u77e5AI\u4ee3\u7406\u7684\u77e5\u8bc6\u5e93\u548c\u9ad8\u6548\u6570\u636e\u7ba1\u9053\u3002AI\u4ee3\u7406\u5728\u6570\u636e\u8bbf\u95ee\u548c\u7cfb\u7edf\u7ba1\u7406\u4e2d\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002", "result": "\u5b9e\u9645\u9a8c\u8bc1\u8868\u660e\uff0cBSDS\u52a0\u901f\u4e86\u6570\u636e\u9a71\u52a8\u9879\u76ee\u7684\u4e0a\u5e02\u65f6\u95f4\uff0c\u589e\u5f3a\u4e86\u8de8\u804c\u80fd\u534f\u4f5c\uff0c\u5e76\u4e3a\u5404\u79cd\u89c4\u6a21\u7684\u4f01\u4e1a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u84dd\u56fe\u3002", "conclusion": "BSDS\u901a\u8fc7\u6574\u5408\u4e1a\u52a1\u8bed\u4e49\u548c\u6280\u672f\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u7cfb\u7edf\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u590d\u6742\u7cfb\u7edf\u548c\u81ea\u9002\u5e94\u7f51\u7edc\u7406\u8bba\u7684\u4f18\u5316\u7b56\u7565\u3002"}}
{"id": "2506.05364", "pdf": "https://arxiv.org/pdf/2506.05364", "abs": "https://arxiv.org/abs/2506.05364", "authors": ["Anjana Sarkar", "Soumyendu Sarkar"], "title": "Survey of LLM Agent Communication with MCP: A Software Design Pattern Centric Review", "categories": ["cs.SE"], "comment": null, "summary": "This survey investigates how classical software design patterns can enhance\nthe reliability and scalability of communication in Large Language Model\n(LLM)-driven agentic AI systems, focusing particularly on the Model Context\nProtocol (MCP). It examines the foundational architectures of LLM-based agents\nand their evolution from isolated operation to sophisticated, multi-agent\ncollaboration, addressing key communication hurdles that arise in this\ntransition. The study revisits well-established patterns, including Mediator,\nObserver, Publish-Subscribe, and Broker, and analyzes their relevance in\nstructuring agent interactions within MCP-compliant frameworks. To clarify\nthese dynamics, the article provides conceptual schematics and formal models\nthat map out communication pathways and optimize data flow. It further explores\narchitectural variations suited to different degrees of agent autonomy and\nsystem complexity. Real-world applications in domains such as real-time\nfinancial processing and investment banking are discussed, illustrating how\nthese patterns and MCP can meet specific operational demands. The article\nconcludes by outlining open challenges, potential security risks, and promising\ndirections for advancing robust, interoperable, and scalable multi-agent LLM\necosystems.", "AI": {"tldr": "\u8be5\u8c03\u67e5\u63a2\u8ba8\u4e86\u7ecf\u5178\u8f6f\u4ef6\u8bbe\u8ba1\u6a21\u5f0f\u5982\u4f55\u63d0\u5347LLM\u9a71\u52a8\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u901a\u4fe1\u53ef\u9760\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3LLM\u667a\u80fd\u4f53\u4ece\u5b64\u7acb\u8fd0\u884c\u5230\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u901a\u4fe1\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5206\u6790\u7ecf\u5178\u8bbe\u8ba1\u6a21\u5f0f\uff08\u5982\u4e2d\u4ecb\u8005\u3001\u89c2\u5bdf\u8005\u3001\u53d1\u5e03-\u8ba2\u9605\u548c\u4ee3\u7406\u6a21\u5f0f\uff09\u5728MCP\u6846\u67b6\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u63d0\u4f9b\u6982\u5ff5\u56fe\u548c\u5f62\u5f0f\u5316\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u8868\u660e\u8fd9\u4e9b\u6a21\u5f0f\u80fd\u6709\u6548\u4f18\u5316\u901a\u4fe1\u8def\u5f84\u548c\u6570\u636e\u6d41\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u81ea\u4e3b\u5ea6\u548c\u590d\u6742\u5ea6\u7684\u7cfb\u7edf\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u672a\u6765\u9700\u89e3\u51b3\u5f00\u653e\u6311\u6218\u548c\u5b89\u5168\u98ce\u9669\uff0c\u4ee5\u63a8\u52a8\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53LLM\u751f\u6001\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2506.05529", "pdf": "https://arxiv.org/pdf/2506.05529", "abs": "https://arxiv.org/abs/2506.05529", "authors": ["Rodney Sanchez", "Ferat Sahin", "Alexander Ororbia", "Jamison Heard"], "title": "Avoiding Death through Fear Intrinsic Conditioning", "categories": ["cs.AI", "cs.LG", "I.2.0; I.2.8"], "comment": null, "summary": "Biological and psychological concepts have inspired reinforcement learning\nalgorithms to create new complex behaviors that expand agents' capacity. These\nbehaviors can be seen in the rise of techniques like goal decomposition,\ncurriculum, and intrinsic rewards, which have paved the way for these complex\nbehaviors. One limitation in evaluating these methods is the requirement for\nengineered extrinsic for realistic environments. A central challenge in\nengineering the necessary reward function(s) comes from these environments\ncontaining states that carry high negative rewards, but provide no feedback to\nthe agent. Death is one such stimuli that fails to provide direct feedback to\nthe agent. In this work, we introduce an intrinsic reward function inspired by\nearly amygdala development and produce this intrinsic reward through a novel\nmemory-augmented neural network (MANN) architecture. We show how this intrinsic\nmotivation serves to deter exploration of terminal states and results in\navoidance behavior similar to fear conditioning observed in animals.\nFurthermore, we demonstrate how modifying a threshold where the fear response\nis active produces a range of behaviors that are described under the paradigm\nof general anxiety disorders (GADs). We demonstrate this behavior in the\nMiniworld Sidewalk environment, which provides a partially observable Markov\ndecision process (POMDP) and a sparse reward with a non-descriptive terminal\ncondition, i.e., death. In effect, this study results in a\nbiologically-inspired neural architecture and framework for fear conditioning\nparadigms; we empirically demonstrate avoidance behavior in a constructed agent\nthat is able to solve environments with non-descriptive terminal conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u65e9\u671f\u674f\u4ec1\u6838\u53d1\u80b2\u542f\u53d1\u7684\u5185\u5728\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u65b0\u578b\u8bb0\u5fc6\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\uff08MANN\uff09\u67b6\u6784\u5b9e\u73b0\uff0c\u7528\u4e8e\u907f\u514d\u7ec8\u7aef\u72b6\u6001\u63a2\u7d22\uff0c\u6a21\u62df\u52a8\u7269\u6050\u60e7\u6761\u4ef6\u53cd\u5c04\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9700\u8981\u8bbe\u8ba1\u5916\u5728\u5956\u52b1\u51fd\u6570\uff0c\u4f46\u67d0\u4e9b\u72b6\u6001\uff08\u5982\u6b7b\u4ea1\uff09\u65e0\u6cd5\u63d0\u4f9b\u76f4\u63a5\u53cd\u9988\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u53d7\u65e9\u671f\u674f\u4ec1\u6838\u53d1\u80b2\u542f\u53d1\u7684\u5185\u5728\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408MANN\u67b6\u6784\uff0c\u8c03\u6574\u6050\u60e7\u54cd\u5e94\u9608\u503c\u4ee5\u6a21\u62df\u5e7f\u6cdb\u6027\u7126\u8651\u969c\u788d\uff08GADs\uff09\u884c\u4e3a\u3002", "result": "\u5728Miniworld Sidewalk\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u975e\u63cf\u8ff0\u6027\u7ec8\u7aef\u6761\u4ef6\uff08\u5982\u6b7b\u4ea1\uff09\u7684\u56de\u907f\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u7269\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u6050\u60e7\u6761\u4ef6\u53cd\u5c04\u6846\u67b6\uff0c\u4e3a\u5904\u7406\u975e\u63cf\u8ff0\u6027\u7ec8\u7aef\u6761\u4ef6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.05451", "pdf": "https://arxiv.org/pdf/2506.05451", "abs": "https://arxiv.org/abs/2506.05451", "authors": ["Seongmin Lee", "Aeree Cho", "Grace C. Kim", "ShengYun Peng", "Mansi Phute", "Duen Horng Chau"], "title": "Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "31 pages, 1 figure", "summary": "As large language models (LLMs) see wider real-world use, understanding and\nmitigating their unsafe behaviors is critical. Interpretation techniques can\nreveal causes of unsafe outputs and guide safety, but such connections with\nsafety are often overlooked in prior surveys. We present the first survey that\nbridges this gap, introducing a unified framework that connects safety-focused\ninterpretation methods, the safety enhancements they inform, and the tools that\noperationalize them. Our novel taxonomy, organized by LLM workflow stages,\nsummarizes nearly 70 works at their intersections. We conclude with open\nchallenges and future directions. This timely survey helps researchers and\npractitioners navigate key advancements for safer, more interpretable LLMs.", "AI": {"tldr": "\u672c\u6587\u662f\u7b2c\u4e00\u4efd\u8fde\u63a5\u5b89\u5168\u6027\u89e3\u91ca\u65b9\u6cd5\u4e0e\u5b9e\u9645\u5b89\u5168\u589e\u5f3a\u7684\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u603b\u7ed3\u4e86\u8fd170\u9879\u76f8\u5173\u7814\u7a76\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3\u548c\u51cf\u5c11\u5176\u4e0d\u5b89\u5168\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u5ffd\u7565\u4e86\u89e3\u91ca\u6280\u672f\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u5b89\u5168\u6027\u89e3\u91ca\u65b9\u6cd5\u3001\u5b89\u5168\u589e\u5f3a\u63aa\u65bd\u53ca\u64cd\u4f5c\u5de5\u5177\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u6309LLM\u5de5\u4f5c\u6d41\u7a0b\u9636\u6bb5\u5206\u7c7b\u603b\u7ed3\u4e86\u8fd170\u9879\u7814\u7a76\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u7c7b\u6cd5\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u66f4\u597d\u5730\u7406\u89e3\u5982\u4f55\u901a\u8fc7\u89e3\u91ca\u6280\u672f\u63d0\u5347LLMs\u7684\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u548c\u5f00\u653e\u6311\u6218\uff0c\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u89e3\u91ca\u7684LLMs\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2506.05579", "pdf": "https://arxiv.org/pdf/2506.05579", "abs": "https://arxiv.org/abs/2506.05579", "authors": ["Quan Shi", "Carlos E. Jimenez", "Shunyu Yao", "Nick Haber", "Diyi Yang", "Karthik Narasimhan"], "title": "When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "For code, data, visualizer, visit: https:kite-live.vercel.app", "summary": "Recent advancements in AI reasoning have driven substantial improvements\nacross diverse tasks. A critical open question is whether these improvements\nalso yields better knowledge transfer: the ability of models to communicate\nreasoning in ways humans can understand, apply, and learn from. To investigate\nthis, we introduce Knowledge Integration and Transfer Evaluation (KITE), a\nconceptual and experimental framework for Human-AI knowledge transfer\ncapabilities and conduct the first large-scale human study (N=118) explicitly\ndesigned to measure it. In our two-phase setup, humans first ideate with an AI\non problem-solving strategies, then independently implement solutions,\nisolating model explanations' influence on human understanding. Our findings\nreveal that although model benchmark performance correlates with collaborative\noutcomes, this relationship is notably inconsistent, featuring significant\noutliers, indicating that knowledge transfer requires dedicated optimization.\nOur analysis identifies behavioral and strategic factors mediating successful\nknowledge transfer. We release our code, dataset, and evaluation framework to\nsupport future work on communicatively aligned models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AI\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u662f\u5426\u6539\u5584\u4e86\u77e5\u8bc6\u8f6c\u79fb\uff08\u4eba\u7c7b\u7406\u89e3\u4e0e\u5e94\u7528AI\u63a8\u7406\u7684\u80fd\u529b\uff09\uff0c\u5e76\u63d0\u51fa\u4e86KITE\u6846\u67b6\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u6027\u80fd\u4e0e\u534f\u4f5c\u6548\u679c\u76f8\u5173\u4f46\u4e0d\u4e00\u81f4\uff0c\u77e5\u8bc6\u8f6c\u79fb\u9700\u8981\u4e13\u95e8\u4f18\u5316\u3002", "motivation": "\u63a2\u8ba8AI\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u662f\u5426\u4fc3\u8fdb\u4e86\u4eba\u7c7b\u5bf9AI\u63a8\u7406\u7684\u7406\u89e3\u4e0e\u5e94\u7528\uff0c\u5373\u77e5\u8bc6\u8f6c\u79fb\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faKITE\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5b9e\u9a8c\uff08\u4eba\u7c7b\u4e0eAI\u5171\u540c\u6784\u601d\u7b56\u7565\u540e\u72ec\u7acb\u5b9e\u65bd\u89e3\u51b3\u65b9\u6848\uff09\u8bc4\u4f30\u77e5\u8bc6\u8f6c\u79fb\u80fd\u529b\u3002", "result": "\u6a21\u578b\u6027\u80fd\u4e0e\u534f\u4f5c\u6548\u679c\u76f8\u5173\u4f46\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u77e5\u8bc6\u8f6c\u79fb\u9700\u8981\u4e13\u95e8\u4f18\u5316\u3002", "conclusion": "\u77e5\u8bc6\u8f6c\u79fb\u7684\u6210\u529f\u9700\u8981\u9488\u5bf9\u6027\u7684\u4f18\u5316\uff0c\u5e76\u53d7\u884c\u4e3a\u548c\u7b56\u7565\u56e0\u7d20\u5f71\u54cd\u3002"}}
{"id": "2506.05614", "pdf": "https://arxiv.org/pdf/2506.05614", "abs": "https://arxiv.org/abs/2506.05614", "authors": ["E. G. Santana Jr", "Gabriel Benjamin", "Melissa Araujo", "Harrison Santos", "David Freitas", "Eduardo Almeida", "Paulo Anselmo da M. S. Neto", "Jiawei Li", "Jina Chun", "Iftekhar Ahmed"], "title": "Which Prompting Technique Should I Use? An Empirical Investigation of Prompting Techniques for Software Engineering Tasks", "categories": ["cs.SE"], "comment": null, "summary": "A growing variety of prompt engineering techniques has been proposed for\nLarge Language Models (LLMs), yet systematic evaluation of each technique on\nindividual software engineering (SE) tasks remains underexplored. In this\nstudy, we present a systematic evaluation of 14 established prompt techniques\nacross 10 SE tasks using four LLM models. As identified in the prior\nliterature, the selected prompting techniques span six core dimensions\n(Zero-Shot, Few-Shot, Thought Generation, Ensembling, Self-Criticism, and\nDecomposition). They are evaluated on tasks such as code generation, bug\nfixing, and code-oriented question answering, to name a few. Our results show\nwhich prompting techniques are most effective for SE tasks requiring complex\nlogic and intensive reasoning versus those that rely more on contextual\nunderstanding and example-driven scenarios. We also analyze correlations\nbetween the linguistic characteristics of prompts and the factors that\ncontribute to the effectiveness of prompting techniques in enhancing\nperformance on SE tasks. Additionally, we report the time and token consumption\nfor each prompting technique when applied to a specific task and model,\noffering guidance for practitioners in selecting the optimal prompting\ntechnique for their use cases.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e8614\u79cd\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u572810\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u6280\u672f\u5bf9\u903b\u8f91\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u4efb\u52a1\u7684\u6548\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u65f6\u95f4\u548c\u8d44\u6e90\u6d88\u8017\u7684\u6307\u5bfc\u3002", "motivation": "\u5c3d\u7ba1\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5bf9\u5176\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u4ecd\u4e0d\u8db3\u3002", "method": "\u4f7f\u75284\u79cdLLM\u6a21\u578b\uff0c\u8bc4\u4f3014\u79cd\u63d0\u793a\u6280\u672f\uff08\u6db5\u76d66\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff09\u572810\u4e2aSE\u4efb\u52a1\uff08\u5982\u4ee3\u7801\u751f\u6210\u3001\u9519\u8bef\u4fee\u590d\u7b49\uff09\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u63d0\u793a\u6280\u672f\u5bf9\u903b\u8f91\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u4efb\u52a1\u7684\u6548\u679c\u5dee\u5f02\u663e\u8457\uff0c\u5e76\u5206\u6790\u4e86\u63d0\u793a\u8bed\u8a00\u7279\u5f81\u4e0e\u6548\u679c\u7684\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u9009\u62e9\u6700\u4f73\u63d0\u793a\u6280\u672f\u7684\u6307\u5bfc\uff0c\u5e76\u91cf\u5316\u4e86\u65f6\u95f4\u548c\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2506.05587", "pdf": "https://arxiv.org/pdf/2506.05587", "abs": "https://arxiv.org/abs/2506.05587", "authors": ["Junjie Xing", "Yeye He", "Mengyu Zhou", "Haoyu Dong", "Shi Han", "Lingjiao Chen", "Dongmei Zhang", "Surajit Chaudhuri", "H. V. Jagadish"], "title": "MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LG"], "comment": null, "summary": "Tables and table-based use cases play a crucial role in many important\nreal-world applications, such as spreadsheets, databases, and computational\nnotebooks, which traditionally require expert-level users like data engineers,\ndata analysts, and database administrators to operate. Although LLMs have shown\nremarkable progress in working with tables (e.g., in spreadsheet and database\ncopilot scenarios), comprehensive benchmarking of such capabilities remains\nlimited. In contrast to an extensive and growing list of NLP benchmarks,\nevaluations of table-related tasks are scarce, and narrowly focus on tasks like\nNL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks\nthat professional users face. This gap limits our understanding and model\nprogress in this important area.\n  In this work, we introduce MMTU, a large-scale benchmark with over 30K\nquestions across 25 real-world table tasks, designed to comprehensively\nevaluate models ability to understand, reason, and manipulate real tables at\nthe expert-level. These tasks are drawn from decades' worth of computer science\nresearch on tabular data, with a focus on complex table tasks faced by\nprofessional users. We show that MMTU require a combination of skills --\nincluding table understanding, reasoning, and coding -- that remain challenging\nfor today's frontier models, where even frontier reasoning models like OpenAI\no4-mini and DeepSeek R1 score only around 60%, suggesting significant room for\nimprovement. We highlight key findings in our evaluation using MMTU and hope\nthat this benchmark drives further advances in understanding and developing\nfoundation models for structured data processing and analysis. Our code and\ndata are available at https://github.com/MMTU-Benchmark/MMTU and\nhttps://huggingface.co/datasets/MMTU-benchmark/MMTU.", "AI": {"tldr": "MMTU\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b25\u79cd\u771f\u5b9e\u4e16\u754c\u8868\u683c\u4efb\u52a1\u768430K\u95ee\u9898\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5728\u4e13\u5bb6\u7ea7\u8868\u683c\u7406\u89e3\u3001\u63a8\u7406\u548c\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5bf9\u8868\u683c\u76f8\u5173\u4efb\u52a1\u7684\u8bc4\u4f30\u6709\u9650\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728NL-to-SQL\u548cTable-QA\u7b49\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u4e13\u4e1a\u7528\u6237\u9762\u4e34\u7684\u66f4\u5e7f\u6cdb\u4efb\u52a1\u3002", "method": "MMTU\u4ece\u51e0\u5341\u5e74\u7684\u8ba1\u7b97\u673a\u79d1\u5b66\u7814\u7a76\u4e2d\u63d0\u53d6\u590d\u6742\u8868\u683c\u4efb\u52a1\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b30K\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u524d\u6cbf\u6a21\u578b\uff08\u5982OpenAI o4-mini\u548cDeepSeek R1\uff09\u5728MMTU\u4e0a\u7684\u8868\u73b0\u4ec5\u7ea660%\uff0c\u8868\u660e\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "MMTU\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u5904\u7406\u548c\u5206\u6790\u7684\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u5e0c\u671b\u63a8\u52a8\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.05623", "pdf": "https://arxiv.org/pdf/2506.05623", "abs": "https://arxiv.org/abs/2506.05623", "authors": ["Tianyi Zhang", "Shidong Pan", "Zejun Zhang", "Zhenchang Xing", "Xiaoyu Sun"], "title": "Deployability-Centric Infrastructure-as-Code Generation: An LLM-based Iterative Framework", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Infrastructure-as-Code (IaC) generation holds significant promise for\nautomating cloud infrastructure provisioning. Recent advances in Large Language\nModels (LLMs) present a promising opportunity to democratize IaC development by\ngenerating deployable infrastructure templates from natural language\ndescriptions, but current evaluation focuses on syntactic correctness while\nignoring deployability, the fatal measure of IaC template utility. We address\nthis gap through two contributions: (1) IaCGen, an LLM-based\ndeployability-centric framework that uses iterative feedback mechanism to\ngenerate IaC templates, and (2) DPIaC-Eval, a deployability-centric IaC\ntemplate benchmark consists of 153 real-world scenarios that can evaluate\nsyntax, deployment, user intent, and security. Our evaluation reveals that\nstate-of-the-art LLMs initially performed poorly, with Claude-3.5 and\nClaude-3.7 achieving only 30.2% and 26.8% deployment success on the first\nattempt respectively. However, IaCGen transforms this performance dramatically:\nall evaluated models reach over 90% passItr@25, with Claude-3.5 and Claude-3.7\nachieving 98% success rate. Despite these improvements, critical challenges\nremain in user intent alignment (25.2% accuracy) and security compliance (8.4%\npass rate), highlighting areas requiring continued research. Our work provides\nthe first comprehensive assessment of deployability-centric IaC template\ngeneration and establishes a foundation for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faIaCGen\u6846\u67b6\u548cDPIaC-Eval\u57fa\u51c6\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u673a\u5236\u63d0\u5347IaC\u6a21\u677f\u7684\u53ef\u90e8\u7f72\u6027\uff0c\u4f46\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u548c\u5b89\u5168\u6027\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u5f53\u524dIaC\u751f\u6210\u8bc4\u4f30\u4ec5\u5173\u6ce8\u8bed\u6cd5\u6b63\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u53ef\u90e8\u7f72\u6027\u8fd9\u4e00\u5173\u952e\u6307\u6807\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faIaCGen\u6846\u67b6\uff08\u57fa\u4e8eLLM\u7684\u8fed\u4ee3\u53cd\u9988\u673a\u5236\uff09\u548cDPIaC-Eval\u57fa\u51c6\uff08153\u4e2a\u771f\u5b9e\u573a\u666f\uff09\u3002", "result": "\u521d\u59cbLLM\u8868\u73b0\u5dee\uff0830%\u90e8\u7f72\u6210\u529f\u7387\uff09\uff0c\u4f46IaCGen\u663e\u8457\u63d0\u5347\u81f390%\u4ee5\u4e0a\uff1b\u7528\u6237\u610f\u56fe\u548c\u5b89\u5168\u6027\u4ecd\u662f\u77ed\u677f\u3002", "conclusion": "\u8bba\u6587\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u53ef\u90e8\u7f72\u6027\u4e3a\u4e2d\u5fc3\u7684IaC\u751f\u6210\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2506.05616", "pdf": "https://arxiv.org/pdf/2506.05616", "abs": "https://arxiv.org/abs/2506.05616", "authors": ["Lianhao Zhou", "Hongyi Ling", "Keqiang Yan", "Kaiji Zhao", "Xiaoning Qian", "Raymundo Arr\u00f3yave", "Xiaofeng Qian", "Shuiwang Ji"], "title": "Toward Greater Autonomy in Materials Discovery Agents: Unifying Planning, Physics, and Scientists", "categories": ["cs.AI", "cond-mat.mtrl-sci", "physics.comp-ph"], "comment": null, "summary": "We aim at designing language agents with greater autonomy for crystal\nmaterials discovery. While most of existing studies restrict the agents to\nperform specific tasks within predefined workflows, we aim to automate workflow\nplanning given high-level goals and scientist intuition. To this end, we\npropose Materials Agent unifying Planning, Physics, and Scientists, known as\nMAPPS. MAPPS consists of a Workflow Planner, a Tool Code Generator, and a\nScientific Mediator. The Workflow Planner uses large language models (LLMs) to\ngenerate structured and multi-step workflows. The Tool Code Generator\nsynthesizes executable Python code for various tasks, including invoking a\nforce field foundation model that encodes physics. The Scientific Mediator\ncoordinates communications, facilitates scientist feedback, and ensures\nrobustness through error reflection and recovery. By unifying planning,\nphysics, and scientists, MAPPS enables flexible and reliable materials\ndiscovery with greater autonomy, achieving a five-fold improvement in\nstability, uniqueness, and novelty rates compared with prior generative models\nwhen evaluated on the MP-20 data. We provide extensive experiments across\ndiverse tasks to show that MAPPS is a promising framework for autonomous\nmaterials discovery.", "AI": {"tldr": "MAPPS\u662f\u4e00\u4e2a\u7528\u4e8e\u6676\u4f53\u6750\u6599\u53d1\u73b0\u7684\u81ea\u4e3b\u8bed\u8a00\u4ee3\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5de5\u4f5c\u6d41\u89c4\u5212\u3001\u7269\u7406\u6a21\u578b\u548c\u79d1\u5b66\u5bb6\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a33\u5b9a\u6027\u548c\u65b0\u9896\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u9650\u5236\u4ee3\u7406\u5728\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\u4e2d\u6267\u884c\u7279\u5b9a\u4efb\u52a1\uff0c\u800cMAPPS\u65e8\u5728\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u89c4\u5212\uff0c\u5b9e\u73b0\u66f4\u9ad8\u5c42\u6b21\u7684\u81ea\u4e3b\u6027\u3002", "method": "MAPPS\u7531\u5de5\u4f5c\u6d41\u89c4\u5212\u5668\u3001\u5de5\u5177\u4ee3\u7801\u751f\u6210\u5668\u548c\u79d1\u5b66\u534f\u8c03\u5668\u7ec4\u6210\uff0c\u5229\u7528LLM\u751f\u6210\u591a\u6b65\u5de5\u4f5c\u6d41\uff0c\u5408\u6210\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u5e76\u534f\u8c03\u79d1\u5b66\u5bb6\u53cd\u9988\u3002", "result": "\u5728MP-20\u6570\u636e\u4e0a\uff0cMAPPS\u5728\u7a33\u5b9a\u6027\u3001\u72ec\u7279\u6027\u548c\u65b0\u9896\u6027\u65b9\u9762\u6bd4\u73b0\u6709\u751f\u6210\u6a21\u578b\u63d0\u5347\u4e86\u4e94\u500d\u3002", "conclusion": "MAPPS\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u81ea\u4e3b\u6750\u6599\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u89c4\u5212\u3001\u7269\u7406\u548c\u79d1\u5b66\u5bb6\u53cd\u9988\u5b9e\u73b0\u4e86\u7075\u6d3b\u53ef\u9760\u7684\u6750\u6599\u63a2\u7d22\u3002"}}
{"id": "2506.05817", "pdf": "https://arxiv.org/pdf/2506.05817", "abs": "https://arxiv.org/abs/2506.05817", "authors": ["Zihan Wang", "Siyao Liu", "Yang Sun", "Hongyan Li", "Kai Shen"], "title": "CodeContests+: High-Quality Test Case Generation for Competitive Programming", "categories": ["cs.SE", "cs.CL"], "comment": "28 pages, 7 figures", "summary": "Competitive programming, due to its high reasoning difficulty and precise\ncorrectness feedback, has become a key task for both training and evaluating\nthe reasoning capabilities of large language models (LLMs). However, while a\nlarge amount of public problem data, such as problem statements and solutions,\nis available, the test cases of these problems are often difficult to obtain.\nTherefore, test case generation is a necessary task for building large-scale\ndatasets, and the quality of the test cases directly determines the accuracy of\nthe evaluation. In this paper, we introduce an LLM-based agent system that\ncreates high-quality test cases for competitive programming problems. We apply\nthis system to the CodeContests dataset and propose a new version with improved\ntest cases, named CodeContests+. We evaluated the quality of test cases in\nCodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels\nto examine the accuracy of these test cases in evaluation. The results\nindicated that CodeContests+ achieves significantly higher accuracy than\nCodeContests, particularly with a notably higher True Positive Rate (TPR).\nSubsequently, our experiments in LLM Reinforcement Learning (RL) further\nconfirmed that improvements in test case quality yield considerable advantages\nfor RL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7f16\u7a0b\u7ade\u8d5b\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u6784\u5efa\u4e86\u6539\u8fdb\u7248\u6570\u636e\u96c6CodeContests+\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u7f16\u7a0b\u7ade\u8d5b\u56e0\u5176\u9ad8\u96be\u5ea6\u548c\u7cbe\u786e\u53cd\u9988\u6210\u4e3a\u8bc4\u4f30LLM\u63a8\u7406\u80fd\u529b\u7684\u5173\u952e\u4efb\u52a1\uff0c\u4f46\u6d4b\u8bd5\u7528\u4f8b\u96be\u4ee5\u83b7\u53d6\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\u5bf9\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u7cfb\u7edf\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e94\u7528\u4e8eCodeContests\u6570\u636e\u96c6\u5e76\u6539\u8fdb\u4e3aCodeContests+\uff0c\u901a\u8fc7\u767e\u4e07\u6b21\u63d0\u4ea4\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\u3002", "result": "CodeContests+\u5728\u8bc4\u4f30\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u539f\u6570\u636e\u96c6\uff0c\u5c24\u5176\u662f\u771f\u9633\u6027\u7387\uff08TPR\uff09\u66f4\u9ad8\uff0c\u4e14\u5728LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\u5bf9\u7f16\u7a0b\u7ade\u8d5b\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u548cLLM\u8bad\u7ec3\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u57fa\u4e8eLLM\u7684\u751f\u6210\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6570\u636e\u8d28\u91cf\u3002"}}
{"id": "2506.05619", "pdf": "https://arxiv.org/pdf/2506.05619", "abs": "https://arxiv.org/abs/2506.05619", "authors": ["Kihyun Kim", "Jiawei Zhang", "Asuman Ozdaglar", "Pablo A. Parrilo"], "title": "Population-Proportional Preference Learning from Human Feedback: An Axiomatic Approach", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Conventional preference learning methods often prioritize opinions held more\nwidely when aggregating preferences from multiple evaluators. This may result\nin policies that are biased in favor of some types of opinions or groups. The\nobjective of this paper is to develop a novel preference learning framework\ncapable of aligning aggregate opinions and policies proportionally with the\ntrue population distribution of evaluator preferences. Our approach infers the\nfeasible set of evaluator population distributions directly from pairwise\ncomparison data. Using these estimates, the algorithm constructs a policy that\nsatisfies foundational axioms from social choice theory, namely monotonicity\nand Pareto efficiency, as well as our newly-introduced axioms of\npopulation-proportional representation and population-bounded robustness. We\npropose a soft-max relaxation method that smoothly trade-offs\npopulation-proportional representation with the selection of the Condorcet\nwinner (which beats all other options in pairwise comparisons). Finally, we\nvalidate the effectiveness and scalability of our approach through experiments\non both tabular recommendation tasks and large-scale language model alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u504f\u597d\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6bd4\u4f8b\u4ee3\u8868\u6027\u548c\u9c81\u68d2\u6027\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u7684\u504f\u89c1\u3002", "motivation": "\u4f20\u7edf\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\u53ef\u80fd\u504f\u5411\u5e7f\u6cdb\u6301\u6709\u7684\u610f\u89c1\uff0c\u5bfc\u81f4\u653f\u7b56\u504f\u89c1\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u6bd4\u4f8b\u53cd\u6620\u8bc4\u4f30\u8005\u504f\u597d\u5206\u5e03\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6210\u5bf9\u6bd4\u8f83\u6570\u636e\u63a8\u65ad\u8bc4\u4f30\u8005\u7fa4\u4f53\u5206\u5e03\uff0c\u6784\u5efa\u6ee1\u8db3\u793e\u4f1a\u9009\u62e9\u7406\u8bba\u516c\u7406\uff08\u5982\u5355\u8c03\u6027\u548c\u5e15\u7d2f\u6258\u6548\u7387\uff09\u53ca\u65b0\u516c\u7406\uff08\u5982\u6bd4\u4f8b\u4ee3\u8868\u6027\u548c\u9c81\u68d2\u6027\uff09\u7684\u653f\u7b56\u3002\u91c7\u7528\u8f6f\u6700\u5927\u677e\u5f1b\u65b9\u6cd5\u5e73\u8861\u6bd4\u4f8b\u4ee3\u8868\u6027\u548cCondorcet\u80dc\u8005\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u8868\u683c\u63a8\u8350\u4efb\u52a1\u548c\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u3002", "conclusion": "\u65b0\u6846\u67b6\u80fd\u66f4\u516c\u5e73\u5730\u53cd\u6620\u8bc4\u4f30\u8005\u504f\u597d\u5206\u5e03\uff0c\u540c\u65f6\u6ee1\u8db3\u5173\u952e\u516c\u7406\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.05822", "pdf": "https://arxiv.org/pdf/2506.05822", "abs": "https://arxiv.org/abs/2506.05822", "authors": ["Lucas Mauser", "Eva Zimmermann", "Pavel Nedv\u011bdick\u00fd", "Tobias Eisenreich", "Moritz W\u00e4schle", "Stefan Wagner"], "title": "Towards Mixed-Criticality Software Architectures for Centralized HPC Platforms in Software-Defined Vehicles: A Systematic Literature Review", "categories": ["cs.SE"], "comment": "Preprint for research paper track of ECSA 2025", "summary": "Centralized electrical/electronic architectures and High-Performance\nComputers (HPCs) are redefining automotive software development, challenging\ntraditional microcontroller-based approaches. Ensuring real-time, safety, and\nscalability in software-defined vehicles necessitates reevaluating how\nmixed-criticality software is integrated into centralized architectures. While\nexisting research on automotive SoftWare Architectures (SWAs) is relevant to\nthe industry, it often lacks validation through systematic, empirical methods.\nTo address this gap, we conduct a systematic literature review focusing on\nautomotive mixed-criticality SWAs. Our goal is to provide practitioner-oriented\nguidelines that assist automotive software architects and developers design\ncentralized, mixed-criticality SWAs based on a rigorous and transparent\nmethodology. First, we set up a systematic review protocol grounded in\nestablished guidelines. Second, we apply this protocol to identify relevant\nstudies. Third, we extract key functional domains, constraints, and enabling\ntechnologies that drive changes in automotive SWAs, thereby assessing the\nprotocol's effectiveness. Additionally, we extract techniques, architectural\npatterns, and design practices for integrating mixed-criticality requirements\ninto HPC-based SWAs, further demonstrating the protocol's applicability. Based\non these insights, we propose an exemplary SWA for a microprocessor-based\nsystem-on-chip. In conclusion, this study provides a structured approach to\nexplore and realize mixed-criticality software integration for next-generation\nautomotive SWAs, offering valuable insights for industry and research\napplications.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u4e3a\u6c7d\u8f66\u8f6f\u4ef6\u67b6\u6784\u5e08\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u96c6\u4e2d\u5f0f\u6df7\u5408\u5173\u952e\u6027\u8f6f\u4ef6\u67b6\u6784\u7684\u8bbe\u8ba1\u6307\u5357\uff0c\u57fa\u4e8e\u4e25\u683c\u900f\u660e\u7684\u65b9\u6cd5\u8bba\u3002", "motivation": "\u96c6\u4e2d\u5f0f\u7535\u5b50\u67b6\u6784\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u673a\uff08HPC\uff09\u6b63\u5728\u91cd\u65b0\u5b9a\u4e49\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\uff0c\u9700\u91cd\u65b0\u8bc4\u4f30\u6df7\u5408\u5173\u952e\u6027\u8f6f\u4ef6\u5728\u96c6\u4e2d\u5f0f\u67b6\u6784\u4e2d\u7684\u96c6\u6210\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5236\u5b9a\u534f\u8bae\u5e76\u63d0\u53d6\u5173\u952e\u529f\u80fd\u57df\u3001\u7ea6\u675f\u548c\u4f7f\u80fd\u6280\u672f\uff0c\u8bc4\u4f30\u534f\u8bae\u6709\u6548\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u5904\u7406\u5668\u7684\u7cfb\u7edf\u7ea7\u82af\u7247\u7684\u793a\u8303\u6027\u8f6f\u4ef6\u67b6\u6784\uff0c\u5c55\u793a\u4e86\u534f\u8bae\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4e0b\u4e00\u4ee3\u6c7d\u8f66\u8f6f\u4ef6\u67b6\u6784\u7684\u6df7\u5408\u5173\u952e\u6027\u8f6f\u4ef6\u96c6\u6210\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u5bf9\u884c\u4e1a\u548c\u7814\u7a76\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2506.05744", "pdf": "https://arxiv.org/pdf/2506.05744", "abs": "https://arxiv.org/abs/2506.05744", "authors": ["Gouki Minegishi", "Hiroki Furuta", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties", "categories": ["cs.AI"], "comment": null, "summary": "Recent large-scale reasoning models have achieved state-of-the-art\nperformance on challenging mathematical benchmarks, yet the internal mechanisms\nunderlying their success remain poorly understood. In this work, we introduce\nthe notion of a reasoning graph, extracted by clustering hidden-state\nrepresentations at each reasoning step, and systematically analyze three key\ngraph-theoretic properties: cyclicity, diameter, and small-world index, across\nmultiple tasks (GSM8K, MATH500, AIME 2024). Our findings reveal that distilled\nreasoning models (e.g., DeepSeek-R1-Distill-Qwen-32B) exhibit significantly\nmore recurrent cycles (about 5 per sample), substantially larger graph\ndiameters, and pronounced small-world characteristics (about 6x) compared to\ntheir base counterparts. Notably, these structural advantages grow with task\ndifficulty and model capacity, with cycle detection peaking at the 14B scale\nand exploration diameter maximized in the 32B variant, correlating positively\nwith accuracy. Furthermore, we show that supervised fine-tuning on an improved\ndataset systematically expands reasoning graph diameters in tandem with\nperformance gains, offering concrete guidelines for dataset design aimed at\nboosting reasoning capabilities. By bridging theoretical insights into\nreasoning graph structures with practical recommendations for data\nconstruction, our work advances both the interpretability and the efficacy of\nlarge reasoning models.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u63a8\u7406\u56fe\u7684\u6982\u5ff5\uff0c\u5206\u6790\u4e86\u63a8\u7406\u6a21\u578b\u7684\u56fe\u8bba\u7279\u6027\uff08\u5faa\u73af\u6027\u3001\u76f4\u5f84\u548c\u5c0f\u4e16\u754c\u6307\u6570\uff09\uff0c\u53d1\u73b0\u84b8\u998f\u6a21\u578b\u5728\u7ed3\u6784\u4e0a\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u4e14\u8fd9\u4e9b\u4f18\u52bf\u4e0e\u4efb\u52a1\u96be\u5ea6\u548c\u6a21\u578b\u89c4\u6a21\u76f8\u5173\u3002", "motivation": "\u7406\u89e3\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u6210\u529f\u7684\u5185\u90e8\u673a\u5236\uff0c\u5e76\u4e3a\u63d0\u5347\u5176\u6027\u80fd\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u9690\u85cf\u72b6\u6001\u8868\u793a\u63d0\u53d6\u63a8\u7406\u56fe\uff0c\u5e76\u5206\u6790\u5176\u56fe\u8bba\u7279\u6027\uff08\u5faa\u73af\u6027\u3001\u76f4\u5f84\u548c\u5c0f\u4e16\u754c\u6307\u6570\uff09\u3002", "result": "\u84b8\u998f\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u591a\u5faa\u73af\u3001\u66f4\u5927\u76f4\u5f84\u548c\u66f4\u663e\u8457\u7684\u5c0f\u4e16\u754c\u7279\u6027\uff0c\u4e14\u8fd9\u4e9b\u7279\u6027\u4e0e\u4efb\u52a1\u96be\u5ea6\u548c\u6a21\u578b\u89c4\u6a21\u6b63\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63a8\u7406\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2506.05836", "pdf": "https://arxiv.org/pdf/2506.05836", "abs": "https://arxiv.org/abs/2506.05836", "authors": ["Nakhat Syeda", "Harsh Shah", "Rajvinder Singh", "Suraj Jaju", "Sumedha Kumar", "Gourav Chhabra", "Maria Spichkova"], "title": "Analysis of cost-efficiency of serverless approaches", "categories": ["cs.SE"], "comment": null, "summary": "In this paper, we present a survey of research studies related to the\ncost-effectiveness of serverless approach and corresponding cost savings. We\nconducted a systematic literature review using Google Scholar search engine,\ncovering the period from 2010 to 2024. We identified 34 related studies, from\nwhich we extracted 17 parameters that might influence the relative cost savings\nof applying the serverless approach.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5173\u4e8e\u65e0\u670d\u52a1\u5668\u65b9\u6cd5\u6210\u672c\u6548\u76ca\u53ca\u76f8\u5173\u8282\u7701\u7684\u7814\u7a76\uff0c\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u56de\u987e\u63d0\u53d6\u4e86\u5f71\u54cd\u8282\u7701\u768417\u4e2a\u53c2\u6570\u3002", "motivation": "\u63a2\u8ba8\u65e0\u670d\u52a1\u5668\u65b9\u6cd5\u7684\u6210\u672c\u6548\u76ca\u53ca\u5176\u8282\u7701\u6f5c\u529b\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u4f7f\u7528Google Scholar\u8fdb\u884c\u7cfb\u7edf\u6587\u732e\u56de\u987e\uff0c\u6db5\u76d62010\u81f32024\u5e74\u768434\u9879\u7814\u7a76\u3002", "result": "\u63d0\u53d6\u4e8617\u4e2a\u53ef\u80fd\u5f71\u54cd\u65e0\u670d\u52a1\u5668\u65b9\u6cd5\u6210\u672c\u8282\u7701\u7684\u53c2\u6570\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u65e0\u670d\u52a1\u5668\u65b9\u6cd5\u7684\u6210\u672c\u6548\u76ca\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002"}}
{"id": "2506.05745", "pdf": "https://arxiv.org/pdf/2506.05745", "abs": "https://arxiv.org/abs/2506.05745", "authors": ["Emil Biju", "Shayan Talaei", "Zhemin Huang", "Mohammadreza Pourreza", "Azalia Mirhoseini", "Amin Saberi"], "title": "SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models", "categories": ["cs.AI", "cs.LG"], "comment": "Emil Biju, Shayan Talaei, and Zhemin Huang contributed equally to\n  this work", "summary": "Large reasoning models (LRMs) excel at complex reasoning tasks but typically\ngenerate lengthy sequential chains-of-thought, resulting in long inference\ntimes before arriving at the final answer. To address this challenge, we\nintroduce SPRINT, a novel post-training and inference-time framework designed\nto enable LRMs to dynamically identify and exploit opportunities for\nparallelization during their reasoning process. SPRINT incorporates an\ninnovative data curation pipeline that reorganizes natural language reasoning\ntrajectories into structured rounds of long-horizon planning and parallel\nexecution. By fine-tuning LRMs on a small amount of such curated data, the\nmodels learn to dynamically identify independent subtasks within extended\nreasoning processes and effectively execute them in parallel. Through extensive\nevaluations, we show that the models fine-tuned with the SPRINT framework match\nthe performance of reasoning models on complex domains such as mathematics\nwhile generating up to ~39% fewer sequential tokens on problems requiring more\nthan 8000 output tokens. Finally, we observe consistent results transferred to\ntwo out-of-distribution tasks of GPQA and Countdown with up to 45% and 65%\nreduction in average sequential tokens for longer reasoning trajectories, while\nachieving the performance of the fine-tuned reasoning model.", "AI": {"tldr": "SPRINT\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5e76\u884c\u5316\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u63a8\u7406\u6548\u7387\uff0c\u51cf\u5c11\u5e8f\u5217\u5316\u63a8\u7406\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u901a\u5e38\u4e3a\u957f\u5e8f\u5217\u5316\u94fe\u5f0f\u601d\u7ef4\uff0c\u5bfc\u81f4\u63a8\u7406\u65f6\u95f4\u8fc7\u957f\u3002", "method": "SPRINT\u901a\u8fc7\u6570\u636e\u91cd\u7ec4\u548c\u5fae\u8c03\uff0c\u4f7fLRMs\u80fd\u591f\u52a8\u6001\u8bc6\u522b\u5e76\u884c\u5316\u673a\u4f1a\uff0c\u5c06\u63a8\u7406\u8fc7\u7a0b\u5206\u4e3a\u957f\u65f6\u89c4\u5212\u548c\u5e76\u884c\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSPRINT\u5728\u6570\u5b66\u7b49\u9886\u57df\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e8639%\u7684\u5e8f\u5217\u5316\u63a8\u7406\u65f6\u95f4\uff0c\u5e76\u5728GPQA\u548cCountdown\u4efb\u52a1\u4e2d\u5206\u522b\u51cf\u5c1145%\u548c65%\u7684\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "SPRINT\u6709\u6548\u63d0\u5347\u4e86LRMs\u7684\u63a8\u7406\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff0c\u4e14\u5177\u6709\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.05990", "pdf": "https://arxiv.org/pdf/2506.05990", "abs": "https://arxiv.org/abs/2506.05990", "authors": ["Stefan Dascalescu", "Adrian Marius Dumitran", "Mihai Alexandru Vasiluta"], "title": "Leveraging Generative AI for Enhancing Automated Assessment in Programming Education Contests", "categories": ["cs.SE", "cs.AI"], "comment": "11 pages, 2 chart pies, 1 figure Pre-print version Accepted at BEA\n  2025", "summary": "Competitive programming contests play a crucial role in cultivating\ncomputational thinking and algorithmic skills among learners. However,\ngenerating comprehensive test cases to effectively assess programming solutions\nremains resource-intensive and challenging for educators. This paper introduces\nan innovative NLP-driven method leveraging generative AI (large language\nmodels) to automate the creation of high-quality test cases for competitive\nprogramming assessments. We extensively evaluated our approach on diverse\ndatasets, including 25 years of Romanian Informatics Olympiad (OJI) data for\n5th graders, recent competitions hosted on the Kilonova.ro platform, and the\nInternational Informatics Olympiad in Teams (IIOT). Our results demonstrate\nthat AI-generated test cases substantially enhanced assessments, notably\nidentifying previously undetected errors in 67% of the OJI 5th grade\nprogramming problems. These improvements underscore the complementary\neducational value of our technique in formative assessment contexts. By openly\nsharing our prompts, translated datasets, and methodologies, we offer practical\nNLP-based tools that educators and contest organizers can readily integrate to\nenhance assessment quality, reduce workload, and deepen insights into learner\nperformance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNLP\u548c\u751f\u6210\u5f0fAI\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u7f16\u7a0b\u7ade\u8d5b\u6d4b\u8bd5\u7528\u4f8b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u6548\u679c\u3002", "motivation": "\u7f16\u7a0b\u7ade\u8d5b\u5bf9\u57f9\u517b\u8ba1\u7b97\u601d\u7ef4\u548c\u7b97\u6cd5\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u751f\u6210\u5168\u9762\u7684\u6d4b\u8bd5\u7528\u4f8b\u5bf9\u6559\u80b2\u8005\u6765\u8bf4\u8d44\u6e90\u5bc6\u96c6\u4e14\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u751f\u6210\u5f0fAI\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09\u9a71\u52a8\u7684NLP\u65b9\u6cd5\uff0c\u81ea\u52a8\u5316\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cAI\u751f\u6210\u7684\u6d4b\u8bd5\u7528\u4f8b\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u7f57\u9a6c\u5c3c\u4e9a\u4fe1\u606f\u5b66\u5965\u8d5b5\u5e74\u7ea7\u9898\u76ee\u4e2d\u53d1\u73b0\u4e8667%\u7684\u672a\u68c0\u6d4b\u9519\u8bef\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6559\u80b2\u8005\u548c\u7ade\u8d5b\u7ec4\u7ec7\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684NLP\u5de5\u5177\uff0c\u53ef\u63d0\u5347\u8bc4\u4f30\u8d28\u91cf\u3001\u51cf\u8f7b\u5de5\u4f5c\u91cf\u5e76\u6df1\u5316\u5bf9\u5b66\u4e60\u8005\u8868\u73b0\u7684\u6d1e\u5bdf\u3002"}}
{"id": "2506.05754", "pdf": "https://arxiv.org/pdf/2506.05754", "abs": "https://arxiv.org/abs/2506.05754", "authors": ["Emmanuel Anaya Gonzalez", "Sairam Vaidya", "Kanghee Park", "Ruyi Ji", "Taylor Berg-Kirkpatrick", "Loris D'Antoni"], "title": "Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Constrained decoding enables Language Models (LMs) to produce samples that\nprovably satisfy hard constraints. However, existing constrained-decoding\napproaches often distort the underlying model distribution, a limitation that\nis especially problematic in applications like program fuzzing, where one wants\nto generate diverse and valid program inputs for testing purposes. We propose a\nnew constrained sampling framework based on Markov Chain Monte Carlo (MCMC)\nthat simultaneously satisfies three core desiderata: constraint satisfying\n(every sample satisfies the constraint), monotonically converging (the sampling\nprocess converges to the true conditional distribution), and efficient\n(high-quality samples emerge in few steps). Our method constructs a proposal\ndistribution over valid outputs and applies a Metropolis-Hastings acceptance\ncriterion based on the LM's likelihood, ensuring principled and efficient\nexploration of the constrained space. Empirically, our sampler outperforms\nexisting methods on both synthetic benchmarks and real-world program fuzzing\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMCMC\u7684\u7ea6\u675f\u91c7\u6837\u6846\u67b6\uff0c\u6ee1\u8db3\u7ea6\u675f\u6ee1\u8db3\u3001\u5355\u8c03\u6536\u655b\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7ea6\u675f\u89e3\u7801\u65b9\u6cd5\u4f1a\u626d\u66f2\u6a21\u578b\u5206\u5e03\uff0c\u5f71\u54cd\u7a0b\u5e8f\u6a21\u7cca\u6d4b\u8bd5\u7b49\u5e94\u7528\u7684\u591a\u6837\u6027\u3002", "method": "\u6784\u5efa\u6709\u6548\u8f93\u51fa\u7684\u63d0\u8bae\u5206\u5e03\uff0c\u5e76\u57fa\u4e8eLM\u4f3c\u7136\u5e94\u7528Metropolis-Hastings\u63a5\u53d7\u51c6\u5219\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u5b9e\u9645\u7a0b\u5e8f\u6a21\u7cca\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u6846\u67b6\u80fd\u9ad8\u6548\u751f\u6210\u6ee1\u8db3\u7ea6\u675f\u7684\u591a\u6837\u6837\u672c\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.06202", "pdf": "https://arxiv.org/pdf/2506.06202", "abs": "https://arxiv.org/abs/2506.06202", "authors": ["Renato Cordeiro Ferreira", "Rowanne Trapmann", "Willem-Jan van den Heuvel"], "title": "MLOps with Microservices: A Case Study on the Maritime Domain", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.11; D.2.9; I.2.m; I.5.0"], "comment": "13 pages, 3 figures, to be published in SummerSOC 2025", "summary": "This case study describes challenges and lessons learned on building Ocean\nGuard: a Machine Learning-Enabled System (MLES) for anomaly detection in the\nmaritime domain. First, the paper presents the system's specification, and\narchitecture. Ocean Guard was designed with a microservices' architecture to\nenable multiple teams to work on the project in parallel. Then, the paper\ndiscusses how the developers adapted contract-based design to MLOps for\nachieving that goal. As a MLES, Ocean Guard employs code, model, and data\ncontracts to establish guidelines between its services. This case study hopes\nto inspire software engineers, machine learning engineers, and data scientists\nto leverage similar approaches for their systems.", "AI": {"tldr": "Ocean Guard\u662f\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\uff0c\u91c7\u7528\u5fae\u670d\u52a1\u67b6\u6784\u548c\u5408\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4e3a\u591a\u56e2\u961f\u534f\u4f5c\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u4e3a\u6d77\u4e8b\u9886\u57df\u7684\u5f02\u5e38\u68c0\u6d4b\u6784\u5efa\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u534f\u4f5c\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u5fae\u670d\u52a1\u67b6\u6784\u548c\u5408\u540c\u8bbe\u8ba1\uff08\u4ee3\u7801\u3001\u6a21\u578b\u3001\u6570\u636e\u5408\u540c\uff09\u4ee5\u5b9e\u73b0\u591a\u56e2\u961f\u5e76\u884c\u5f00\u53d1\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86Ocean Guard\u7cfb\u7edf\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5408\u540c\u8bbe\u8ba1\u5728MLOps\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6848\u4f8b\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u3001\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u5e08\u548c\u6570\u636e\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u53ef\u501f\u9274\u7684\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u6cd5\u3002"}}
{"id": "2506.05810", "pdf": "https://arxiv.org/pdf/2506.05810", "abs": "https://arxiv.org/abs/2506.05810", "authors": ["Yesheng Zhang", "Wenjian Sun", "Yuheng Chen", "Qingwei Liu", "Qi Lin", "Rui Zhang", "Xu Zhao"], "title": "Trajectory Entropy: Modeling Game State Stability from Multimodality Trajectory Prediction", "categories": ["cs.AI", "cs.RO"], "comment": "10 pages", "summary": "Complex interactions among agents present a significant challenge for\nautonomous driving in real-world scenarios. Recently, a promising approach has\nemerged, which formulates the interactions of agents as a level-k game\nframework. It effectively decouples agent policies by hierarchical game levels.\nHowever, this framework ignores both the varying driving complexities among\nagents and the dynamic changes in agent states across game levels, instead\ntreating them uniformly. Consequently, redundant and error-prone computations\nare introduced into this framework. To tackle the issue, this paper proposes a\nmetric, termed as Trajectory Entropy, to reveal the game status of agents\nwithin the level-k game framework. The key insight stems from recognizing the\ninherit relationship between agent policy uncertainty and the associated\ndriving complexity. Specifically, Trajectory Entropy extracts statistical\nsignals representing uncertainty from the multimodality trajectory prediction\nresults of agents in the game. Then, the signal-to-noise ratio of this signal\nis utilized to quantify the game status of agents. Based on the proposed\nTrajectory Entropy, we refine the current level-k game framework through a\nsimple gating mechanism, significantly improving overall accuracy while\nreducing computational costs. Our method is evaluated on the Waymo and nuPlan\ndatasets, in terms of trajectory prediction, open-loop and closed-loop planning\ntasks. The results demonstrate the state-of-the-art performance of our method,\nwith precision improved by up to 19.89% for prediction and up to 16.48% for\nplanning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u8f68\u8ff9\u71b5\u201d\u7684\u6307\u6807\uff0c\u7528\u4e8e\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u4e2d\u57fa\u4e8elevel-k\u535a\u5f08\u6846\u67b6\u7684\u4ea4\u4e92\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709level-k\u535a\u5f08\u6846\u67b6\u5ffd\u7565\u4e86\u9a7e\u9a76\u590d\u6742\u6027\u548c\u52a8\u6001\u53d8\u5316\u7684\u5dee\u5f02\u6027\uff0c\u5bfc\u81f4\u5197\u4f59\u548c\u9519\u8bef\u8ba1\u7b97\u3002", "method": "\u63d0\u51fa\u8f68\u8ff9\u71b5\u6307\u6807\uff0c\u4ece\u591a\u6a21\u6001\u8f68\u8ff9\u9884\u6d4b\u7ed3\u679c\u4e2d\u63d0\u53d6\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u4fe1\u566a\u6bd4\u91cf\u5316\u535a\u5f08\u72b6\u6001\uff0c\u7ed3\u5408\u95e8\u63a7\u673a\u5236\u4f18\u5316\u6846\u67b6\u3002", "result": "\u5728Waymo\u548cnuPlan\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u9884\u6d4b\u7cbe\u5ea6\u63d0\u534719.89%\uff0c\u89c4\u5212\u7cbe\u5ea6\u63d0\u534716.48%\u3002", "conclusion": "\u8f68\u8ff9\u71b5\u6709\u6548\u6539\u8fdb\u4e86level-k\u535a\u5f08\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u4ea4\u4e92\u5efa\u6a21\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2506.06247", "pdf": "https://arxiv.org/pdf/2506.06247", "abs": "https://arxiv.org/abs/2506.06247", "authors": ["Sedick David Baker Effendi", "Xavier Pinho", "Andrei Michael Dreyer", "Fabian Yamaguchi"], "title": "Scalable Language Agnostic Taint Tracking using Explicit Data Dependencies", "categories": ["cs.SE", "D.2.4"], "comment": "9 pages including appendix, SOAP'25", "summary": "Taint analysis using explicit whole-program data-dependence graphs is\npowerful for vulnerability discovery but faces two major challenges. First,\naccurately modeling taint propagation through calls to external library\nprocedures requires extensive manual annotations, which becomes impractical for\nlarge ecosystems. Second, the sheer size of whole-program graph representations\nleads to serious scalability and performance issues, particularly when quick\nanalysis is needed in continuous development pipelines.\n  This paper presents the design and implementation of a system for a\nlanguage-agnostic data-dependence representation. The system accommodates\nmissing annotations describing the behavior of library procedures by\nover-approximating data flows, allowing annotations to be added later without\nrecalculation. We contribute this data-flow analysis system to the open-source\ncode analysis platform Joern making it available to the community.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u8a00\u65e0\u5173\u7684\u6570\u636e\u4f9d\u8d56\u8868\u793a\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u6c61\u70b9\u5206\u6790\u4e2d\u5916\u90e8\u5e93\u8c03\u7528\u5efa\u6a21\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u6c61\u70b9\u5206\u6790\u5728\u6f0f\u6d1e\u53d1\u73b0\u4e2d\u5f88\u5f3a\u5927\uff0c\u4f46\u9762\u4e34\u5916\u90e8\u5e93\u8c03\u7528\u5efa\u6a21\u9700\u8981\u5927\u91cf\u624b\u52a8\u6ce8\u91ca\u548c\u5168\u7a0b\u5e8f\u56fe\u8868\u793a\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u8bed\u8a00\u65e0\u5173\u7684\u6570\u636e\u4f9d\u8d56\u8868\u793a\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fc7\u5ea6\u8fd1\u4f3c\u6570\u636e\u6d41\u6765\u9002\u5e94\u7f3a\u5931\u7684\u5e93\u884c\u4e3a\u6ce8\u91ca\uff0c\u5e76\u652f\u6301\u540e\u7eed\u6dfb\u52a0\u6ce8\u91ca\u800c\u65e0\u9700\u91cd\u65b0\u8ba1\u7b97\u3002", "result": "\u8be5\u7cfb\u7edf\u88ab\u8d21\u732e\u7ed9\u5f00\u6e90\u4ee3\u7801\u5206\u6790\u5e73\u53f0Joern\uff0c\u4f9b\u793e\u533a\u4f7f\u7528\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u89e3\u51b3\u4e86\u6c61\u70b9\u5206\u6790\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u5347\u4e86\u5206\u6790\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.05887", "pdf": "https://arxiv.org/pdf/2506.05887", "abs": "https://arxiv.org/abs/2506.05887", "authors": ["Marilyn Bello", "Rafael Bello", "Maria-Matilde Garc\u00eda", "Ann Now\u00e9", "Iv\u00e1n Sevillano-Garc\u00eda", "Francisco Herrera"], "title": "Explainability in Context: A Multilevel Framework Aligning AI Explanations with Stakeholder with LLMs", "categories": ["cs.AI"], "comment": "22 pages, 5 figures", "summary": "The growing application of artificial intelligence in sensitive domains has\nintensified the demand for systems that are not only accurate but also\nexplainable and trustworthy. Although explainable AI (XAI) methods have\nproliferated, many do not consider the diverse audiences that interact with AI\nsystems: from developers and domain experts to end-users and society. This\npaper addresses how trust in AI is influenced by the design and delivery of\nexplanations and proposes a multilevel framework that aligns explanations with\nthe epistemic, contextual, and ethical expectations of different stakeholders.\nThe framework consists of three layers: algorithmic and domain-based,\nhuman-centered, and social explainability. We highlight the emerging role of\nLarge Language Models (LLMs) in enhancing the social layer by generating\naccessible, natural language explanations. Through illustrative case studies,\nwe demonstrate how this approach facilitates technical fidelity, user\nengagement, and societal accountability, reframing XAI as a dynamic,\ntrust-building process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u6b21\u7684\u89e3\u91ca\u6027AI\u6846\u67b6\uff0c\u65e8\u5728\u6ee1\u8db3\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\uff0c\u589e\u5f3aAI\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u968f\u7740AI\u5728\u654f\u611f\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bf9\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u4e0d\u540c\u53d7\u4f17\u7684\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u7b97\u6cd5\u4e0e\u9886\u57df\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u548c\u793e\u4f1a\u89e3\u91ca\u6027\u4e09\u4e2a\u5c42\u6b21\u7684\u591a\u5c42\u6b21\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8\u4e86LLMs\u5728\u793e\u4f1a\u5c42\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5982\u4f55\u63d0\u5347\u6280\u672f\u4fdd\u771f\u5ea6\u3001\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u793e\u4f1a\u8d23\u4efb\uff0c\u5c06XAI\u91cd\u5851\u4e3a\u52a8\u6001\u7684\u4fe1\u4efb\u6784\u5efa\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bbe\u8ba1\u66f4\u53ef\u4fe1\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u591a\u5c42\u6b21\u7684\u89e3\u91ca\u6027\u5728\u4fe1\u4efb\u6784\u5efa\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.06251", "pdf": "https://arxiv.org/pdf/2506.06251", "abs": "https://arxiv.org/abs/2506.06251", "authors": ["Jingyu Xiao", "Ming Wang", "Man Ho Lam", "Yuxuan Wan", "Junliang Liu", "Yintong Huo", "Michael R. Lyu"], "title": "DesignBench: A Comprehensive Benchmark for MLLM-based Front-end Code Generation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in automated front-end engineering, e.g., generating UI code from\nvisual designs. However, existing front-end UI code generation benchmarks have\nthe following limitations: (1) While framework-based development becomes\npredominant in modern front-end programming, current benchmarks fail to\nincorporate mainstream development frameworks. (2) Existing evaluations focus\nsolely on the UI code generation task, whereas practical UI development\ninvolves several iterations, including refining editing, and repairing issues.\n(3) Current benchmarks employ unidimensional evaluation, lacking investigation\ninto influencing factors like task difficulty, input context variations, and\nin-depth code-level analysis. To bridge these gaps, we introduce DesignBench, a\nmulti-framework, multi-task evaluation benchmark for assessing MLLMs'\ncapabilities in automated front-end engineering. DesignBench encompasses three\nwidely-used UI frameworks (React, Vue, and Angular) alongside vanilla HTML/CSS,\nand evaluates on three essential front-end tasks (generation, edit, and repair)\nin real-world development workflows. DesignBench contains 900 webpage samples\nspanning over 11 topics, 9 edit types, and 6 issue categories, enabling\ndetailed analysis of MLLM performance across multiple dimensions. Our\nsystematic evaluation reveals critical insights into MLLMs' framework-specific\nlimitations, task-related bottlenecks, and performance variations under\ndifferent conditions, providing guidance for future research in automated\nfront-end development. Our code and data are available at\nhttps://github.com/WebPAI/DesignBench.", "AI": {"tldr": "DesignBench\u662f\u4e00\u4e2a\u591a\u6846\u67b6\u3001\u591a\u4efb\u52a1\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30MLLM\u5728\u81ea\u52a8\u524d\u7aef\u5de5\u7a0b\u4e2d\u7684\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u524d\u7aefUI\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u672a\u80fd\u6db5\u76d6\u4e3b\u6d41\u5f00\u53d1\u6846\u67b6\u3001\u591a\u4efb\u52a1\u8fed\u4ee3\u548c\u591a\u7ef4\u8bc4\u4f30\uff0c\u9650\u5236\u4e86MLLM\u5728\u5b9e\u9645\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165DesignBench\uff0c\u5305\u542bReact\u3001Vue\u3001Angular\u548cHTML/CSS\uff0c\u8bc4\u4f30\u751f\u6210\u3001\u7f16\u8f91\u548c\u4fee\u590d\u4efb\u52a1\uff0c\u6db5\u76d6900\u4e2a\u7f51\u9875\u6837\u672c\u3002", "result": "\u7cfb\u7edf\u8bc4\u4f30\u63ed\u793a\u4e86MLLM\u5728\u6846\u67b6\u9650\u5236\u3001\u4efb\u52a1\u74f6\u9888\u548c\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "DesignBench\u4e3a\u672a\u6765\u81ea\u52a8\u524d\u7aef\u5f00\u53d1\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.05904", "pdf": "https://arxiv.org/pdf/2506.05904", "abs": "https://arxiv.org/abs/2506.05904", "authors": ["Yichi Zhang", "Xin Luna Dong", "Zhaojiang Lin", "Andrea Madotto", "Anuj Kumar", "Babak Damavandi", "Joyce Chai", "Seungwhan Moon"], "title": "Proactive Assistant Dialogue Generation from Streaming Egocentric Videos", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Recent advances in conversational AI have been substantial, but developing\nreal-time systems for perceptual task guidance remains challenging. These\nsystems must provide interactive, proactive assistance based on streaming\nvisual inputs, yet their development is constrained by the costly and\nlabor-intensive process of data collection and system evaluation. To address\nthese limitations, we present a comprehensive framework with three key\ncontributions. First, we introduce a novel data curation pipeline that\nsynthesizes dialogues from annotated egocentric videos, resulting in \\dataset,\na large-scale synthetic dialogue dataset spanning multiple domains. Second, we\ndevelop a suite of automatic evaluation metrics, validated through extensive\nhuman studies. Third, we propose an end-to-end model that processes streaming\nvideo inputs to generate contextually appropriate responses, incorporating\nnovel techniques for handling data imbalance and long-duration videos. This\nwork lays the foundation for developing real-time, proactive AI assistants\ncapable of guiding users through diverse tasks. Project page:\nhttps://pro-assist.github.io/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5b9e\u65f6\u611f\u77e5\u4efb\u52a1\u6307\u5bfc\u7684\u5bf9\u8bddAI\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u5408\u6210\u3001\u81ea\u52a8\u8bc4\u4f30\u548c\u7aef\u5230\u7aef\u6a21\u578b\u3002", "motivation": "\u5f00\u53d1\u5b9e\u65f6\u611f\u77e5\u4efb\u52a1\u6307\u5bfc\u7cfb\u7edf\u9762\u4e34\u6570\u636e\u6536\u96c6\u548c\u8bc4\u4f30\u7684\u9ad8\u6210\u672c\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6570\u636e\u5408\u6210\u7ba1\u9053\u751f\u6210\u5927\u89c4\u6a21\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u8bbe\u8ba1\u7aef\u5230\u7aef\u6a21\u578b\u5904\u7406\u6d41\u5f0f\u89c6\u9891\u8f93\u5165\u3002", "result": "\u751f\u6210\u4e86\u591a\u9886\u57df\u5408\u6210\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b9e\u65f6\u54cd\u5e94\u751f\u6210\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f00\u53d1\u5b9e\u65f6\u3001\u4e3b\u52a8\u7684AI\u52a9\u624b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.06161", "pdf": "https://arxiv.org/pdf/2506.06161", "abs": "https://arxiv.org/abs/2506.06161", "authors": ["Yufeng Wang", "Yuhong Feng", "Yixuan Cao", "Haoran Li", "Haiyue Feng", "Yifeng Wang"], "title": "Obfuscation-Resilient Binary Code Similarity Analysis using Dominance Enhanced Semantic Graph", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Binary code similarity analysis (BCSA) serves as a core technique for binary\nanalysis tasks such as vulnerability detection. While current graph-based BCSA\napproaches capture substantial semantics and show strong performance, their\nperformance suffers under code obfuscation due to the unstable control flow. To\naddress this issue, we develop ORCAS, an Obfuscation-Resilient BCSA model based\non Dominance Enhanced Semantic Graph (DESG). The DESG is an original binary\ncode representation, capturing more binaries' implicit semantics without\ncontrol flow structure, including inter-instruction relations, inter-basic\nblock relations, and instruction-basic block relations. ORCAS robustly scores\nsemantic similarity across binary functions from different obfuscation options,\noptimization levels, and instruction set architectures. Extensive evaluation on\nthe BinKit dataset shows ORCAS significantly outperforms eight baselines,\nachieving an average 12.1% PR-AUC gain when using combined three obfuscation\noptions compared to the state-of-the-art approaches. Furthermore, ORCAS\nimproves recall by up to 43% on an original obfuscated real-world vulnerability\ndataset, which we released to facilitate future research.", "AI": {"tldr": "ORCAS\u662f\u4e00\u79cd\u57fa\u4e8eDESG\u7684\u4e8c\u8fdb\u5236\u4ee3\u7801\u76f8\u4f3c\u6027\u5206\u6790\u6a21\u578b\uff0c\u80fd\u591f\u62b5\u6297\u4ee3\u7801\u6df7\u6dc6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u56fe\u7684BCSA\u65b9\u6cd5\u5728\u4ee3\u7801\u6df7\u6dc6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5b9a\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDESG\u8868\u793a\u65b9\u6cd5\uff0c\u6355\u6349\u4e8c\u8fdb\u5236\u4ee3\u7801\u7684\u9690\u5f0f\u8bed\u4e49\uff0c\u5f00\u53d1ORCAS\u6a21\u578b\u3002", "result": "\u5728BinKit\u6570\u636e\u96c6\u4e0a\uff0cORCAS\u5e73\u5747PR-AUC\u63d0\u534712.1%\uff0c\u53ec\u56de\u7387\u63d0\u534743%\u3002", "conclusion": "ORCAS\u5728\u4ee3\u7801\u6df7\u6dc6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u4e8c\u8fdb\u5236\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.05967", "pdf": "https://arxiv.org/pdf/2506.05967", "abs": "https://arxiv.org/abs/2506.05967", "authors": ["Katarzyna Kobalczyk", "Mihaela van der Schaar"], "title": "Preference Learning for AI Alignment: a Causal Perspective", "categories": ["cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "Reward modelling from preference data is a crucial step in aligning large\nlanguage models (LLMs) with human values, requiring robust generalisation to\nnovel prompt-response pairs. In this work, we propose to frame this problem in\na causal paradigm, providing the rich toolbox of causality to identify the\npersistent challenges, such as causal misidentification, preference\nheterogeneity, and confounding due to user-specific factors. Inheriting from\nthe literature of causal inference, we identify key assumptions necessary for\nreliable generalisation and contrast them with common data collection\npractices. We illustrate failure modes of naive reward models and demonstrate\nhow causally-inspired approaches can improve model robustness. Finally, we\noutline desiderata for future research and practices, advocating targeted\ninterventions to address inherent limitations of observational data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7528\u56e0\u679c\u8303\u5f0f\u89e3\u51b3\u5956\u52b1\u5efa\u6a21\u95ee\u9898\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u5956\u52b1\u5efa\u6a21\u5728\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u56e0\u679c\u8bef\u8bc6\u522b\u3001\u504f\u597d\u5f02\u8d28\u6027\u548c\u7528\u6237\u56e0\u7d20\u5e72\u6270\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u56e0\u679c\u63a8\u7406\u5de5\u5177\uff0c\u5206\u6790\u5173\u952e\u5047\u8bbe\uff0c\u5bf9\u6bd4\u5e38\u89c1\u6570\u636e\u6536\u96c6\u5b9e\u8df5\uff0c\u5c55\u793a\u56e0\u679c\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "result": "\u56e0\u679c\u65b9\u6cd5\u80fd\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u9700\u9488\u5bf9\u89c2\u6d4b\u6570\u636e\u7684\u56fa\u6709\u5c40\u9650\uff0c\u8bbe\u8ba1\u9488\u5bf9\u6027\u5e72\u9884\u63aa\u65bd\u3002"}}
{"id": "2506.06262", "pdf": "https://arxiv.org/pdf/2506.06262", "abs": "https://arxiv.org/abs/2506.06262", "authors": ["Kjetil Vasstein", "Christian Le", "Simon Lerv\u00e5g Breivik", "Trygve Maukon Myhr", "Annette Stahl", "Edmund F\u00f8rland Brekke"], "title": "PyGemini: Unified Software Development towards Maritime Autonomy Systems", "categories": ["cs.RO", "cs.SE", "cs.SY", "eess.SY", "D.2.11; I.6.2; I.2.9"], "comment": "Preprint. Not yet submitted for peer review. Includes 14 figures and\n  3 tables. 18 pages, 1 appendix", "summary": "Ensuring the safety and certifiability of autonomous surface vessels (ASVs)\nrequires robust decision-making systems, supported by extensive simulation,\ntesting, and validation across a broad range of scenarios. However, the current\nlandscape of maritime autonomy development is fragmented -- relying on\ndisparate tools for communication, simulation, monitoring, and system\nintegration -- which hampers interdisciplinary collaboration and inhibits the\ncreation of compelling assurance cases, demanded by insurers and regulatory\nbodies. Furthermore, these disjointed tools often suffer from performance\nbottlenecks, vendor lock-in, and limited support for continuous integration\nworkflows. To address these challenges, we introduce PyGemini, a permissively\nlicensed, Python-native framework that builds on the legacy of Autoferry Gemini\nto unify maritime autonomy development. PyGemini introduces a novel\nConfiguration-Driven Development (CDD) process that fuses Behavior-Driven\nDevelopment (BDD), data-oriented design, and containerization to support\nmodular, maintainable, and scalable software architectures. The framework\nfunctions as a stand-alone application, cloud-based service, or embedded\nlibrary -- ensuring flexibility across research and operational contexts. We\ndemonstrate its versatility through a suite of maritime tools -- including 3D\ncontent generation for simulation and monitoring, scenario generation for\nautonomy validation and training, and generative artificial intelligence\npipelines for augmenting imagery -- thereby offering a scalable, maintainable,\nand performance-oriented foundation for future maritime robotics and autonomy\nresearch.", "AI": {"tldr": "PyGemini\u662f\u4e00\u4e2aPython\u539f\u751f\u6846\u67b6\uff0c\u65e8\u5728\u7edf\u4e00\u6d77\u4e0a\u81ea\u4e3b\u5f00\u53d1\uff0c\u89e3\u51b3\u5f53\u524d\u5de5\u5177\u5206\u6563\u3001\u6027\u80fd\u74f6\u9888\u7b49\u95ee\u9898\uff0c\u652f\u6301\u6a21\u5757\u5316\u3001\u53ef\u7ef4\u62a4\u548c\u53ef\u6269\u5c55\u7684\u8f6f\u4ef6\u67b6\u6784\u3002", "motivation": "\u5f53\u524d\u6d77\u4e0a\u81ea\u4e3b\u5f00\u53d1\u5de5\u5177\u5206\u6563\uff0c\u5bfc\u81f4\u8de8\u5b66\u79d1\u5408\u4f5c\u56f0\u96be\uff0c\u6027\u80fd\u74f6\u9888\u548c\u4f9b\u5e94\u5546\u9501\u5b9a\u95ee\u9898\u7a81\u51fa\uff0c\u4e9f\u9700\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165PyGemini\u6846\u67b6\uff0c\u91c7\u7528\u914d\u7f6e\u9a71\u52a8\u5f00\u53d1\uff08CDD\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u884c\u4e3a\u9a71\u52a8\u5f00\u53d1\uff08BDD\uff09\u3001\u6570\u636e\u5bfc\u5411\u8bbe\u8ba1\u548c\u5bb9\u5668\u5316\u6280\u672f\u3002", "result": "PyGemini\u63d0\u4f9b\u4e86\u4e00\u5957\u591a\u529f\u80fd\u5de5\u5177\uff0c\u652f\u63013D\u5185\u5bb9\u751f\u6210\u3001\u573a\u666f\u751f\u6210\u548c\u751f\u6210\u5f0fAI\u7ba1\u9053\uff0c\u4e3a\u6d77\u4e0a\u673a\u5668\u4eba\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "PyGemini\u4e3a\u6d77\u4e0a\u81ea\u4e3b\u5f00\u53d1\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u548c\u6027\u80fd\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u672a\u6765\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.05981", "pdf": "https://arxiv.org/pdf/2506.05981", "abs": "https://arxiv.org/abs/2506.05981", "authors": ["Qingbin Zeng", "Ruotong Zhao", "Jinzhu Mao", "Haoyang Li", "Fengli Xu", "Yong Li"], "title": "CrimeMind: Simulating Urban Crime with Multi-Modal LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "Modeling urban crime is an important yet challenging task that requires\nunderstanding the subtle visual, social, and cultural cues embedded in urban\nenvironments. Previous work has predominantly focused on rule-based agent-based\nmodeling (ABM) and deep learning methods. ABMs offer interpretability of\ninternal mechanisms but exhibit limited predictive accuracy.In contrast, deep\nlearning methods are often effective in prediction but are less interpretable\nand require extensive training data. Moreover, both lines of work lack the\ncognitive flexibility to adapt to changing environments. Leveraging the\ncapabilities of large language models (LLMs), we propose CrimeMind, a novel\nLLM-driven ABM framework for simulating urban crime within a multi-modal urban\ncontext.A key innovation of our design is the integration of the Routine\nActivity Theory (RAT) into the agentic workflow of CrimeMind, enabling it to\nprocess rich multi-modal urban features and reason about criminal\nbehavior.However, RAT requires LLM agents to infer subtle cues in evaluating\nenvironmental safety as part of assessing guardianship, which can be\nchallenging for LLMs. To address this, we collect a small-scale human-annotated\ndataset and align CrimeMind's perception with human judgment via a\ntraining-free textual gradient method.Experiments across four major U.S. cities\ndemonstrate that CrimeMind outperforms both traditional ABMs and deep learning\nbaselines in crime hotspot prediction and spatial distribution accuracy,\nachieving up to a 24% improvement over the strongest baseline.Furthermore, we\nconduct counterfactual simulations of external incidents and policy\ninterventions and it successfully captures the expected changes in crime\npatterns, demonstrating its ability to reflect counterfactual\nscenarios.Overall, CrimeMind enables fine-grained modeling of individual\nbehaviors and facilitates evaluation of real-world interventions.", "AI": {"tldr": "CrimeMind\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86Routine Activity Theory\uff08RAT\uff09\uff0c\u7528\u4e8e\u6a21\u62df\u57ce\u5e02\u72af\u7f6a\u3002\u5b83\u5728\u9884\u6d4b\u72af\u7f6a\u70ed\u70b9\u548c\u7a7a\u95f4\u5206\u5e03\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u80fd\u6a21\u62df\u653f\u7b56\u5e72\u9884\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u89c4\u5219\u7684ABM\u548c\u6df1\u5ea6\u5b66\u4e60\uff09\u5728\u9884\u6d4b\u51c6\u786e\u6027\u6216\u53ef\u89e3\u91ca\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u4e14\u7f3a\u4e4f\u9002\u5e94\u52a8\u6001\u73af\u5883\u7684\u80fd\u529b\u3002CrimeMind\u65e8\u5728\u7ed3\u5408LLM\u7684\u591a\u6a21\u6001\u5904\u7406\u80fd\u529b\uff0c\u63d0\u5347\u72af\u7f6a\u5efa\u6a21\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51faCrimeMind\u6846\u67b6\uff0c\u6574\u5408RAT\u7406\u8bba\uff0c\u5229\u7528LLM\u5904\u7406\u591a\u6a21\u6001\u57ce\u5e02\u7279\u5f81\u3002\u901a\u8fc7\u5c0f\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u548c\u65e0\u9700\u8bad\u7ec3\u7684\u6587\u672c\u68af\u5ea6\u65b9\u6cd5\uff0c\u5bf9\u9f50\u6a21\u578b\u4e0e\u4eba\u7c7b\u5224\u65ad\u3002", "result": "\u5728\u56db\u4e2a\u7f8e\u56fd\u57ce\u5e02\u7684\u5b9e\u9a8c\u4e2d\uff0cCrimeMind\u5728\u72af\u7f6a\u70ed\u70b9\u9884\u6d4b\u548c\u7a7a\u95f4\u5206\u5e03\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u534724%\u3002\u8fd8\u80fd\u6210\u529f\u6a21\u62df\u653f\u7b56\u5e72\u9884\u7684\u6548\u679c\u3002", "conclusion": "CrimeMind\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u4e2a\u4f53\u884c\u4e3a\u5efa\u6a21\uff0c\u652f\u6301\u771f\u5b9e\u4e16\u754c\u5e72\u9884\u7684\u8bc4\u4f30\uff0c\u4e3a\u57ce\u5e02\u72af\u7f6a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.06052", "pdf": "https://arxiv.org/pdf/2506.06052", "abs": "https://arxiv.org/abs/2506.06052", "authors": ["Kostis Michailidis", "Dimos Tsouros", "Tias Guns"], "title": "CP-Bench: Evaluating Large Language Models for Constraint Modelling", "categories": ["cs.AI"], "comment": null, "summary": "Combinatorial problems are present in a wide range of industries. Constraint\nProgramming (CP) is a well-suited problem-solving paradigm, but its core\nprocess, namely constraint modelling, is a bottleneck for wider adoption.\nAiming to alleviate this bottleneck, recent studies have explored using Large\nLanguage Models (LLMs) as modelling assistants, transforming combinatorial\nproblem descriptions to executable constraint models, similar to coding\nassistants. However, the existing evaluation datasets for constraint modelling\nare often limited to small, homogeneous, or domain-specific instances, which do\nnot capture the diversity of real-world scenarios. This work addresses this gap\nby introducing CP-Bench, a novel benchmark dataset that includes a diverse set\nof well-known combinatorial problem classes sourced from the CP community,\nstructured explicitly for evaluating LLM-driven CP modelling. With this\ndataset, and given the variety of constraint modelling frameworks, we compare\nand evaluate the modelling capabilities of LLMs for three distinct constraint\nmodelling systems, which vary in abstraction level and underlying syntax: the\nhigh-level MiniZinc language and Python-based CPMpy library, and the\nlower-level Python interface of the OR-Tools CP-SAT solver. In order to enhance\nthe ability of LLMs to produce valid constraint models, we systematically\nevaluate the use of prompt-based and inference-time compute methods adapted\nfrom existing LLM-based code generation research. Our results underscore the\nmodelling convenience provided by Python-based frameworks, as well as the\neffectiveness of documentation-rich system prompts, which, augmented with\nrepeated sampling and self-verification, achieve further improvements, reaching\nup to 70\\% accuracy on this new, highly challenging benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CP-Bench\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u7ea6\u675f\u7f16\u7a0b\u5efa\u6a21\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u5efa\u6a21\u7cfb\u7edf\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7ea6\u675f\u5efa\u6a21\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u5c0f\u89c4\u6a21\u3001\u540c\u8d28\u5316\u548c\u9886\u57df\u7279\u5b9a\u95ee\u9898\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u73b0\u5b9e\u573a\u666f\u7684\u591a\u6837\u6027\u3002", "method": "\u5f15\u5165CP-Bench\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LLM\u5728\u4e09\u79cd\u4e0d\u540c\u62bd\u8c61\u7ea7\u522b\u7684\u7ea6\u675f\u5efa\u6a21\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u91c7\u7528\u63d0\u793a\u548c\u63a8\u7406\u65f6\u8ba1\u7b97\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u751f\u6210\u80fd\u529b\u3002", "result": "Python\u6846\u67b6\u63d0\u4f9b\u4e86\u5efa\u6a21\u4fbf\u5229\u6027\uff0c\u6587\u6863\u4e30\u5bcc\u7684\u7cfb\u7edf\u63d0\u793a\u7ed3\u5408\u91cd\u590d\u91c7\u6837\u548c\u81ea\u6211\u9a8c\u8bc1\u53ef\u5c06\u51c6\u786e\u7387\u63d0\u5347\u81f370%\u3002", "conclusion": "CP-Bench\u4e3aLLM\u9a71\u52a8\u7684\u7ea6\u675f\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u5c55\u793a\u4e86\u6587\u6863\u63d0\u793a\u548c\u9a8c\u8bc1\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.06121", "pdf": "https://arxiv.org/pdf/2506.06121", "abs": "https://arxiv.org/abs/2506.06121", "authors": ["Ziyu Zhang", "Peilan Xu", "Yuetong Sun", "Yuhui Shi", "Wenjian Luo"], "title": "Decomposability-Guaranteed Cooperative Coevolution for Large-Scale Itinerary Planning", "categories": ["cs.AI"], "comment": null, "summary": "Large-scale itinerary planning is a variant of the traveling salesman\nproblem, aiming to determine an optimal path that maximizes the collected\npoints of interest (POIs) scores while minimizing travel time and cost, subject\nto travel duration constraints. This paper analyzes the decomposability of\nlarge-scale itinerary planning, proving that strict decomposability is\ndifficult to satisfy, and introduces a weak decomposability definition based on\na necessary condition, deriving the corresponding graph structures that fulfill\nthis property. With decomposability guaranteed, we propose a novel\nmulti-objective cooperative coevolutionary algorithm for large-scale itinerary\nplanning, addressing the challenges of component imbalance and interactions.\nSpecifically, we design a dynamic decomposition strategy based on the\nnormalized fitness within each component, define optimization potential\nconsidering component scale and contribution, and develop a computational\nresource allocation strategy. Finally, we evaluate the proposed algorithm on a\nset of real-world datasets. Comparative experiments with state-of-the-art\nmulti-objective itinerary planning algorithms demonstrate the superiority of\nour approach, with performance advantages increasing as the problem scale\ngrows.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u89c4\u6a21\u884c\u7a0b\u89c4\u5212\u7684\u5f31\u53ef\u5206\u89e3\u6027\u5b9a\u4e49\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u534f\u540c\u8fdb\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u89e3\u548c\u8d44\u6e90\u5206\u914d\u7b56\u7565\u4f18\u5316\u884c\u7a0b\u89c4\u5212\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u884c\u7a0b\u89c4\u5212\u4e2d\u4e25\u683c\u53ef\u5206\u89e3\u6027\u96be\u4ee5\u6ee1\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316\u65c5\u884c\u65f6\u95f4\u548c\u6210\u672c\u3002", "method": "\u5f15\u5165\u5f31\u53ef\u5206\u89e3\u6027\u5b9a\u4e49\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u5f52\u4e00\u5316\u9002\u5e94\u5ea6\u7684\u52a8\u6001\u5206\u89e3\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u7b56\u7565\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u968f\u7740\u95ee\u9898\u89c4\u6a21\u589e\u5927\uff0c\u6027\u80fd\u4f18\u52bf\u66f4\u660e\u663e\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5927\u89c4\u6a21\u884c\u7a0b\u89c4\u5212\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.06216", "pdf": "https://arxiv.org/pdf/2506.06216", "abs": "https://arxiv.org/abs/2506.06216", "authors": ["Jialu Zhang", "Chu-Min Li", "Sami Cherif", "Shuolin Li", "Zhifei Zheng"], "title": "Integer Linear Programming Preprocessing for Maximum Satisfiability", "categories": ["cs.AI"], "comment": null, "summary": "The Maximum Satisfiability problem (MaxSAT) is a major optimization challenge\nwith numerous practical applications. In recent MaxSAT evaluations, most MaxSAT\nsolvers have adopted an ILP solver as part of their portfolios. This paper\ninvestigates the impact of Integer Linear Programming (ILP) preprocessing\ntechniques on MaxSAT solving. Experimental results show that ILP preprocessing\ntechniques help WMaxCDCL-OpenWbo1200, the winner of the MaxSAT evaluation 2024\nin the unweighted track, solve 15 additional instances. Moreover, current\nstate-of-the-art MaxSAT solvers heavily use an ILP solver in their portfolios,\nwhile our proposed approach reduces the need to call an ILP solver in a\nportfolio including WMaxCDCL or MaxCDCL.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08ILP\uff09\u9884\u5904\u7406\u6280\u672f\u5bf9MaxSAT\u6c42\u89e3\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u80fd\u63d0\u5347\u6c42\u89e3\u5668\u6027\u80fd\uff0c\u51cf\u5c11\u5bf9ILP\u6c42\u89e3\u5668\u7684\u4f9d\u8d56\u3002", "motivation": "MaxSAT\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u4f18\u5316\u95ee\u9898\uff0c\u73b0\u6709\u6c42\u89e3\u5668\u591a\u4f9d\u8d56ILP\u6c42\u89e3\u5668\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22ILP\u9884\u5904\u7406\u6280\u672f\u5bf9MaxSAT\u6c42\u89e3\u7684\u6539\u8fdb\u6f5c\u529b\u3002", "method": "\u91c7\u7528ILP\u9884\u5904\u7406\u6280\u672f\uff0c\u5e76\u6d4b\u8bd5\u5176\u5bf9MaxSAT\u6c42\u89e3\u5668\uff08\u5982WMaxCDCL-OpenWbo1200\uff09\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cILP\u9884\u5904\u7406\u6280\u672f\u5e2e\u52a9WMaxCDCL-OpenWbo1200\u57282024\u5e74MaxSAT\u8bc4\u4f30\u4e2d\u591a\u89e3\u51b315\u4e2a\u5b9e\u4f8b\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9ILP\u6c42\u89e3\u5668\u7684\u8c03\u7528\u9700\u6c42\u3002", "conclusion": "ILP\u9884\u5904\u7406\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347MaxSAT\u6c42\u89e3\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u5bf9ILP\u6c42\u89e3\u5668\u7684\u4f9d\u8d56\u3002"}}
{"id": "2506.06254", "pdf": "https://arxiv.org/pdf/2506.06254", "abs": "https://arxiv.org/abs/2506.06254", "authors": ["Weizhi Zhang", "Xinyang Zhang", "Chenwei Zhang", "Liangwei Yang", "Jingbo Shang", "Zhepei Wei", "Henry Peng Zou", "Zijie Huang", "Zhengyang Wang", "Yifan Gao", "Xiaoman Pan", "Lian Xiong", "Jingguo Liu", "Philip S. Yu", "Xian Li"], "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM) empowered agents have recently emerged as advanced\nparadigms that exhibit impressive capabilities in a wide range of domains and\ntasks. Despite their potential, current LLM agents often adopt a\none-size-fits-all approach, lacking the flexibility to respond to users'\nvarying needs and preferences. This limitation motivates us to develop\nPersonaAgent, the first personalized LLM agent framework designed to address\nversatile personalization tasks. Specifically, PersonaAgent integrates two\ncomplementary components - a personalized memory module that includes episodic\nand semantic memory mechanisms; a personalized action module that enables the\nagent to perform tool actions tailored to the user. At the core, the persona\n(defined as unique system prompt for each user) functions as an intermediary:\nit leverages insights from personalized memory to control agent actions, while\nthe outcomes of these actions in turn refine the memory. Based on the\nframework, we propose a test-time user-preference alignment strategy that\nsimulate the latest n interactions to optimize the persona prompt, ensuring\nreal-time user preference alignment through textual loss feedback between\nsimulated and ground-truth responses. Experimental evaluations demonstrate that\nPersonaAgent significantly outperforms other baseline methods by not only\npersonalizing the action space effectively but also scaling during test-time\nreal-world applications. These results underscore the feasibility and potential\nof our approach in delivering tailored, dynamic user experiences.", "AI": {"tldr": "PersonaAgent\u662f\u4e00\u4e2a\u4e2a\u6027\u5316LLM\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u8bb0\u5fc6\u548c\u884c\u52a8\u6a21\u5757\uff0c\u7ed3\u5408\u7528\u6237\u504f\u597d\u4f18\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7528\u6237\u591a\u6837\u5316\u9700\u6c42\uff0c\u56e0\u6b64\u5f00\u53d1\u4e86PersonaAgent\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u4efb\u52a1\u3002", "method": "PersonaAgent\u5305\u542b\u4e2a\u6027\u5316\u8bb0\u5fc6\u6a21\u5757\uff08\u60c5\u666f\u548c\u8bed\u4e49\u8bb0\u5fc6\uff09\u548c\u884c\u52a8\u6a21\u5757\uff0c\u901a\u8fc7\u7528\u6237\u504f\u597d\u5bf9\u9f50\u7b56\u7565\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPersonaAgent\u5728\u4e2a\u6027\u5316\u884c\u52a8\u7a7a\u95f4\u548c\u5b9e\u65f6\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PersonaAgent\u5c55\u793a\u4e86\u63d0\u4f9b\u52a8\u6001\u3001\u5b9a\u5236\u5316\u7528\u6237\u4f53\u9a8c\u7684\u53ef\u884c\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2506.06261", "pdf": "https://arxiv.org/pdf/2506.06261", "abs": "https://arxiv.org/abs/2506.06261", "authors": ["Jihwan Jeong", "Xiaoyu Wang", "Jingmin Wang", "Scott Sanner", "Pascal Poupart"], "title": "Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Offline reinforcement learning (RL) is crucial when online exploration is\ncostly or unsafe but often struggles with high epistemic uncertainty due to\nlimited data. Existing methods rely on fixed conservative policies, restricting\nadaptivity and generalization. To address this, we propose Reflect-then-Plan\n(RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach.\nRefPlan unifies uncertainty modeling and MB planning by recasting planning as\nBayesian posterior estimation. At deployment, it updates a belief over\nenvironment dynamics using real-time observations, incorporating uncertainty\ninto MB planning via marginalization. Empirical results on standard benchmarks\nshow that RefPlan significantly improves the performance of conservative\noffline RL policies. In particular, RefPlan maintains robust performance under\nhigh epistemic uncertainty and limited data, while demonstrating resilience to\nchanging environment dynamics, improving the flexibility, generalizability, and\nrobustness of offline-learned policies.", "AI": {"tldr": "RefPlan\u662f\u4e00\u79cd\u57fa\u4e8e\u53cc\u91cd\u8d1d\u53f6\u65af\u7684\u79bb\u7ebf\u6a21\u578b\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u65f6\u89c2\u6d4b\u66f4\u65b0\u73af\u5883\u52a8\u6001\u4fe1\u5ff5\uff0c\u663e\u8457\u63d0\u5347\u4fdd\u5b88\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5728\u7ebf\u63a2\u7d22\u6210\u672c\u9ad8\u6216\u4e0d\u5b89\u5168\u65f6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5e38\u56e0\u6570\u636e\u6709\u9650\u5bfc\u81f4\u9ad8\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u56fa\u5b9a\u4fdd\u5b88\u7b56\u7565\u9650\u5236\u4e86\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faRefPlan\u65b9\u6cd5\uff0c\u5c06\u89c4\u5212\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8d1d\u53f6\u65af\u540e\u9a8c\u4f30\u8ba1\uff0c\u901a\u8fc7\u5b9e\u65f6\u89c2\u6d4b\u66f4\u65b0\u73af\u5883\u52a8\u6001\u4fe1\u5ff5\uff0c\u5e76\u5c06\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u6a21\u578b\u89c4\u5212\u4e2d\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRefPlan\u663e\u8457\u63d0\u5347\u4e86\u4fdd\u5b88\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u6027\u80fd\uff0c\u5728\u9ad8\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u6709\u9650\u6570\u636e\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "RefPlan\u63d0\u9ad8\u4e86\u79bb\u7ebf\u5b66\u4e60\u7b56\u7565\u7684\u7075\u6d3b\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u73af\u5883\u52a8\u6001\u53d8\u5316\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2303.14005", "pdf": "https://arxiv.org/pdf/2303.14005", "abs": "https://arxiv.org/abs/2303.14005", "authors": ["Chi Xie", "Fangao Zeng", "Yue Hu", "Shuang Liang", "Yichen Wei"], "title": "Category Query Learning for Human-Object Interaction Classification", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2023", "summary": "Unlike most previous HOI methods that focus on learning better human-object\nfeatures, we propose a novel and complementary approach called category query\nlearning. Such queries are explicitly associated to interaction categories,\nconverted to image specific category representation via a transformer decoder,\nand learnt via an auxiliary image-level classification task. This idea is\nmotivated by an earlier multi-label image classification method, but is for the\nfirst time applied for the challenging human-object interaction classification\ntask. Our method is simple, general and effective. It is validated on three\nrepresentative HOI baselines and achieves new state-of-the-art results on two\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7c7b\u522b\u67e5\u8be2\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u4eba-\u7269\u4ea4\u4e92\u5206\u7c7b\u4efb\u52a1\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5b66\u4e60\u66f4\u597d\u7684\u4eba-\u7269\u7279\u5f81\uff0c\u800c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e92\u8865\u7684\u7c7b\u522b\u67e5\u8be2\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u5173\u8054\u4ea4\u4e92\u7c7b\u522b\u7684\u67e5\u8be2\uff0c\u5229\u7528Transformer\u89e3\u7801\u5668\u5c06\u5176\u8f6c\u6362\u4e3a\u56fe\u50cf\u7279\u5b9a\u7684\u7c7b\u522b\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u8f85\u52a9\u7684\u56fe\u50cf\u7ea7\u5206\u7c7b\u4efb\u52a1\u8fdb\u884c\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027HOI\u57fa\u7ebf\u65b9\u6cd5\u4e0a\u9a8c\u8bc1\uff0c\u5e76\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u901a\u7528\u4e14\u6709\u6548\uff0c\u9996\u6b21\u5e94\u7528\u4e8e\u4eba-\u7269\u4ea4\u4e92\u5206\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2506.05171", "pdf": "https://arxiv.org/pdf/2506.05171", "abs": "https://arxiv.org/abs/2506.05171", "authors": ["Linxuan He", "Qing-Shan Jia", "Ang Li", "Hongyan Sang", "Ling Wang", "Jiwen Lu", "Tao Zhang", "Jie Zhou", "Yi Zhang", "Yisen Wang", "Peng Wei", "Zhongyuan Wang", "Henry X. Liu", "Shuo Feng"], "title": "Towards provable probabilistic safety for scalable embodied AI systems", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": null, "summary": "Embodied AI systems, comprising AI models and physical plants, are\nincreasingly prevalent across various applications. Due to the rarity of system\nfailures, ensuring their safety in complex operating environments remains a\nmajor challenge, which severely hinders their large-scale deployment in\nsafety-critical domains, such as autonomous vehicles, medical devices, and\nrobotics. While achieving provable deterministic safety--verifying system\nsafety across all possible scenarios--remains theoretically ideal, the rarity\nand complexity of corner cases make this approach impractical for scalable\nembodied AI systems. To address this challenge, we introduce provable\nprobabilistic safety, which aims to ensure that the residual risk of\nlarge-scale deployment remains below a predefined threshold. Instead of\nattempting exhaustive safety proof across all corner cases, this paradigm\nestablishes a probabilistic safety boundary on overall system performance,\nleveraging statistical methods to enhance feasibility and scalability. A\nwell-defined probabilistic safety boundary enables embodied AI systems to be\ndeployed at scale while allowing for continuous refinement of safety\nguarantees. Our work focuses on three core questions: what is provable\nprobabilistic safety, how to prove the probabilistic safety, and how to achieve\nthe provable probabilistic safety. By bridging the gap between theoretical\nsafety assurance and practical deployment, our work offers a pathway toward\nsafer, large-scale adoption of embodied AI systems in safety-critical\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8bc1\u660e\u7684\u6982\u7387\u5b89\u5168\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5177\u8eabAI\u7cfb\u7edf\u5728\u5927\u89c4\u6a21\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\u786e\u4fdd\u6b8b\u4f59\u98ce\u9669\u4f4e\u4e8e\u9884\u8bbe\u9608\u503c\u3002", "motivation": "\u5177\u8eabAI\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u96be\u4ee5\u4fdd\u8bc1\uff0c\u5c24\u5176\u662f\u6781\u7aef\u60c5\u51b5\u4e0b\u7684\u5b89\u5168\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5927\u89c4\u6a21\u5e94\u7528\u3002", "method": "\u5f15\u5165\u53ef\u8bc1\u660e\u7684\u6982\u7387\u5b89\u5168\u6982\u5ff5\uff0c\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\u5efa\u7acb\u6982\u7387\u5b89\u5168\u8fb9\u754c\uff0c\u800c\u975e\u7a77\u4e3e\u6240\u6709\u6781\u7aef\u60c5\u51b5\u3002", "result": "\u8be5\u65b9\u6cd5\u4e3a\u5177\u8eabAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5927\u89c4\u6a21\u90e8\u7f72\u3002", "conclusion": "\u53ef\u8bc1\u660e\u7684\u6982\u7387\u5b89\u5168\u65b9\u6cd5\u586b\u8865\u4e86\u7406\u8bba\u5b89\u5168\u4fdd\u8bc1\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u5177\u8eabAI\u7cfb\u7edf\u7684\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2506.05351", "pdf": "https://arxiv.org/pdf/2506.05351", "abs": "https://arxiv.org/abs/2506.05351", "authors": ["Rukmal Weerawarana", "Maxwell Braun"], "title": "Infinite Time Turing Machines and their Applications", "categories": ["cs.CC", "cs.AI", "cs.FL", "cs.LG"], "comment": "Published by Ren XYZ Inc", "summary": "This work establishes a rigorous theoretical foundation for analyzing deep\nlearning systems by leveraging Infinite Time Turing Machines (ITTMs), which\nextend classical computation into transfinite ordinal steps. Using ITTMs, we\nreinterpret modern architectures like Transformers, revealing fundamental\nlimitations in scalability, efficiency, and interpretability. Building on these\ninsights, we propose the Universal State Machine (USM), a novel computational\nparadigm designed from first principles. The USM employs a dynamic, queryable\ncomputation graph that evolves in real time, enabling modular, interpretable,\nand resource-efficient computation. This framework not only overcomes the\ninefficiencies and rigidity of current models but also lays the groundwork for\nscalable, generalizable artificial intelligence systems.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u65e0\u9650\u65f6\u95f4\u56fe\u7075\u673a\uff08ITTMs\uff09\u4e3a\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63ed\u793a\u4e86Transformer\u7b49\u73b0\u4ee3\u67b6\u6784\u5728\u53ef\u6269\u5c55\u6027\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u8303\u5f0f\u2014\u2014\u901a\u7528\u72b6\u6001\u673a\uff08USM\uff09\u3002", "motivation": "\u4e3a\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u53ef\u6269\u5c55\u6027\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u65e0\u9650\u65f6\u95f4\u56fe\u7075\u673a\uff08ITTMs\uff09\u5206\u6790\u73b0\u4ee3\u67b6\u6784\uff0c\u63d0\u51fa\u52a8\u6001\u53ef\u67e5\u8be2\u8ba1\u7b97\u56fe\u7684\u901a\u7528\u72b6\u6001\u673a\uff08USM\uff09\u3002", "result": "USM\u514b\u670d\u4e86\u73b0\u6709\u6a21\u578b\u7684\u4f4e\u6548\u6027\u548c\u521a\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u901a\u7528\u7684AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "USM\u662f\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u6a21\u5757\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u8ba1\u7b97\u8303\u5f0f\uff0c\u4e3a\u672a\u6765AI\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.05358", "pdf": "https://arxiv.org/pdf/2506.05358", "abs": "https://arxiv.org/abs/2506.05358", "authors": ["Souradip Nath"], "title": "Can ChatGPT Perform Image Splicing Detection? A Preliminary Study", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) like GPT-4V are capable of reasoning\nacross text and image modalities, showing promise in a variety of complex\nvision-language tasks. In this preliminary study, we investigate the\nout-of-the-box capabilities of GPT-4V in the domain of image forensics,\nspecifically, in detecting image splicing manipulations. Without any\ntask-specific fine-tuning, we evaluate GPT-4V using three prompting strategies:\nZero-Shot (ZS), Few-Shot (FS), and Chain-of-Thought (CoT), applied over a\ncurated subset of the CASIA v2.0 splicing dataset.\n  Our results show that GPT-4V achieves competitive detection performance in\nzero-shot settings (more than 85% accuracy), with CoT prompting yielding the\nmost balanced trade-off across authentic and spliced images. Qualitative\nanalysis further reveals that the model not only detects low-level visual\nartifacts but also draws upon real-world contextual knowledge such as object\nscale, semantic consistency, and architectural facts, to identify implausible\ncomposites. While GPT-4V lags behind specialized state-of-the-art splicing\ndetection models, its generalizability, interpretability, and encyclopedic\nreasoning highlight its potential as a flexible tool in image forensics.", "AI": {"tldr": "GPT-4V\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u8272\uff08\u51c6\u786e\u7387\u8d8585%\uff09\uff0cCoT\u63d0\u793a\u7b56\u7565\u6548\u679c\u6700\u4f73\uff0c\u4f46\u7565\u900a\u4e8e\u4e13\u7528\u68c0\u6d4b\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22GPT-4V\u5728\u56fe\u50cf\u53d6\u8bc1\u9886\u57df\u7684\u73b0\u6210\u80fd\u529b\uff0c\u5c24\u5176\u662f\u68c0\u6d4b\u56fe\u50cf\u62fc\u63a5\u7be1\u6539\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u601d\u7ef4\u94fe\uff09\u5728CASIA v2.0\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30GPT-4V\u3002", "result": "\u96f6\u6837\u672c\u51c6\u786e\u7387\u8d8585%\uff0cCoT\u7b56\u7565\u8868\u73b0\u6700\u5747\u8861\uff0c\u6a21\u578b\u7ed3\u5408\u4f4e\u5c42\u89c6\u89c9\u7279\u5f81\u548c\u4e0a\u4e0b\u6587\u77e5\u8bc6\u3002", "conclusion": "GPT-4V\u867d\u4e0d\u53ca\u4e13\u7528\u6a21\u578b\uff0c\u4f46\u5176\u901a\u7528\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u63a8\u7406\u80fd\u529b\u4f7f\u5176\u6210\u4e3a\u56fe\u50cf\u53d6\u8bc1\u7684\u7075\u6d3b\u5de5\u5177\u3002"}}
{"id": "2506.05368", "pdf": "https://arxiv.org/pdf/2506.05368", "abs": "https://arxiv.org/abs/2506.05368", "authors": ["Valentine Bernasconi", "Gustavo Marfia"], "title": "Speaking images. A novel framework for the automated self-description of artworks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent breakthroughs in generative AI have opened the door to new research\nperspectives in the domain of art and cultural heritage, where a large number\nof artifacts have been digitized. There is a need for innovation to ease the\naccess and highlight the content of digital collections. Such innovations\ndevelop into creative explorations of the digital image in relation to its\nmalleability and contemporary interpretation, in confrontation to the original\nhistorical object. Based on the concept of the autonomous image, we propose a\nnew framework towards the production of self-explaining cultural artifacts\nusing open-source large-language, face detection, text-to-speech and\naudio-to-animation models. The goal is to start from a digitized artwork and to\nautomatically assemble a short video of the latter where the main character\nanimates to explain its content. The whole process questions cultural biases\nencapsulated in large-language models, the potential of digital images and\ndeepfakes of artworks for educational purposes, along with concerns of the\nfield of art history regarding such creative diversions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u751f\u6210\u5f0fAI\u6280\u672f\u81ea\u52a8\u751f\u6210\u81ea\u89e3\u91ca\u6587\u5316\u827a\u672f\u54c1\u89c6\u9891\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u3001\u4eba\u8138\u68c0\u6d4b\u3001\u6587\u672c\u8f6c\u8bed\u97f3\u548c\u97f3\u9891\u8f6c\u52a8\u753b\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u6570\u5b57\u5316\u827a\u672f\u54c1\u6536\u85cf\u7684\u8bbf\u95ee\u548c\u5185\u5bb9\u5c55\u793a\u95ee\u9898\uff0c\u63a2\u7d22\u6570\u5b57\u56fe\u50cf\u7684\u521b\u9020\u6027\u548c\u5f53\u4ee3\u89e3\u8bfb\u6f5c\u529b\u3002", "method": "\u57fa\u4e8e\u81ea\u4e3b\u56fe\u50cf\u6982\u5ff5\uff0c\u5229\u7528\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u3001\u4eba\u8138\u68c0\u6d4b\u3001\u6587\u672c\u8f6c\u8bed\u97f3\u548c\u97f3\u9891\u8f6c\u52a8\u753b\u6a21\u578b\uff0c\u4ece\u6570\u5b57\u5316\u827a\u672f\u54c1\u751f\u6210\u81ea\u89e3\u91ca\u89c6\u9891\u3002", "result": "\u5b9e\u73b0\u4e86\u4ece\u6570\u5b57\u5316\u827a\u672f\u54c1\u81ea\u52a8\u751f\u6210\u52a8\u753b\u89c6\u9891\uff0c\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6587\u5316\u504f\u89c1\u3001\u6570\u5b57\u56fe\u50cf\u548c\u6df1\u5ea6\u4f2a\u9020\u7684\u6559\u80b2\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6587\u5316\u827a\u672f\u54c1\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u5c55\u793a\u65b9\u5f0f\uff0c\u540c\u65f6\u5f15\u53d1\u4e86\u5bf9\u827a\u672f\u53f2\u9886\u57df\u76f8\u5173\u95ee\u9898\u7684\u601d\u8003\u3002"}}
{"id": "2506.05376", "pdf": "https://arxiv.org/pdf/2506.05376", "abs": "https://arxiv.org/abs/2506.05376", "authors": ["Zifan Wang", "Christina Q. Knight", "Jeremy Kritz", "Willow E. Primack", "Julian Michael"], "title": "A Red Teaming Roadmap Towards System-Level Safety", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM) safeguards, which implement request refusals, have\nbecome a widely adopted mitigation strategy against misuse. At the intersection\nof adversarial machine learning and AI safety, safeguard red teaming has\neffectively identified critical vulnerabilities in state-of-the-art\nrefusal-trained LLMs. However, in our view the many conference submissions on\nLLM red teaming do not, in aggregate, prioritize the right research problems.\nFirst, testing against clear product safety specifications should take a higher\npriority than abstract social biases or ethical principles. Second, red teaming\nshould prioritize realistic threat models that represent the expanding risk\nlandscape and what real attackers might do. Finally, we contend that\nsystem-level safety is a necessary step to move red teaming research forward,\nas AI models present new threats as well as affordances for threat mitigation\n(e.g., detection and banning of malicious users) once placed in a deployment\ncontext. Adopting these priorities will be necessary in order for red teaming\nresearch to adequately address the slate of new threats that rapid AI advances\npresent today and will present in the very near future.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20LLM\u7ea2\u961f\u6d4b\u8bd5\u5e94\u66f4\u5173\u6ce8\u4ea7\u54c1\u5b89\u5168\u89c4\u8303\u3001\u73b0\u5b9e\u5a01\u80c1\u6a21\u578b\u548c\u7cfb\u7edf\u7ea7\u5b89\u5168\uff0c\u800c\u975e\u62bd\u8c61\u7684\u793e\u4f1a\u504f\u89c1\u6216\u4f26\u7406\u539f\u5219\u3002", "motivation": "\u5f53\u524dLLM\u7ea2\u961f\u6d4b\u8bd5\u7814\u7a76\u672a\u4f18\u5148\u89e3\u51b3\u5173\u952e\u95ee\u9898\uff0c\u5982\u4ea7\u54c1\u5b89\u5168\u89c4\u8303\u548c\u73b0\u5b9e\u5a01\u80c1\u6a21\u578b\uff0c\u672a\u80fd\u6709\u6548\u5e94\u5bf9AI\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u7684\u65b0\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u7ea2\u961f\u6d4b\u8bd5\u5e94\u805a\u7126\u4e8e\u660e\u786e\u7684\u4ea7\u54c1\u5b89\u5168\u89c4\u8303\u3001\u73b0\u5b9e\u5a01\u80c1\u6a21\u578b\u548c\u7cfb\u7edf\u7ea7\u5b89\u5168\uff0c\u800c\u975e\u62bd\u8c61\u95ee\u9898\u3002", "result": "\u5f3a\u8c03\u7ea2\u961f\u6d4b\u8bd5\u9700\u9002\u5e94\u5feb\u901f\u53d1\u5c55\u7684AI\u5a01\u80c1\uff0c\u4f18\u5148\u89e3\u51b3\u5b9e\u9645\u5b89\u5168\u95ee\u9898\u3002", "conclusion": "\u7ea2\u961f\u6d4b\u8bd5\u7814\u7a76\u9700\u8c03\u6574\u4f18\u5148\u7ea7\uff0c\u4ee5\u5e94\u5bf9AI\u6280\u672f\u5e26\u6765\u7684\u65b0\u5a01\u80c1\u548c\u6311\u6218\u3002"}}
{"id": "2506.05379", "pdf": "https://arxiv.org/pdf/2506.05379", "abs": "https://arxiv.org/abs/2506.05379", "authors": ["Seyed Moein Ayyoubzadeh", "Kourosh Shahnazari", "Mohammmadali Keshtparvar", "MohammadAmin Fazli"], "title": "Designing DSIC Mechanisms for Data Sharing in the Era of Large Language Models", "categories": ["cs.GT", "cs.AI", "cs.CY"], "comment": null, "summary": "Training large language models (LLMs) requires vast amounts of high-quality\ndata from institutions that face legal, privacy, and strategic constraints.\nExisting data procurement methods often rely on unverifiable trust or ignore\nheterogeneous provider costs. We introduce a mechanism-design framework for\ntruthful, trust-minimized data sharing that ensures dominant-strategy incentive\ncompatibility (DSIC), individual rationality, and weak budget balance, while\nrewarding data based on both quality and learning utility. We formalize a model\nwhere providers privately know their data cost and quality, and value arises\nsolely from the data's contribution to model performance. Based on this, we\npropose the Quality-Weighted Marginal-Incentive Auction (Q-MIA), which ranks\nproviders using a virtual cost metric and uses Myerson-style payments to ensure\nDSIC and budget feasibility. To support settings with limited liquidity or\nlong-term incentives, we introduce the Marginal Utility Token (MUT), which\nallocates future rights based on marginal contributions. We unify these in\nMixed-MIA, a hybrid mechanism balancing upfront payments and deferred rewards.\nAll mechanisms support verifiable, privacy-preserving implementation.\nTheoretically and empirically, they outperform volume-based and trust-based\nbaselines, eliciting higher-quality data under budget constraints while\nremaining robust to misreporting and collusion. This establishes a principled\nfoundation for sustainable and fair data markets for future LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5236\u8bbe\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u771f\u5b9e\u3001\u6700\u5c0f\u5316\u4fe1\u4efb\u7684\u6570\u636e\u5171\u4eab\uff0c\u786e\u4fdd\u6fc0\u52b1\u517c\u5bb9\u6027\u3001\u4e2a\u4f53\u5408\u7406\u6027\u548c\u9884\u7b97\u5e73\u8861\uff0c\u540c\u65f6\u57fa\u4e8e\u6570\u636e\u8d28\u91cf\u548c\u5b66\u4e60\u6548\u7528\u8fdb\u884c\u5956\u52b1\u3002", "motivation": "\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e0d\u53ef\u9a8c\u8bc1\u7684\u4fe1\u4efb\u6216\u5ffd\u7565\u63d0\u4f9b\u8005\u6210\u672c\u5dee\u5f02\uff0c\u5bfc\u81f4\u6570\u636e\u83b7\u53d6\u6548\u7387\u4f4e\u3002", "method": "\u63d0\u51fa\u4e86Quality-Weighted Marginal-Incentive Auction (Q-MIA)\u548cMarginal Utility Token (MUT)\u673a\u5236\uff0c\u7ed3\u5408\u865a\u62df\u6210\u672c\u6307\u6807\u548cMyerson\u652f\u4ed8\u65b9\u5f0f\uff0c\u786e\u4fdd\u6fc0\u52b1\u517c\u5bb9\u6027\u548c\u9884\u7b97\u53ef\u884c\u6027\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u8868\u660e\uff0c\u8fd9\u4e9b\u673a\u5236\u4f18\u4e8e\u57fa\u4e8e\u6570\u91cf\u6216\u4fe1\u4efb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u5728\u9884\u7b97\u7ea6\u675f\u4e0b\u83b7\u53d6\u66f4\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u5e76\u5bf9\u8bef\u62a5\u548c\u5171\u8c0b\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u4e3a\u672a\u6765\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u5e02\u573a\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u4e14\u516c\u5e73\u7684\u539f\u5219\u6027\u57fa\u7840\u3002"}}
{"id": "2506.05382", "pdf": "https://arxiv.org/pdf/2506.05382", "abs": "https://arxiv.org/abs/2506.05382", "authors": ["Francesco Panebianco", "Mario D'Onghia", "Stefano Zanero aand Michele Carminati"], "title": "How stealthy is stealthy? Studying the Efficacy of Black-Box Adversarial Attacks in the Real World", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Deep learning systems, critical in domains like autonomous vehicles, are\nvulnerable to adversarial examples (crafted inputs designed to mislead\nclassifiers). This study investigates black-box adversarial attacks in computer\nvision. This is a realistic scenario, where attackers have query-only access to\nthe target model. Three properties are introduced to evaluate attack\nfeasibility: robustness to compression, stealthiness to automatic detection,\nand stealthiness to human inspection. State-of-the-Art methods tend to\nprioritize one criterion at the expense of others. We propose ECLIPSE, a novel\nattack method employing Gaussian blurring on sampled gradients and a local\nsurrogate model. Comprehensive experiments on a public dataset highlight\nECLIPSE's advantages, demonstrating its contribution to the trade-off between\nthe three properties.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aECLIPSE\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u6a21\u7cca\u548c\u5c40\u90e8\u66ff\u4ee3\u6a21\u578b\u4f18\u5316\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u6613\u53d7\u5bf9\u6297\u6837\u672c\u653b\u51fb\uff0c\u9ed1\u76d2\u653b\u51fb\u662f\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u91cd\u8981\u95ee\u9898\u3002", "method": "\u63d0\u51faECLIPSE\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u65af\u6a21\u7cca\u5904\u7406\u91c7\u6837\u68af\u5ea6\u548c\u5c40\u90e8\u66ff\u4ee3\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eECLIPSE\u5728\u538b\u7f29\u9c81\u68d2\u6027\u3001\u81ea\u52a8\u68c0\u6d4b\u9690\u853d\u6027\u548c\u4eba\u7c7b\u9690\u853d\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002", "conclusion": "ECLIPSE\u4e3a\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u5316\u4e86\u591a\u76ee\u6807\u6743\u8861\u3002"}}
{"id": "2506.05384", "pdf": "https://arxiv.org/pdf/2506.05384", "abs": "https://arxiv.org/abs/2506.05384", "authors": ["Zhuoxuan Cai", "Jian Zhang", "Xinbin Yuan", "Pengtao Jiang", "Wenxiang Chen", "Bowen Tang", "Lujian Yao", "Qiyuan Wang", "Jinwen Chen", "Bo Li"], "title": "Q-Ponder: A Unified Training Pipeline for Reasoning-based Visual Quality Assessment", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Recent studies demonstrate that multimodal large language models (MLLMs) can\nproficiently evaluate visual quality through interpretable assessments.\nHowever, existing approaches typically treat quality scoring and reasoning\ndescriptions as separate tasks with disjoint optimization objectives, leading\nto a trade-off: models adept at quality reasoning descriptions struggle with\nprecise score regression, while score-focused models lack interpretability.\nThis limitation hinders the full potential of MLLMs in visual quality\nassessment, where accuracy and interpretability should be mutually reinforcing.\nTo address this, we propose a unified two-stage training framework comprising a\ncold-start stage and a reinforcement learning-based fine-tuning stage.\nSpecifically, in the first stage, we distill high-quality data from a teacher\nmodel through expert-designed prompts, initializing reasoning capabilities via\ncross-entropy loss supervision. In the second stage, we introduce a novel\nreward with Group Relative Policy Optimization (GRPO) to jointly optimize\nscoring accuracy and reasoning consistency. We designate the models derived\nfrom these two stages as Q-Ponder-CI and Q-Ponder. Extensive experiments show\nthat Q-Ponder achieves state-of-the-art (SOTA) performance on quality score\nregression benchmarks, delivering up to 6.5% higher SRCC on cross-domain\ndatasets. Furthermore, Q-Ponder significantly outperforms description-based\nSOTA models, including its teacher model Qwen-2.5-VL-72B, particularly in\ndescription accuracy and reasonableness, demonstrating the generalization\npotential over diverse tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u51b7\u542f\u52a8\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\u4e2d\u8bc4\u5206\u4e0e\u89e3\u91ca\u4efb\u52a1\u5206\u79bb\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u8d28\u91cf\u8bc4\u5206\u548c\u89e3\u91ca\u4efb\u52a1\u5206\u5f00\u4f18\u5316\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u8bc4\u5206\u51c6\u786e\u6027\u548c\u89e3\u91ca\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u51b7\u542f\u52a8\u9636\u6bb5\u901a\u8fc7\u4e13\u5bb6\u8bbe\u8ba1\u7684\u63d0\u793a\u4ece\u6559\u5e08\u6a21\u578b\u84b8\u998f\u6570\u636e\uff1b\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u9636\u6bb5\u5f15\u5165\u65b0\u5956\u52b1\u673a\u5236\u8054\u5408\u4f18\u5316\u8bc4\u5206\u51c6\u786e\u6027\u548c\u89e3\u91ca\u4e00\u81f4\u6027\u3002", "result": "Q-Ponder\u5728\u8d28\u91cf\u8bc4\u5206\u56de\u5f52\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u8de8\u57df\u6570\u636e\u96c6SRCC\u63d0\u53476.5%\uff0c\u4e14\u5728\u89e3\u91ca\u51c6\u786e\u6027\u548c\u5408\u7406\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u63cf\u8ff0\u7684SOTA\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u8bc4\u5206\u4e0e\u89e3\u91ca\u4efb\u52a1\u7684\u534f\u540c\u4f18\u5316\uff0c\u5c55\u793a\u4e86\u5728\u591a\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u6f5c\u529b\u3002"}}
{"id": "2506.05386", "pdf": "https://arxiv.org/pdf/2506.05386", "abs": "https://arxiv.org/abs/2506.05386", "authors": ["Lo Pang-Yun Ting", "Chengshuai Zhao", "Yu-Hua Zeng", "Yuan Jee Lim", "Kun-Ta Chuang"], "title": "Beyond RAG: Reinforced Reasoning Augmented Generation for Clinical Notes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Clinical note generation aims to automatically produce free-text summaries of\na patient's condition and diagnostic process, with discharge instructions being\na representative long-form example. While recent large language model\n(LLM)-based methods pre-trained on general clinical corpora show promise in\nclinical text generation, they fall short in producing long-form notes from\nlimited patient information. In this paper, we propose R2AG, the first\nreinforced retriever for long-form discharge instruction generation based on\npre-admission data. R2AG is trained with reinforcement learning to retrieve\nreasoning paths from a medical knowledge graph, providing explicit semantic\nguidance to the LLM. To bridge the information gap, we propose Group-Based\nRetriever Optimization (GRO) which improves retrieval quality with\ngroup-relative rewards, encouraging reasoning leaps for deeper inference by the\nLLM. Comprehensive experiments on the MIMIC-IV-Note dataset show that R2AG\noutperforms baselines in both clinical efficacy and natural language generation\nmetrics. Further analysis reveals that R2AG fills semantic gaps in sparse input\nscenarios, and retrieved reasoning paths help LLMs avoid clinical\nmisinterpretation by focusing on key evidence and following coherent reasoning.", "AI": {"tldr": "R2AG\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u68c0\u7d22\u5668\uff0c\u7528\u4e8e\u4ece\u5165\u9662\u524d\u6570\u636e\u751f\u6210\u957f\u7bc7\u51fa\u9662\u6307\u5bfc\uff0c\u901a\u8fc7\u68c0\u7d22\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u7684\u63a8\u7406\u8def\u5f84\u4e3aLLM\u63d0\u4f9b\u8bed\u4e49\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u4ece\u6709\u9650\u60a3\u8005\u4fe1\u606f\u751f\u6210\u957f\u7bc7\u4e34\u5e8a\u7b14\u8bb0\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faR2AG\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548cGRO\u4f18\u5316\u68c0\u7d22\u8d28\u91cf\uff0c\u9f13\u52b1LLM\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\u3002", "result": "\u5728MIMIC-IV-Note\u6570\u636e\u96c6\u4e0a\uff0cR2AG\u5728\u4e34\u5e8a\u6548\u679c\u548c\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "R2AG\u586b\u8865\u4e86\u7a00\u758f\u8f93\u5165\u573a\u666f\u7684\u8bed\u4e49\u7a7a\u767d\uff0c\u5e2e\u52a9LLM\u907f\u514d\u4e34\u5e8a\u8bef\u89e3\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.05387", "pdf": "https://arxiv.org/pdf/2506.05387", "abs": "https://arxiv.org/abs/2506.05387", "authors": ["Jaydip Sen", "Saptarshi Sengupta. Subhasis Dasgupta"], "title": "Advancing Decoding Strategies: Enhancements in Locally Typical Sampling for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "This is the accepted but pre-reviewed version of the chapter that has\n  been accepted for publication in the Springer volume 'Decision-Making in\n  Computational Intelligence-Based Systems,' edited by Witold Pedrycz, Gilberto\n  Rivera, Rose Ma Rodriguez, and Salvador Ibarra Martinez. The chapter is 39\n  pages long, and it contains 2 figures and 6 tables. This is NOT the final\n  camera-ready version", "summary": "This chapter explores advancements in decoding strategies for large language\nmodels (LLMs), focusing on enhancing the Locally Typical Sampling (LTS)\nalgorithm. Traditional decoding methods, such as top-k and nucleus sampling,\noften struggle to balance fluency, diversity, and coherence in text generation.\nTo address these challenges, Adaptive Semantic-Aware Typicality Sampling (ASTS)\nis proposed as an improved version of LTS, incorporating dynamic entropy\nthresholding, multi-objective scoring, and reward-penalty adjustments. ASTS\nensures contextually coherent and diverse text generation while maintaining\ncomputational efficiency. Its performance is evaluated across multiple\nbenchmarks, including story generation and abstractive summarization, using\nmetrics such as perplexity, MAUVE, and diversity scores. Experimental results\ndemonstrate that ASTS outperforms existing sampling techniques by reducing\nrepetition, enhancing semantic alignment, and improving fluency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5c40\u90e8\u5178\u578b\u91c7\u6837\u7b97\u6cd5\uff08ASTS\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u71b5\u9608\u503c\u3001\u591a\u76ee\u6807\u8bc4\u5206\u548c\u5956\u52b1\u60e9\u7f5a\u8c03\u6574\uff0c\u63d0\u5347\u4e86\u6587\u672c\u751f\u6210\u7684\u6d41\u7545\u6027\u3001\u591a\u6837\u6027\u548c\u8fde\u8d2f\u6027\u3002", "motivation": "\u4f20\u7edf\u89e3\u7801\u65b9\u6cd5\uff08\u5982top-k\u548c\u6838\u91c7\u6837\uff09\u5728\u5e73\u8861\u6587\u672c\u751f\u6210\u7684\u6d41\u7545\u6027\u3001\u591a\u6837\u6027\u548c\u8fde\u8d2f\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u8bed\u4e49\u611f\u77e5\u5178\u578b\u6027\u91c7\u6837\uff08ASTS\uff09\uff0c\u7ed3\u5408\u52a8\u6001\u71b5\u9608\u503c\u3001\u591a\u76ee\u6807\u8bc4\u5206\u548c\u5956\u52b1\u60e9\u7f5a\u8c03\u6574\uff0c\u4f18\u5316\u89e3\u7801\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cASTS\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u91c7\u6837\u6280\u672f\uff0c\u51cf\u5c11\u4e86\u91cd\u590d\uff0c\u589e\u5f3a\u4e86\u8bed\u4e49\u5bf9\u9f50\u548c\u6d41\u7545\u6027\u3002", "conclusion": "ASTS\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u7801\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u8d28\u91cf\u6587\u672c\u751f\u6210\u7684\u4efb\u52a1\u3002"}}
{"id": "2506.05397", "pdf": "https://arxiv.org/pdf/2506.05397", "abs": "https://arxiv.org/abs/2506.05397", "authors": ["Jerrin Bright", "Zhibo Wang", "Yuhao Chen", "Sirisha Rambhatla", "John Zelek", "David Clausi"], "title": "Gen4D: Synthesizing Humans and Scenes in the Wild", "categories": ["cs.GR", "cs.AI"], "comment": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) Workshops", "summary": "Lack of input data for in-the-wild activities often results in low\nperformance across various computer vision tasks. This challenge is\nparticularly pronounced in uncommon human-centric domains like sports, where\nreal-world data collection is complex and impractical. While synthetic datasets\noffer a promising alternative, existing approaches typically suffer from\nlimited diversity in human appearance, motion, and scene composition due to\ntheir reliance on rigid asset libraries and hand-crafted rendering pipelines.\nTo address this, we introduce Gen4D, a fully automated pipeline for generating\ndiverse and photorealistic 4D human animations. Gen4D integrates expert-driven\nmotion encoding, prompt-guided avatar generation using diffusion-based Gaussian\nsplatting, and human-aware background synthesis to produce highly varied and\nlifelike human sequences. Based on Gen4D, we present SportPAL, a large-scale\nsynthetic dataset spanning three sports: baseball, icehockey, and soccer.\nTogether, Gen4D and SportPAL provide a scalable foundation for constructing\nsynthetic datasets tailored to in-the-wild human-centric vision tasks, with no\nneed for manual 3D modeling or scene design.", "AI": {"tldr": "Gen4D\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u751f\u6210\u591a\u6837\u53164D\u4eba\u7c7b\u52a8\u753b\u7684\u7ba1\u9053\uff0c\u7ed3\u5408\u8fd0\u52a8\u7f16\u7801\u3001\u6269\u6563\u6a21\u578b\u548c\u80cc\u666f\u5408\u6210\uff0c\u521b\u5efa\u4e86SportPAL\u5408\u6210\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u91ce\u5916\u4eba\u7c7b\u6d3b\u52a8\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u91ce\u5916\u4eba\u7c7b\u6d3b\u52a8\u6570\u636e\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u4f53\u80b2\u7b49\u4e0d\u5e38\u89c1\u9886\u57df\uff0c\u73b0\u6709\u5408\u6210\u65b9\u6cd5\u56e0\u4f9d\u8d56\u56fa\u5b9a\u8d44\u4ea7\u5e93\u548c\u624b\u5de5\u6e32\u67d3\u800c\u7f3a\u4e4f\u591a\u6837\u6027\u3002", "method": "Gen4D\u6574\u5408\u4e86\u8fd0\u52a8\u7f16\u7801\u3001\u6269\u6563\u6a21\u578b\u9a71\u52a8\u7684\u5934\u50cf\u751f\u6210\u548c\u4eba\u7c7b\u611f\u77e5\u80cc\u666f\u5408\u6210\uff0c\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u4e14\u903c\u771f\u76844D\u52a8\u753b\u3002", "result": "\u57fa\u4e8eGen4D\u6784\u5efa\u4e86SportPAL\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u68d2\u7403\u3001\u51b0\u7403\u548c\u8db3\u7403\u4e09\u79cd\u8fd0\u52a8\uff0c\u4e3a\u91ce\u5916\u4eba\u7c7b\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u57fa\u7840\u3002", "conclusion": "Gen4D\u548cSportPAL\u4e3a\u65e0\u9700\u624b\u5de5\u5efa\u6a21\u7684\u5408\u6210\u6570\u636e\u96c6\u6784\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u4eba\u7c7b\u89c6\u89c9\u4efb\u52a1\u3002"}}
{"id": "2506.05399", "pdf": "https://arxiv.org/pdf/2506.05399", "abs": "https://arxiv.org/abs/2506.05399", "authors": ["Israa A. Albadarneh", "Bassam H. Hammo", "Omar S. Al-Kadi"], "title": "Attention-based transformer models for image captioning across languages: An in-depth survey and evaluation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "31 pages, 15 figures, 6 tables", "summary": "Image captioning involves generating textual descriptions from input images,\nbridging the gap between computer vision and natural language processing.\nRecent advancements in transformer-based models have significantly improved\ncaption generation by leveraging attention mechanisms for better scene\nunderstanding. While various surveys have explored deep learning-based\napproaches for image captioning, few have comprehensively analyzed\nattention-based transformer models across multiple languages. This survey\nreviews attention-based image captioning models, categorizing them into\ntransformer-based, deep learning-based, and hybrid approaches. It explores\nbenchmark datasets, discusses evaluation metrics such as BLEU, METEOR, CIDEr,\nand ROUGE, and highlights challenges in multilingual captioning. Additionally,\nthis paper identifies key limitations in current models, including semantic\ninconsistencies, data scarcity in non-English languages, and limitations in\nreasoning ability. Finally, we outline future research directions, such as\nmultimodal learning, real-time applications in AI-powered assistants,\nhealthcare, and forensic analysis. This survey serves as a comprehensive\nreference for researchers aiming to advance the field of attention-based image\ncaptioning.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u6a21\u578b\uff0c\u5206\u7c7b\u4e3a\u57fa\u4e8eTransformer\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u6df7\u5408\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u591a\u8bed\u8a00\u63cf\u8ff0\u4e2d\u7684\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u586b\u8865\u73b0\u6709\u7efc\u8ff0\u5bf9\u591a\u8bed\u8a00\u6ce8\u610f\u529b\u6a21\u578b\u5206\u6790\u7684\u4e0d\u8db3\uff0c\u63a8\u52a8\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u5206\u7c7b\u5206\u6790\u57fa\u4e8eTransformer\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u6df7\u5408\u65b9\u6cd5\uff0c\u8bc4\u4f30\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6307\u6807\uff08\u5982BLEU\u3001METEOR\u7b49\uff09\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5982\u8bed\u4e49\u4e0d\u4e00\u81f4\u3001\u975e\u82f1\u8bed\u6570\u636e\u7a00\u7f3a\u548c\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u591a\u6a21\u6001\u5b66\u4e60\u548c\u5b9e\u65f6\u5e94\u7528\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u5168\u9762\u53c2\u8003\u3002"}}
{"id": "2506.05404", "pdf": "https://arxiv.org/pdf/2506.05404", "abs": "https://arxiv.org/abs/2506.05404", "authors": ["Lianming Huang", "Haibo Hu", "Yufei Cui", "Jiacheng Zuo", "Shangyu Wu", "Nan Guan", "Chun Jason Xue"], "title": "AD-EE: Early Exiting for Fast and Reliable Vision-Language Models in Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages", "summary": "With the rapid advancement of autonomous driving, deploying Vision-Language\nModels (VLMs) to enhance perception and decision-making has become increasingly\ncommon. However, the real-time application of VLMs is hindered by high latency\nand computational overhead, limiting their effectiveness in time-critical\ndriving scenarios. This challenge is particularly evident when VLMs exhibit\nover-inference, continuing to process unnecessary layers even after confident\npredictions have been reached. To address this inefficiency, we propose AD-EE,\nan Early Exit framework that incorporates domain characteristics of autonomous\ndriving and leverages causal inference to identify optimal exit layers. We\nevaluate our method on large-scale real-world autonomous driving datasets,\nincluding Waymo and the corner-case-focused CODA, as well as on a real vehicle\nrunning the Autoware Universe platform. Extensive experiments across multiple\nVLMs show that our method significantly reduces latency, with maximum\nimprovements reaching up to 57.58%, and enhances object detection accuracy,\nwith maximum gains of up to 44%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAD-EE\u7684\u65e9\u671f\u9000\u51fa\u6846\u67b6\uff0c\u7528\u4e8e\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5ef6\u8fdf\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5b9e\u65f6\u5e94\u7528\u53d7\u5230\u9ad8\u5ef6\u8fdf\u548c\u8ba1\u7b97\u5f00\u9500\u7684\u9650\u5236\uff0c\u5c24\u5176\u662f\u5728\u65f6\u95f4\u5173\u952e\u7684\u9a7e\u9a76\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51faAD-EE\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7279\u6027\u548c\u56e0\u679c\u63a8\u7406\uff0c\u786e\u5b9a\u6700\u4f73\u9000\u51fa\u5c42\u3002", "result": "\u5728Waymo\u548cCODA\u7b49\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u5ef6\u8fdf\u6700\u591a\u51cf\u5c1157.58%\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u6700\u9ad8\u63d0\u534744%\u3002", "conclusion": "AD-EE\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.05413", "pdf": "https://arxiv.org/pdf/2506.05413", "abs": "https://arxiv.org/abs/2506.05413", "authors": ["Patrik Czak\u00f3", "G\u00e1bor Kert\u00e9sz", "S\u00e1ndor Sz\u00e9n\u00e1si"], "title": "SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 3 figures, 5 tables. Submitted to the IEEE SMC 2025\n  conference", "summary": "We present SmoothRot, a novel post-training quantization technique to enhance\nthe efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot\naddresses the critical challenge of massive activation outliers, by integrating\nchannel-wise scaling with Hadamard transformations. Our technique effectively\ntransforms extreme outliers into quantization-friendly activations,\nsignificantly improving quantization accuracy. Experiments conducted on popular\nLLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot\nconsistently reduces the performance gap between quantized and FP16 models by\napproximately 10-30\\% across language generation and zero-shot reasoning tasks,\nwithout introducing additional inference latency. Code is available at\nhttps://github.com/czakop/smoothrot.", "AI": {"tldr": "SmoothRot\u662f\u4e00\u79cd\u65b0\u9896\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u6280\u672f\uff0c\u901a\u8fc7\u7ed3\u5408\u901a\u9053\u7ea7\u7f29\u653e\u548cHadamard\u53d8\u6362\uff0c\u6709\u6548\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d4\u4f4d\u91cf\u5316\u7684\u6fc0\u6d3b\u5f02\u5e38\u503c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u91cf\u5316\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3LLM\u4e2d4\u4f4d\u91cf\u5316\u65f6\u6fc0\u6d3b\u5f02\u5e38\u503c\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u901a\u9053\u7ea7\u7f29\u653e\u548cHadamard\u53d8\u6362\uff0c\u5c06\u6781\u7aef\u5f02\u5e38\u503c\u8f6c\u5316\u4e3a\u9002\u5408\u91cf\u5316\u7684\u6fc0\u6d3b\u3002", "result": "\u5728LLaMA2 7B\u3001LLaMA3.1 8B\u548cMistral 7B\u7b49\u6a21\u578b\u4e0a\uff0c\u91cf\u5316\u4e0eFP16\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u7f29\u5c0f\u4e8610-30%\uff0c\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "SmoothRot\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u91cf\u5316\u6027\u80fd\u3002"}}
{"id": "2506.05414", "pdf": "https://arxiv.org/pdf/2506.05414", "abs": "https://arxiv.org/abs/2506.05414", "authors": ["Mingfei Chen", "Zijun Cui", "Xiulong Liu", "Jinlin Xiang", "Caleb Zheng", "Jingyuan Li", "Eli Shlizerman"], "title": "SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "Project website with demo videos: https://zijuncui02.github.io/SAVVY/", "summary": "3D spatial reasoning in dynamic, audio-visual environments is a cornerstone\nof human cognition yet remains largely unexplored by existing Audio-Visual\nLarge Language Models (AV-LLMs) and benchmarks, which predominantly focus on\nstatic or 2D scenes. We introduce SAVVY-Bench, the first benchmark for 3D\nspatial reasoning in dynamic scenes with synchronized spatial audio.\nSAVVY-Bench is comprised of thousands of relationships involving static and\nmoving objects, and requires fine-grained temporal grounding, consistent 3D\nlocalization, and multi-modal annotation. To tackle this challenge, we propose\nSAVVY, a novel training-free reasoning pipeline that consists of two stages:\n(i) Egocentric Spatial Tracks Estimation, which leverages AV-LLMs as well as\nother audio-visual methods to track the trajectories of key objects related to\nthe query using both visual and spatial audio cues, and (ii) Dynamic Global Map\nConstruction, which aggregates multi-modal queried object trajectories and\nconverts them into a unified global dynamic map. Using the constructed map, a\nfinal QA answer is obtained through a coordinate transformation that aligns the\nglobal map with the queried viewpoint. Empirical evaluation demonstrates that\nSAVVY substantially enhances performance of state-of-the-art AV-LLMs, setting a\nnew standard and stage for approaching dynamic 3D spatial reasoning in AV-LLMs.", "AI": {"tldr": "SAVVY-Bench\u662f\u9996\u4e2a\u9488\u5bf9\u52a8\u6001\u573a\u666f\u4e2d3D\u7a7a\u95f4\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408\u540c\u6b65\u7a7a\u95f4\u97f3\u9891\uff0c\u63d0\u51fa\u4e86SAVVY\u8bad\u7ec3\u514d\u8d39\u63a8\u7406\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86AV-LLMs\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AV-LLMs\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u62162D\u573a\u666f\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u60013D\u7a7a\u95f4\u63a8\u7406\u7684\u63a2\u7d22\u3002", "method": "SAVVY\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u5229\u7528AV-LLMs\u8ddf\u8e2a\u5173\u952e\u5bf9\u8c61\u8f68\u8ff9\uff1b2) \u6784\u5efa\u5168\u5c40\u52a8\u6001\u5730\u56fe\u5e76\u901a\u8fc7\u5750\u6807\u8f6c\u6362\u56de\u7b54\u95ee\u9898\u3002", "result": "SAVVY\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709AV-LLMs\u7684\u6027\u80fd\uff0c\u4e3a\u52a8\u60013D\u7a7a\u95f4\u63a8\u7406\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "SAVVY-Bench\u548cSAVVY\u65b9\u6cd5\u586b\u8865\u4e86\u52a8\u60013D\u7a7a\u95f4\u63a8\u7406\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.05416", "pdf": "https://arxiv.org/pdf/2506.05416", "abs": "https://arxiv.org/abs/2506.05416", "authors": ["David Zagardo"], "title": "FERRET: Private Deep Learning Faster And Better Than DPSGD", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "28 pages, 6 figures", "summary": "We revisit 1-bit gradient compression through the lens of mutual-information\ndifferential privacy (MI-DP). Building on signSGD, we propose FERRET--Fast and\nEffective Restricted Release for Ethical Training--which transmits at most one\nsign bit per parameter group with Bernoulli masking.\n  Theory: We prove each fired group leaks at most ln 2 nats; after subsampling\nwith rate s, the total privacy loss of G groups trained for T steps with firing\nprobability p is epsilon = G * T * s * p * ln 2. Thus FERRET achieves MI-DP for\nepsilon in [0.1, 2] without additive noise.\n  Practice: We evaluate three granularities--FERRET-MAX (finest), FERRET-EIGHTH\n(medium), and FERRET-2 (coarsest)--on five LLMs (137M-1.8B parameters) against\nDPSGD and Non-DP baselines. All methods trained for 1, 3, and 5 epochs.\n  Utility: Across all settings, FERRET-MAX/EIGHTH beat DPSGD's perplexity. At\nepsilon=0.5, 5 epochs: FERRET-EIGHTH achieves 3.98 perplexity vs DPSGD's 11.61\n(2.9x better), within 23% of Non-DP (3.25).\n  Privacy: MI-AUC stays at chance for FERRET-MAX/EIGHTH (~0.51), matching DPSGD\nvs Non-DP's 0.76-0.99. FERRET-2 shows higher leakage (~0.55) due to lower\nheadroom.\n  Efficiency: Stricter budgets fire fewer signs, so FERRET uses 19-33% of\nDPSGD's training time and only 34-36% of Non-DP training time.\n  Take-away: Sign-based MI-DP gets closer to achieving all three qualities of\nthe privacy, utility, performance trilemma: FERRET trains up to 5x faster,\nachieves 3x lower perplexity compared to DPSGD and 1.2x greater than Non-DP,\nall while providing formal, mathematically provable privacy guarantees using\nzero additive noise. The results also show that, in certain instances, masked\n1-bit updates can match non-private training utility while safeguarding data.", "AI": {"tldr": "FERRET\u662f\u4e00\u79cd\u57fa\u4e8e1\u4f4d\u68af\u5ea6\u538b\u7f29\u548c\u4e92\u4fe1\u606f\u5dee\u5206\u9690\u79c1\uff08MI-DP\uff09\u7684\u9ad8\u6548\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7Bernoulli\u63a9\u7801\u4f20\u8f93\u7b26\u53f7\u4f4d\uff0c\u5728\u9690\u79c1\u3001\u6548\u7528\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u68af\u5ea6\u538b\u7f29\u4e2d\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8bad\u7ec3\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51faFERRET\u65b9\u6cd5\uff0c\u901a\u8fc7Bernoulli\u63a9\u7801\u4f20\u8f93\u7b26\u53f7\u4f4d\uff0c\u7ed3\u5408\u4e92\u4fe1\u606f\u5dee\u5206\u9690\u79c1\u7406\u8bba\uff0c\u5206\u6790\u9690\u79c1\u6cc4\u9732\u548c\u6548\u7528\u3002", "result": "FERRET\u5728\u591a\u4e2aLLM\u4e0a\u8868\u73b0\u4f18\u4e8eDPSGD\uff0c\u9690\u79c1\u4fdd\u62a4\u6548\u679c\u63a5\u8fd1\u975e\u9690\u79c1\u57fa\u7ebf\uff0c\u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "FERRET\u5728\u9690\u79c1\u3001\u6548\u7528\u548c\u6027\u80fd\u4e09\u65b9\u9762\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u9ad8\u6548\u9690\u79c1\u4fdd\u62a4\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.05418", "pdf": "https://arxiv.org/pdf/2506.05418", "abs": "https://arxiv.org/abs/2506.05418", "authors": ["Kyungsoo Kim", "Jeongsoo Ha", "Yusung Kim"], "title": "Self-Predictive Dynamics for Generalization of Vision-based Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "IJCAI 2022", "summary": "Vision-based reinforcement learning requires efficient and robust\nrepresentations of image-based observations, especially when the images contain\ndistracting (task-irrelevant) elements such as shadows, clouds, and light. It\nbecomes more important if those distractions are not exposed during training.\nWe design a Self-Predictive Dynamics (SPD) method to extract task-relevant\nfeatures efficiently, even in unseen observations after training. SPD uses weak\nand strong augmentations in parallel, and learns representations by predicting\ninverse and forward transitions across the two-way augmented versions. In a set\nof MuJoCo visual control tasks and an autonomous driving task (CARLA), SPD\noutperforms previous studies in complex observations, and significantly\nimproves the generalization performance for unseen observations. Our code is\navailable at https://github.com/unigary/SPD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9884\u6d4b\u52a8\u6001\uff08SPD\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u4e2d\u9ad8\u6548\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf\u5305\u542b\u5e72\u6270\u5143\u7d20\u65f6\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u4e2d\u56fe\u50cf\u89c2\u5bdf\u56e0\u5e72\u6270\u5143\u7d20\uff08\u5982\u9634\u5f71\u3001\u4e91\u3001\u5149\u7ebf\uff09\u5bfc\u81f4\u7684\u7279\u5f81\u63d0\u53d6\u6548\u7387\u4f4e\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "SPD\u65b9\u6cd5\u901a\u8fc7\u5e76\u884c\u4f7f\u7528\u5f31\u589e\u5f3a\u548c\u5f3a\u589e\u5f3a\uff0c\u5e76\u901a\u8fc7\u9884\u6d4b\u53cc\u5411\u589e\u5f3a\u7248\u672c\u4e4b\u95f4\u7684\u9006\u5411\u548c\u6b63\u5411\u8f6c\u6362\u6765\u5b66\u4e60\u8868\u793a\u3002", "result": "\u5728MuJoCo\u89c6\u89c9\u63a7\u5236\u4efb\u52a1\u548cCARLA\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\uff0cSPD\u5728\u590d\u6742\u89c2\u5bdf\u4e2d\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u7814\u7a76\uff0c\u663e\u8457\u63d0\u5347\u4e86\u672a\u89c1\u89c2\u5bdf\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "SPD\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff0c\u63d0\u5347\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u548c\u672a\u89c1\u89c2\u5bdf\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.05419", "pdf": "https://arxiv.org/pdf/2506.05419", "abs": "https://arxiv.org/abs/2506.05419", "authors": ["Jeongsoo Ha", "Kyungsoo Kim", "Yusung Kim"], "title": "Dream to Generalize: Zero-Shot Model-Based Reinforcement Learning for Unseen Visual Distractions", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI 2023", "summary": "Model-based reinforcement learning (MBRL) has been used to efficiently solve\nvision-based control tasks in highdimensional image observations. Although\nrecent MBRL algorithms perform well in trained observations, they fail when\nfaced with visual distractions in observations. These task-irrelevant\ndistractions (e.g., clouds, shadows, and light) may be constantly present in\nreal-world scenarios. In this study, we propose a novel self-supervised method,\nDream to Generalize (Dr. G), for zero-shot MBRL. Dr. G trains its encoder and\nworld model with dual contrastive learning which efficiently captures\ntask-relevant features among multi-view data augmentations. We also introduce a\nrecurrent state inverse dynamics model that helps the world model to better\nunderstand the temporal structure. The proposed methods can enhance the\nrobustness of the world model against visual distractions. To evaluate the\ngeneralization performance, we first train Dr. G on simple backgrounds and then\ntest it on complex natural video backgrounds in the DeepMind Control suite, and\nthe randomizing environments in Robosuite. Dr. G yields a performance\nimprovement of 117% and 14% over prior works, respectively. Our code is\nopen-sourced and available at https://github.com/JeongsooHa/DrG.git", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDr. G\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u7528\u4e8e\u96f6\u6837\u672cMBRL\uff0c\u901a\u8fc7\u53cc\u5bf9\u6bd4\u5b66\u4e60\u548c\u9012\u5f52\u72b6\u6001\u9006\u52a8\u529b\u5b66\u6a21\u578b\u63d0\u5347\u6a21\u578b\u5bf9\u89c6\u89c9\u5e72\u6270\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3MBRL\u5728\u9762\u5bf9\u89c6\u89c9\u5e72\u6270\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u4efb\u52a1\u65e0\u5173\u5e72\u6270\u3002", "method": "\u91c7\u7528\u53cc\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u7f16\u7801\u5668\u548c\u4e16\u754c\u6a21\u578b\uff0c\u5f15\u5165\u9012\u5f52\u72b6\u6001\u9006\u52a8\u529b\u5b66\u6a21\u578b\u4ee5\u7406\u89e3\u65f6\u95f4\u7ed3\u6784\u3002", "result": "\u5728DeepMind Control suite\u548cRobosuite\u4e2d\uff0cDr. G\u5206\u522b\u6bd4\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u63d0\u5347117%\u548c14%\u3002", "conclusion": "Dr. G\u663e\u8457\u63d0\u5347\u4e86MBRL\u5728\u89c6\u89c9\u5e72\u6270\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73b0\u5b9e\u573a\u666f\u3002"}}
{"id": "2506.05425", "pdf": "https://arxiv.org/pdf/2506.05425", "abs": "https://arxiv.org/abs/2506.05425", "authors": ["Fanqi Kong", "Weiqin Zu", "Xinyu Chen", "Yaodong Yang", "Song-Chun Zhu", "Xue Feng"], "title": "SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The rich and multifaceted nature of human social interaction, encompassing\nmultimodal cues, unobservable relations and mental states, and dynamical\nbehavior, presents a formidable challenge for artificial intelligence. To\nadvance research in this area, we introduce SIV-Bench, a novel video benchmark\nfor rigorously evaluating the capabilities of Multimodal Large Language Models\n(MLLMs) across Social Scene Understanding (SSU), Social State Reasoning (SSR),\nand Social Dynamics Prediction (SDP). SIV-Bench features 2,792 video clips and\n8,792 meticulously generated question-answer pairs derived from a human-LLM\ncollaborative pipeline. It is originally collected from TikTok and YouTube,\ncovering a wide range of video genres, presentation styles, and linguistic and\ncultural backgrounds. It also includes a dedicated setup for analyzing the\nimpact of different textual cues-original on-screen text, added dialogue, or no\ntext. Our comprehensive experiments on leading MLLMs reveal that while models\nadeptly handle SSU, they significantly struggle with SSR and SDP, where\nRelation Inference (RI) is an acute bottleneck, as further examined in our\nanalysis. Our study also confirms the critical role of transcribed dialogue in\naiding comprehension of complex social interactions. By systematically\nidentifying current MLLMs' strengths and limitations, SIV-Bench offers crucial\ninsights to steer the development of more socially intelligent AI. The dataset\nand code are available at https://kfq20.github.io/sivbench/.", "AI": {"tldr": "SIV-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u793e\u4ea4\u573a\u666f\u7406\u89e3\u3001\u793e\u4ea4\u72b6\u6001\u63a8\u7406\u548c\u793e\u4ea4\u52a8\u6001\u9884\u6d4b\u65b9\u9762\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u6a21\u578b\u5728\u793e\u4ea4\u72b6\u6001\u63a8\u7406\u548c\u52a8\u6001\u9884\u6d4b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5173\u7cfb\u63a8\u7406\u662f\u4e3b\u8981\u74f6\u9888\u3002", "motivation": "\u4eba\u7c7b\u793e\u4ea4\u4e92\u52a8\u7684\u590d\u6742\u6027\u5bf9\u4eba\u5de5\u667a\u80fd\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u667a\u80fd\u7684\u6a21\u578b\u6765\u7406\u89e3\u548c\u63a8\u7406\u793e\u4ea4\u573a\u666f\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b-LLM\u534f\u4f5c\u6d41\u7a0b\u751f\u62102,792\u4e2a\u89c6\u9891\u7247\u6bb5\u548c8,792\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u8986\u76d6\u591a\u79cd\u89c6\u9891\u7c7b\u578b\u548c\u6587\u5316\u80cc\u666f\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u6587\u672c\u63d0\u793a\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u5728\u793e\u4ea4\u573a\u666f\u7406\u89e3\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u793e\u4ea4\u72b6\u6001\u63a8\u7406\u548c\u52a8\u6001\u9884\u6d4b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5173\u7cfb\u63a8\u7406\u662f\u4e3b\u8981\u74f6\u9888\u3002\u8f6c\u5f55\u5bf9\u8bdd\u5bf9\u7406\u89e3\u590d\u6742\u793e\u4ea4\u4e92\u52a8\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "SIV-Bench\u4e3a\u5f00\u53d1\u66f4\u5177\u793e\u4ea4\u667a\u80fd\u7684AI\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2506.05426", "pdf": "https://arxiv.org/pdf/2506.05426", "abs": "https://arxiv.org/abs/2506.05426", "authors": ["Wenhao Wu", "Fuhong Liu", "Haoru Li", "Zican Hu", "Daoyi Dong", "Chunlin Chen", "Zhi Wang"], "title": "Mixture-of-Experts Meets In-Context Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "26 pages, 13 figures", "summary": "In-context reinforcement learning (ICRL) has emerged as a promising paradigm\nfor adapting RL agents to downstream tasks through prompt conditioning.\nHowever, two notable challenges remain in fully harnessing in-context learning\nwithin RL domains: the intrinsic multi-modality of the state-action-reward data\nand the diverse, heterogeneous nature of decision tasks. To tackle these\nchallenges, we propose \\textbf{T2MIR} (\\textbf{T}oken- and \\textbf{T}ask-wise\n\\textbf{M}oE for \\textbf{I}n-context \\textbf{R}L), an innovative framework that\nintroduces architectural advances of mixture-of-experts (MoE) into\ntransformer-based decision models. T2MIR substitutes the feedforward layer with\ntwo parallel layers: a token-wise MoE that captures distinct semantics of input\ntokens across multiple modalities, and a task-wise MoE that routes diverse\ntasks to specialized experts for managing a broad task distribution with\nalleviated gradient conflicts. To enhance task-wise routing, we introduce a\ncontrastive learning method that maximizes the mutual information between the\ntask and its router representation, enabling more precise capture of\ntask-relevant information. The outputs of two MoE components are concatenated\nand fed into the next layer. Comprehensive experiments show that T2MIR\nsignificantly facilitates in-context learning capacity and outperforms various\ntypes of baselines. We bring the potential and promise of MoE to ICRL, offering\na simple and scalable architectural enhancement to advance ICRL one step closer\ntoward achievements in language and vision communities. Our code is available\nat https://github.com/NJU-RL/T2MIR.", "AI": {"tldr": "T2MIR\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4ee4\u724c\u7ea7\u548c\u4efb\u52a1\u7ea7MoE\u5c42\u89e3\u51b3ICRL\u4e2d\u7684\u591a\u6a21\u6001\u548c\u4efb\u52a1\u591a\u6837\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "ICRL\u5728\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u65f6\u9762\u4e34\u72b6\u6001-\u52a8\u4f5c-\u5956\u52b1\u6570\u636e\u7684\u591a\u6a21\u6001\u6027\u548c\u4efb\u52a1\u591a\u6837\u6027\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "T2MIR\u91c7\u7528\u4ee4\u724c\u7ea7\u548c\u4efb\u52a1\u7ea7MoE\u5c42\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u4f18\u5316\u4efb\u52a1\u8def\u7531\uff0c\u589e\u5f3a\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\u7684\u6355\u6349\u3002", "result": "\u5b9e\u9a8c\u8868\u660eT2MIR\u663e\u8457\u63d0\u5347\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "T2MIR\u4e3aICRL\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u67b6\u6784\u6539\u8fdb\uff0c\u63a8\u52a8\u4e86ICRL\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2506.05427", "pdf": "https://arxiv.org/pdf/2506.05427", "abs": "https://arxiv.org/abs/2506.05427", "authors": ["Zishan Shu", "Yufan Deng", "Hongyu Zhang", "Zhiwei Nie", "Jie Chen"], "title": "MTPNet: Multi-Grained Target Perception for Unified Activity Cliff Prediction", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "Activity cliff prediction is a critical task in drug discovery and material\ndesign. Existing computational methods are limited to handling single binding\ntargets, which restricts the applicability of these prediction models. In this\npaper, we present the Multi-Grained Target Perception network (MTPNet) to\nincorporate the prior knowledge of interactions between the molecules and their\ntarget proteins. Specifically, MTPNet is a unified framework for activity cliff\nprediction, which consists of two components: Macro-level Target Semantic (MTS)\nguidance and Micro-level Pocket Semantic (MPS) guidance. By this way, MTPNet\ndynamically optimizes molecular representations through multi-grained protein\nsemantic conditions. To our knowledge, it is the first time to employ the\nreceptor proteins as guiding information to effectively capture critical\ninteraction details. Extensive experiments on 30 representative activity cliff\ndatasets demonstrate that MTPNet significantly outperforms previous approaches,\nachieving an average RMSE improvement of 18.95% on top of several mainstream\nGNN architectures. Overall, MTPNet internalizes interaction patterns through\nconditional deep learning to achieve unified predictions of activity cliffs,\nhelping to accelerate compound optimization and design. Codes are available at:\nhttps://github.com/ZishanShu/MTPNet.", "AI": {"tldr": "MTPNet\u662f\u4e00\u79cd\u65b0\u578b\u7f51\u7edc\uff0c\u7528\u4e8e\u591a\u7c92\u5ea6\u76ee\u6807\u611f\u77e5\u7684\u6d3b\u52a8\u60ac\u5d16\u9884\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u4e00\u7ed3\u5408\u76ee\u6807\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u6a21\u578b\u7684\u9002\u7528\u6027\u3002", "method": "MTPNet\u7ed3\u5408\u5b8f\u89c2\u76ee\u6807\u8bed\u4e49\uff08MTS\uff09\u548c\u5fae\u89c2\u53e3\u888b\u8bed\u4e49\uff08MPS\uff09\u6307\u5bfc\uff0c\u52a8\u6001\u4f18\u5316\u5206\u5b50\u8868\u793a\u3002", "result": "\u572830\u4e2a\u4ee3\u8868\u6027\u6570\u636e\u96c6\u4e0a\uff0cMTPNet\u5e73\u5747RMSE\u63d0\u534718.95%\u3002", "conclusion": "MTPNet\u901a\u8fc7\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u7edf\u4e00\u9884\u6d4b\u6d3b\u52a8\u60ac\u5d16\uff0c\u52a0\u901f\u5316\u5408\u7269\u4f18\u5316\u4e0e\u8bbe\u8ba1\u3002"}}
{"id": "2506.05428", "pdf": "https://arxiv.org/pdf/2506.05428", "abs": "https://arxiv.org/abs/2506.05428", "authors": ["Zhihao Tang", "Chaozhuo Li", "Litian Zhang", "Xi Zhang"], "title": "Diffusion with a Linguistic Compass: Steering the Generation of Clinically Plausible Future sMRI Representations for Early MCI Conversion Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Early prediction of Mild Cognitive Impairment (MCI) conversion is hampered by\na trade-off between immediacy--making fast predictions from a single baseline\nsMRI--and accuracy--leveraging longitudinal scans to capture disease\nprogression. We propose MCI-Diff, a diffusion-based framework that synthesizes\nclinically plausible future sMRI representations directly from baseline data,\nachieving both real-time risk assessment and high predictive performance.\nFirst, a multi-task sequence reconstruction strategy trains a shared denoising\nnetwork on interpolation and extrapolation tasks to handle irregular follow-up\nsampling and learn robust latent trajectories. Second, an LLM-driven\n\"linguistic compass\" is introduced for clinical plausibility sampling:\ngenerated feature candidates are quantized, tokenized, and scored by a\nfine-tuned language model conditioned on expected structural biomarkers,\nguiding autoregressive generation toward realistic disease patterns.\nExperiments on ADNI and AIBL cohorts show that MCI-Diff outperforms\nstate-of-the-art baselines, improving early conversion accuracy by 5-12%.", "AI": {"tldr": "MCI-Diff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u672a\u6765sMRI\u56fe\u50cf\u5b9e\u73b0MCI\u65e9\u671f\u9884\u6d4b\uff0c\u517c\u987e\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u65e9\u671f\u9884\u6d4bMCI\u8f6c\u5316\u9762\u4e34\u5373\u65f6\u6027\uff08\u5355\u6b21\u57fa\u7ebfsMRI\uff09\u4e0e\u51c6\u786e\u6027\uff08\u7eb5\u5411\u626b\u63cf\uff09\u7684\u6743\u8861\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5e8f\u5217\u91cd\u5efa\u7b56\u7565\u8bad\u7ec3\u5171\u4eab\u53bb\u566a\u7f51\u7edc\uff0c\u7ed3\u5408LLM\u9a71\u52a8\u7684\u8bed\u8a00\u6307\u5357\u786e\u4fdd\u4e34\u5e8a\u5408\u7406\u6027\u3002", "result": "\u5728ADNI\u548cAIBL\u6570\u636e\u96c6\u4e0a\uff0cMCI-Diff\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u65e9\u671f\u8f6c\u5316\u51c6\u786e\u7387\u63d0\u53475-12%\u3002", "conclusion": "MCI-Diff\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u9ad8\u7cbe\u5ea6\u7684MCI\u8f6c\u5316\u9884\u6d4b\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.05429", "pdf": "https://arxiv.org/pdf/2506.05429", "abs": "https://arxiv.org/abs/2506.05429", "authors": ["Ashwin Ramesh Babu", "Sajad Mousavi", "Vineet Gundecha", "Sahand Ghorbanpour", "Avisek Naug", "Antonio Guillen", "Ricardo Luna Gutierrez", "Soumyendu Sarkar"], "title": "Coordinated Robustness Evaluation Framework for Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted: IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW) 2025", "summary": "Vision-language models, which integrate computer vision and natural language\nprocessing capabilities, have demonstrated significant advancements in tasks\nsuch as image captioning and visual question and answering. However, similar to\ntraditional models, they are susceptible to small perturbations, posing a\nchallenge to their robustness, particularly in deployment scenarios. Evaluating\nthe robustness of these models requires perturbations in both the vision and\nlanguage modalities to learn their inter-modal dependencies. In this work, we\ntrain a generic surrogate model that can take both image and text as input and\ngenerate joint representation which is further used to generate adversarial\nperturbations for both the text and image modalities. This coordinated attack\nstrategy is evaluated on the visual question and answering and visual reasoning\ndatasets using various state-of-the-art vision-language models. Our results\nindicate that the proposed strategy outperforms other multi-modal attacks and\nsingle-modality attacks from the recent literature. Our results demonstrate\ntheir effectiveness in compromising the robustness of several state-of-the-art\npre-trained multi-modal models such as instruct-BLIP, ViLT and others.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u751f\u6210\u5bf9\u6297\u6270\u52a8\uff0c\u8bc4\u4f30\u4e86\u5176\u5728\u89c6\u89c9\u95ee\u7b54\u548c\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u7b56\u7565\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u63cf\u8ff0\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5bf9\u5fae\u5c0f\u6270\u52a8\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u90e8\u7f72\u573a\u666f\u4e2d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u6a21\u6001\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2a\u901a\u7528\u4ee3\u7406\u6a21\u578b\uff0c\u63a5\u6536\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165\u5e76\u751f\u6210\u8054\u5408\u8868\u793a\uff0c\u8fdb\u4e00\u6b65\u7528\u4e8e\u751f\u6210\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u7684\u5bf9\u6297\u6270\u52a8\u3002\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u95ee\u7b54\u548c\u89c6\u89c9\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u7684\u534f\u8c03\u653b\u51fb\u7b56\u7565\u5728\u591a\u79cd\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982instruct-BLIP\u3001ViLT\u7b49\uff09\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u591a\u6a21\u6001\u548c\u5355\u6a21\u6001\u653b\u51fb\u65b9\u6cd5\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u5728\u591a\u6a21\u6001\u6270\u52a8\u4e0b\u7684\u8106\u5f31\u6027\u3002"}}
{"id": "2506.05430", "pdf": "https://arxiv.org/pdf/2506.05430", "abs": "https://arxiv.org/abs/2506.05430", "authors": ["Mingjie Chen", "Tiancheng Zhu", "Mingxue Zhang", "Yiling He", "Minghao Lin", "Penghui Li", "Kui Ren"], "title": "Explainer-guided Targeted Adversarial Attacks against Binary Code Similarity Detection Models", "categories": ["cs.CR", "cs.AI"], "comment": "12 pages, 3 figures", "summary": "Binary code similarity detection (BCSD) serves as a fundamental technique for\nvarious software engineering tasks, e.g., vulnerability detection and\nclassification. Attacks against such models have therefore drawn extensive\nattention, aiming at misleading the models to generate erroneous predictions.\nPrior works have explored various approaches to generating semantic-preserving\nvariants, i.e., adversarial samples, to evaluate the robustness of the models\nagainst adversarial attacks. However, they have mainly relied on heuristic\ncriteria or iterative greedy algorithms to locate salient code influencing the\nmodel output, failing to operate on a solid theoretical basis. Moreover, when\nprocessing programs with high complexities, such attacks tend to be\ntime-consuming.\n  In this work, we propose a novel optimization for adversarial attacks against\nBCSD models. In particular, we aim to improve the attacks in a challenging\nscenario, where the attack goal is to limit the model predictions to a specific\nrange, i.e., the targeted attacks. Our attack leverages the superior capability\nof black-box, model-agnostic explainers in interpreting the model decision\nboundaries, thereby pinpointing the critical code snippet to apply\nsemantic-preserving perturbations. The evaluation results demonstrate that\ncompared with the state-of-the-art attacks, the proposed attacks achieve higher\nattack success rate in almost all scenarios, while also improving the\nefficiency and transferability. Our real-world case studies on vulnerability\ndetection and classification further demonstrate the security implications of\nour attacks, highlighting the urgent need to further enhance the robustness of\nexisting BCSD models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e8c\u8fdb\u5236\u4ee3\u7801\u76f8\u4f3c\u6027\u68c0\u6d4b\uff08BCSD\uff09\u6a21\u578b\u7684\u4f18\u5316\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u9ed1\u76d2\u89e3\u91ca\u5668\u5b9a\u4f4d\u5173\u952e\u4ee3\u7801\u7247\u6bb5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u8d2a\u5a6a\u7b97\u6cd5\uff0c\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u7a0b\u5e8f\u65f6\u3002\u672c\u6587\u65e8\u5728\u6539\u8fdb\u9488\u5bf9\u7279\u5b9a\u8303\u56f4\u9884\u6d4b\u7684\u653b\u51fb\u573a\u666f\u3002", "method": "\u5229\u7528\u9ed1\u76d2\u3001\u6a21\u578b\u65e0\u5173\u7684\u89e3\u91ca\u5668\u5206\u6790\u6a21\u578b\u51b3\u7b56\u8fb9\u754c\uff0c\u5b9a\u4f4d\u5173\u952e\u4ee3\u7801\u7247\u6bb5\u5e76\u8fdb\u884c\u8bed\u4e49\u4fdd\u7559\u7684\u6270\u52a8\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\uff0c\u672c\u6587\u65b9\u6cd5\u5728\u51e0\u4e4e\u6240\u6709\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6548\u7387\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u73b0\u6709BCSD\u6a21\u578b\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u6539\u8fdb\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2506.05431", "pdf": "https://arxiv.org/pdf/2506.05431", "abs": "https://arxiv.org/abs/2506.05431", "authors": ["Ashwin Ramesh Babu", "Sajad Mousavi", "Vineet Gundecha", "Sahand Ghorbanpour", "Avisek Naug", "Antonio Guillen", "Ricardo Luna Gutierrez", "Soumyendu Sarkar"], "title": "Robustness Evaluation for Video Models with Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted at the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW) 2025", "summary": "Evaluating the robustness of Video classification models is very challenging,\nspecifically when compared to image-based models. With their increased temporal\ndimension, there is a significant increase in complexity and computational\ncost. One of the key challenges is to keep the perturbations to a minimum to\ninduce misclassification. In this work, we propose a multi-agent reinforcement\nlearning approach (spatial and temporal) that cooperatively learns to identify\nthe given video's sensitive spatial and temporal regions. The agents consider\ntemporal coherence in generating fine perturbations, leading to a more\neffective and visually imperceptible attack. Our method outperforms the\nstate-of-the-art solutions on the Lp metric and the average queries. Our method\nenables custom distortion types, making the robustness evaluation more relevant\nto the use case. We extensively evaluate 4 popular models for video action\nrecognition on two popular datasets, HMDB-51 and UCF-101.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u5206\u7c7b\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u6270\u52a8\u751f\u6210\u66f4\u6709\u6548\u7684\u653b\u51fb\u3002", "motivation": "\u89c6\u9891\u5206\u7c7b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u6bd4\u56fe\u50cf\u6a21\u578b\u66f4\u5177\u6311\u6218\u6027\uff0c\u56e0\u5176\u65f6\u95f4\u7ef4\u5ea6\u589e\u52a0\u4e86\u590d\u6742\u6027\u548c\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08\u7a7a\u95f4\u548c\u65f6\u95f4\uff09\uff0c\u534f\u540c\u5b66\u4e60\u89c6\u9891\u7684\u654f\u611f\u533a\u57df\uff0c\u751f\u6210\u7ec6\u5fae\u6270\u52a8\u3002", "result": "\u5728Lp\u6307\u6807\u548c\u5e73\u5747\u67e5\u8be2\u6b21\u6570\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u5931\u771f\u7c7b\u578b\uff0c\u5e76\u5728HMDB-51\u548cUCF-101\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e864\u79cd\u6d41\u884c\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u8bc4\u4f30\u89c6\u9891\u5206\u7c7b\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4e14\u6270\u52a8\u66f4\u9690\u853d\u3002"}}
{"id": "2506.05432", "pdf": "https://arxiv.org/pdf/2506.05432", "abs": "https://arxiv.org/abs/2506.05432", "authors": ["Yuxuan Yue", "Zukang Xu", "Zhihang Yuan", "Dawei Yang", "Jianglong Wu", "Liqiang Nie"], "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) face significant challenges in edge deployment\ndue to their massive parameter scale. Vector Quantization (VQ), a\nclustering-based quantization method, serves as a prevalent solution to this\nissue for its extremely low-bit (even at 2-bit) and considerable accuracy.\nSince a vector is a quantity in mathematics and physics that has both direction\nand magnitude, existing VQ works typically quantize them in a coupled manner.\nHowever, we find that direction exhibits significantly greater sensitivity to\nquantization compared to the magnitude. For instance, when separately\nclustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the\naccuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap\neven increases with the reduction of clustering centers. Further, Euclidean\ndistance, a common metric to access vector similarities in current VQ works,\nplaces greater emphasis on reducing the magnitude error. This property is\ncontrary to the above finding, unavoidably leading to larger quantization\nerrors. To these ends, this paper proposes Polar Coordinate Decoupled Vector\nQuantization (PCDVQ), an effective and efficient VQ framework consisting of two\nkey modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors\ninto their polar coordinate representations and perform independent\nquantization of the direction and magnitude parameters.2) Distribution Aligned\nCodebook Construction (DACC), which optimizes the direction and magnitude\ncodebooks in accordance with the source distribution. Experimental results show\nthat PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\%\nzero-shot accuracy, establishing a novel paradigm for accurate and highly\ncompressed LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPCDVQ\u7684\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u65b9\u5411\u548c\u5e45\u5ea6\u7684\u91cf\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18\u90e8\u7f72\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u538b\u7f29\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5411\u91cf\u91cf\u5316\u65b9\u6cd5\u5728\u91cf\u5316\u65b9\u5411\u548c\u5e45\u5ea6\u65f6\u8026\u5408\u5904\u7406\uff0c\u5bfc\u81f4\u65b9\u5411\u91cf\u5316\u8bef\u5dee\u8f83\u5927\uff0c\u5f71\u54cd\u6a21\u578b\u7cbe\u5ea6\u3002", "method": "PCDVQ\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u6a21\u5757\uff1a\u6781\u5750\u6807\u89e3\u8026\uff08PCD\uff09\u548c\u5206\u5e03\u5bf9\u9f50\u7801\u672c\u6784\u5efa\uff08DACC\uff09\uff0c\u5206\u522b\u72ec\u7acb\u91cf\u5316\u65b9\u5411\u548c\u5e45\u5ea6\uff0c\u5e76\u4f18\u5316\u7801\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPCDVQ\u57282-bit\u91cf\u5316\u6c34\u5e73\u4e0b\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u81f3\u5c11\u63d0\u53471.5%\u7684\u96f6\u6837\u672c\u51c6\u786e\u7387\u3002", "conclusion": "PCDVQ\u4e3a\u9ad8\u538b\u7f29\u4e14\u9ad8\u7cbe\u5ea6\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.05433", "pdf": "https://arxiv.org/pdf/2506.05433", "abs": "https://arxiv.org/abs/2506.05433", "authors": ["Zikang Liu", "Tongtian Yue", "Yepeng Tang", "Longteng Guo", "Junxian Cai", "Qingbin Liu", "Xi Chen", "Jing Liu"], "title": "Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, technical report", "summary": "Group Relative Policy Optimization (GRPO) enhances policy learning by\ncomputing gradients from relative comparisons among candidate outputs that\nshare a common input prefix. Despite its effectiveness, GRPO introduces\nsubstantial computational overhead when processing long shared prefixes, which\nmust be redundantly encoded for each group member. This inefficiency becomes a\nmajor scalability bottleneck in long-context learning scenarios. We propose\nPrefix Grouper, an efficient GRPO training algorithm that eliminates redundant\nprefix computation via a Shared-Prefix Forward strategy. In particular, by\nrestructuring self-attention into two parts, our method enables the shared\nprefix to be encoded only once, while preserving full differentiability and\ncompatibility with end-to-end training. We provide both theoretical and\nempirical evidence that Prefix Grouper is training-equivalent to standard GRPO:\nit yields identical forward outputs and backward gradients, ensuring that the\noptimization dynamics and final policy performance remain unchanged.\nEmpirically, our experiments confirm that Prefix Grouper achieves consistent\nresults while significantly reducing the computational cost of training,\nparticularly in long-prefix scenarios. The proposed method is fully\nplug-and-play: it is compatible with existing GRPO-based architectures and can\nbe seamlessly integrated into current training pipelines as a drop-in\nreplacement, requiring no structural modifications and only minimal changes to\ninput construction and attention computation. Prefix Grouper enables the use of\nlarger group sizes under the same computational budget, thereby improving the\nscalability of GRPO to more complex tasks and larger models. Code is now\navailable at https://github.com/johncaged/PrefixGrouper", "AI": {"tldr": "Prefix Grouper\u662f\u4e00\u79cd\u9ad8\u6548\u7684GRPO\u8bad\u7ec3\u7b97\u6cd5\uff0c\u901a\u8fc7\u5171\u4eab\u524d\u7f00\u8ba1\u7b97\u51cf\u5c11\u5197\u4f59\uff0c\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "GRPO\u5728\u5904\u7406\u957f\u5171\u4eab\u524d\u7f00\u65f6\u5b58\u5728\u8ba1\u7b97\u5197\u4f59\u95ee\u9898\uff0c\u6210\u4e3a\u6269\u5c55\u6027\u74f6\u9888\u3002", "method": "\u901a\u8fc7Shared-Prefix Forward\u7b56\u7565\uff0c\u5c06\u81ea\u6ce8\u610f\u529b\u5206\u4e3a\u4e24\u90e8\u5206\uff0c\u5171\u4eab\u524d\u7f00\u4ec5\u7f16\u7801\u4e00\u6b21\uff0c\u4fdd\u6301\u7aef\u5230\u7aef\u8bad\u7ec3\u517c\u5bb9\u6027\u3002", "result": "Prefix Grouper\u5728\u4fdd\u6301\u8bad\u7ec3\u7b49\u6548\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u66f4\u5927\u7ec4\u89c4\u6a21\u548c\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "Prefix Grouper\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u65e0\u9700\u7ed3\u6784\u4fee\u6539\u5373\u53ef\u63d0\u5347GRPO\u7684\u6269\u5c55\u6027\u3002"}}
{"id": "2506.05434", "pdf": "https://arxiv.org/pdf/2506.05434", "abs": "https://arxiv.org/abs/2506.05434", "authors": ["Thomas Massena", "L\u00e9o and\u00e9ol", "Thibaut Boissin", "Franck Mamalet", "Corentin Friedrich", "Mathieu Serrurier", "S\u00e9bastien Gerchinovitz"], "title": "Efficient Robust Conformal Prediction via Lipschitz-Bounded Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Conformal Prediction (CP) has proven to be an effective post-hoc method for\nimproving the trustworthiness of neural networks by providing prediction sets\nwith finite-sample guarantees. However, under adversarial attacks, classical\nconformal guarantees do not hold anymore: this problem is addressed in the\nfield of Robust Conformal Prediction. Several methods have been proposed to\nprovide robust CP sets with guarantees under adversarial perturbations, but,\nfor large scale problems, these sets are either too large or the methods are\ntoo computationally demanding to be deployed in real life scenarios. In this\nwork, we propose a new method that leverages Lipschitz-bounded networks to\nprecisely and efficiently estimate robust CP sets. When combined with a\n1-Lipschitz robust network, we demonstrate that our lip-rcp method outperforms\nstate-of-the-art results in both the size of the robust CP sets and\ncomputational efficiency in medium and large-scale scenarios such as ImageNet.\nTaking a different angle, we also study vanilla CP under attack, and derive new\nworst-case coverage bounds of vanilla CP sets, which are valid simultaneously\nfor all adversarial attack levels. Our lip-rcp method makes this second\napproach as efficient as vanilla CP while also allowing robustness guarantees.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLipschitz\u7ea6\u675f\u7f51\u7edc\u7684\u65b0\u65b9\u6cd5\uff08lip-rcp\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u7cbe\u786e\u5730\u4f30\u8ba1\u9c81\u68d2\u5171\u5f62\u9884\u6d4b\u96c6\uff0c\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5171\u5f62\u9884\u6d4b\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u5931\u6548\uff0c\u73b0\u6709\u9c81\u68d2\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u9884\u6d4b\u96c6\u8fc7\u5927\uff0c\u96be\u4ee5\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5229\u7528Lipschitz\u7ea6\u675f\u7f51\u7edc\uff081-Lipschitz\u9c81\u68d2\u7f51\u7edc\uff09\u7ed3\u5408\u65b0\u65b9\u6cd5lip-rcp\uff0c\u9ad8\u6548\u751f\u6210\u9c81\u68d2\u9884\u6d4b\u96c6\u3002", "result": "lip-rcp\u5728ImageNet\u7b49\u5927\u89c4\u6a21\u573a\u666f\u4e2d\uff0c\u9884\u6d4b\u96c6\u66f4\u5c0f\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "lip-rcp\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u63d0\u4f9b\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u4e3a\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u5171\u5f62\u9884\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.05435", "pdf": "https://arxiv.org/pdf/2506.05435", "abs": "https://arxiv.org/abs/2506.05435", "authors": ["Manon Renault", "Hamoud Younes", "Hugo Tessier", "Ronan Le Roy", "Bastien Pasdeloup", "Mathieu L\u00e9onardon"], "title": "Event Classification of Accelerometer Data for Industrial Package Monitoring with Embedded Deep Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Package monitoring is an important topic in industrial applications, with\nsignificant implications for operational efficiency and ecological\nsustainability. In this study, we propose an approach that employs an embedded\nsystem, placed on reusable packages, to detect their state (on a Forklift, in a\nTruck, or in an undetermined location). We aim to design a system with a\nlifespan of several years, corresponding to the lifespan of reusable packages.\nOur analysis demonstrates that maximizing device lifespan requires minimizing\nwake time. We propose a pipeline that includes data processing, training, and\nevaluation of the deep learning model designed for imbalanced, multiclass time\nseries data collected from an embedded sensor. The method uses a\none-dimensional Convolutional Neural Network architecture to classify\naccelerometer data from the IoT device. Before training, two data augmentation\ntechniques are tested to solve the imbalance problem of the dataset: the\nSynthetic Minority Oversampling TEchnique and the ADAptive SYNthetic sampling\napproach. After training, compression techniques are implemented to have a\nsmall model size. On the considered twoclass problem, the methodology yields a\nprecision of 94.54% for the first class and 95.83% for the second class, while\ncompression techniques reduce the model size by a factor of four. The trained\nmodel is deployed on the IoT device, where it operates with a power consumption\nof 316 mW during inference.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u5305\u88c5\u72b6\u6001\u76d1\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u7c7b\u52a0\u901f\u5ea6\u8ba1\u6570\u636e\uff0c\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u4f18\u5316\u6a21\u578b\u5927\u5c0f\u548c\u529f\u8017\u3002", "motivation": "\u5de5\u4e1a\u5e94\u7528\u4e2d\u5305\u88c5\u76d1\u6d4b\u5bf9\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8bbe\u8ba1\u957f\u5bff\u547d\u7cfb\u7edf\u4ee5\u5339\u914d\u53ef\u91cd\u590d\u4f7f\u7528\u5305\u88c5\u7684\u5bff\u547d\u3002", "method": "\u4f7f\u7528\u4e00\u7ef4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u52a0\u901f\u5ea6\u8ba1\u6570\u636e\uff0c\u6d4b\u8bd5\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\u89e3\u51b3\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5e94\u7528\u538b\u7f29\u6280\u672f\u51cf\u5c0f\u6a21\u578b\u5927\u5c0f\u3002", "result": "\u6a21\u578b\u5728\u4e24\u5206\u7c7b\u95ee\u9898\u4e0a\u7cbe\u5ea6\u5206\u522b\u8fbe94.54%\u548c95.83%\uff0c\u538b\u7f29\u540e\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u56db\u500d\uff0c\u63a8\u7406\u529f\u8017\u4e3a316 mW\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u957f\u5bff\u547d\u3001\u4f4e\u529f\u8017\u7684\u5305\u88c5\u72b6\u6001\u76d1\u6d4b\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u5e94\u7528\u3002"}}
{"id": "2506.05437", "pdf": "https://arxiv.org/pdf/2506.05437", "abs": "https://arxiv.org/abs/2506.05437", "authors": ["Julien Soul\u00e9", "Jean-Paul Jamont", "Michel Occello", "Louis-Marie Traonouez", "Paul Th\u00e9ron"], "title": "A MARL-based Approach for Easing MAS Organization Engineering", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Multi-Agent Systems (MAS) have been successfully applied in industry for\ntheir ability to address complex, distributed problems, especially in IoT-based\nsystems. Their efficiency in achieving given objectives and meeting design\nrequirements is strongly dependent on the MAS organization during the\nengineering process of an application-specific MAS. To design a MAS that can\nachieve given goals, available methods rely on the designer's knowledge of the\ndeployment environment. However, high complexity and low readability in some\ndeployment environments make the application of these methods to be costly or\nraise safety concerns. In order to ease the MAS organization design regarding\nthose concerns, we introduce an original Assisted MAS Organization Engineering\nApproach (AOMEA). AOMEA relies on combining a Multi-Agent Reinforcement\nLearning (MARL) process with an organizational model to suggest relevant\norganizational specifications to help in MAS engineering.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f85\u52a9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7ec4\u7ec7\u5de5\u7a0b\u65b9\u6cd5\uff08AOMEA\uff09\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u548c\u7ec4\u7ec7\u6a21\u578b\uff0c\u4ee5\u7b80\u5316\u590d\u6742\u73af\u5883\u4e0b\u7684MAS\u8bbe\u8ba1\u3002", "motivation": "\u5728\u590d\u6742\u6216\u4f4e\u53ef\u8bfb\u6027\u7684\u90e8\u7f72\u73af\u5883\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8bbe\u8ba1\u8005\u77e5\u8bc6\uff0c\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u9700\u6539\u8fdb\u3002", "method": "AOMEA\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u548c\u7ec4\u7ec7\u6a21\u578b\uff0c\u4e3aMAS\u5de5\u7a0b\u63d0\u4f9b\u76f8\u5173\u7ec4\u7ec7\u89c4\u8303\u5efa\u8bae\u3002", "result": "AOMEA\u80fd\u591f\u6709\u6548\u8f85\u52a9MAS\u7ec4\u7ec7\u8bbe\u8ba1\uff0c\u964d\u4f4e\u590d\u6742\u73af\u5883\u4e0b\u7684\u5de5\u7a0b\u6210\u672c\u548c\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "AOMEA\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684MAS\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.05438", "pdf": "https://arxiv.org/pdf/2506.05438", "abs": "https://arxiv.org/abs/2506.05438", "authors": ["Tongda Sun", "Chen Yin", "Huailiang Zheng", "Yining Dong"], "title": "An Unsupervised Framework for Dynamic Health Indicator Construction and Its Application in Rolling Bearing Prognostics", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Health indicator (HI) plays a key role in degradation assessment and\nprognostics of rolling bearings. Although various HI construction methods have\nbeen investigated, most of them rely on expert knowledge for feature extraction\nand overlook capturing dynamic information hidden in sequential degradation\nprocesses, which limits the ability of the constructed HI for degradation trend\nrepresentation and prognostics. To address these concerns, a novel dynamic HI\nthat considers HI-level temporal dependence is constructed through an\nunsupervised framework. Specifically, a degradation feature learning module\ncomposed of a skip-connection-based autoencoder first maps raw signals to a\nrepresentative degradation feature space (DFS) to automatically extract\nessential degradation features without the need for expert knowledge.\nSubsequently, in this DFS, a new HI-generating module embedded with an inner\nHI-prediction block is proposed for dynamic HI construction, where the temporal\ndependence between past and current HI states is guaranteed and modeled\nexplicitly. On this basis, the dynamic HI captures the inherent dynamic\ncontents of the degradation process, ensuring its effectiveness for degradation\ntendency modeling and future degradation prognostics. The experiment results on\ntwo bearing lifecycle datasets demonstrate that the proposed HI construction\nmethod outperforms comparison methods, and the constructed dynamic HI is\nsuperior for prognostic tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u6001\u5065\u5eb7\u6307\u6807\uff08HI\uff09\u6784\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u6846\u67b6\u81ea\u52a8\u63d0\u53d6\u9000\u5316\u7279\u5f81\u5e76\u663e\u5f0f\u5efa\u6a21HI\u7ea7\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709HI\u6784\u5efa\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u4e14\u5ffd\u7565\u52a8\u6001\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u9000\u5316\u8d8b\u52bf\u8868\u793a\u548c\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u8df3\u8dc3\u8fde\u63a5\u7684\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u9000\u5316\u7279\u5f81\uff0c\u5e76\u5d4c\u5165HI\u9884\u6d4b\u5757\u6784\u5efa\u52a8\u6001HI\uff0c\u663e\u5f0f\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u8f74\u627f\u751f\u547d\u5468\u671f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\uff0c\u52a8\u6001HI\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u63d0\u51fa\u7684\u52a8\u6001HI\u80fd\u6709\u6548\u6355\u6349\u9000\u5316\u8fc7\u7a0b\u7684\u52a8\u6001\u5185\u5bb9\uff0c\u63d0\u5347\u9000\u5316\u8d8b\u52bf\u5efa\u6a21\u548c\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2506.05439", "pdf": "https://arxiv.org/pdf/2506.05439", "abs": "https://arxiv.org/abs/2506.05439", "authors": ["Sho Takishita", "Jay Gala", "Abdelrahman Mohamed", "Kentaro Inui", "Yova Kementchedjhieva"], "title": "LLMs Can Compensate for Deficiencies in Visual Representations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Many vision-language models (VLMs) that prove very effective at a range of\nmultimodal task, build on CLIP-based vision encoders, which are known to have\nvarious limitations. We investigate the hypothesis that the strong language\nbackbone in VLMs compensates for possibly weak visual features by\ncontextualizing or enriching them. Using three CLIP-based VLMs, we perform\ncontrolled self-attention ablations on a carefully designed probing task. Our\nfindings show that despite known limitations, CLIP visual representations offer\nready-to-read semantic information to the language decoder. However, in\nscenarios of reduced contextualization in the visual representations, the\nlanguage decoder can largely compensate for the deficiency and recover\nperformance. This suggests a dynamic division of labor in VLMs and motivates\nfuture architectures that offload more visual processing to the language\ndecoder.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cCLIP\u89c6\u89c9\u7f16\u7801\u5668\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u867d\u6709\u9650\u5236\uff0c\u4f46\u8bed\u8a00\u89e3\u7801\u5668\u80fd\u8865\u507f\u5176\u4e0d\u8db3\uff0c\u52a8\u6001\u5206\u5de5\u3002", "motivation": "\u63a2\u7a76CLIP\u89c6\u89c9\u7f16\u7801\u5668\u7684\u5c40\u9650\u6027\u662f\u5426\u88ab\u8bed\u8a00\u89e3\u7801\u5668\u8865\u507f\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u57fa\u4e8eCLIP\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "CLIP\u89c6\u89c9\u8868\u5f81\u63d0\u4f9b\u8bed\u4e49\u4fe1\u606f\uff0c\u8bed\u8a00\u89e3\u7801\u5668\u80fd\u8865\u507f\u5176\u4e0d\u8db3\u3002", "conclusion": "\u672a\u6765\u53ef\u8bbe\u8ba1\u66f4\u591a\u89c6\u89c9\u5904\u7406\u7531\u8bed\u8a00\u89e3\u7801\u5668\u627f\u62c5\u7684\u67b6\u6784\u3002"}}
{"id": "2506.05440", "pdf": "https://arxiv.org/pdf/2506.05440", "abs": "https://arxiv.org/abs/2506.05440", "authors": ["Ludovic Arnould", "Salim Khazem", "Hugues Ali Mehenni"], "title": "BYO-Eval: Build Your Own Dataset for Fine-Grained Visual Assessment of Multimodal Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Visual Language Models (VLMs) are now sufficiently advanced to support a\nbroad range of applications, including answering complex visual questions, and\nare increasingly expected to interact with images in varied ways. To evaluate\nthem, current benchmarks often focus on specific domains (e.g., reading\ncharts), constructing datasets of annotated real images paired with pre-defined\nMultiple Choice Questions (MCQs) to report aggregate accuracy scores. However,\nsuch benchmarks entail high annotation costs, risk information leakage, and do\nnot clarify whether failures stem from limitations in visual perception,\nreasoning, or general knowledge. We propose a new evaluation methodology,\ninspired by ophthalmologic diagnostics, leveraging procedural generation of\nsynthetic images to obtain control over visual attributes and precisely reveal\nperception failures in VLMs. Specifically, we build collections of images with\ngradually more challenging variations in the content of interest (e.g., number\nof objects in a counting task) while holding other visual parameters constant.\nThis diagnostic allows systematic stress testing and fine-grained failure\nanalysis, shifting the focus from coarse benchmarking toward targeted and\ninterpretable assessment of VLM capabilities. Our code is available at\nhttps://github.com/byoeval/BYO-EVAL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u89c6\u89c9\u5c5e\u6027\u7cbe\u786e\u63ed\u793a\u611f\u77e5\u5931\u8d25\uff0c\u66ff\u4ee3\u4f20\u7edf\u9ad8\u6210\u672c\u3001\u6613\u6cc4\u9732\u4fe1\u606f\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u6210\u672c\u9ad8\u3001\u6613\u6cc4\u9732\u4fe1\u606f\uff0c\u4e14\u65e0\u6cd5\u660e\u786e\u5931\u8d25\u539f\u56e0\uff08\u89c6\u89c9\u611f\u77e5\u3001\u63a8\u7406\u6216\u5e38\u8bc6\uff09\u3002", "method": "\u5229\u7528\u7a0b\u5e8f\u751f\u6210\u5408\u6210\u56fe\u50cf\uff0c\u9010\u6b65\u589e\u52a0\u5185\u5bb9\u96be\u5ea6\uff0c\u4fdd\u6301\u5176\u4ed6\u89c6\u89c9\u53c2\u6570\u4e0d\u53d8\uff0c\u8fdb\u884c\u7cfb\u7edf\u6027\u538b\u529b\u6d4b\u8bd5\u548c\u7ec6\u7c92\u5ea6\u5931\u8d25\u5206\u6790\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u9488\u5bf9\u6027\u3001\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u3001\u53ef\u63a7\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2506.05442", "pdf": "https://arxiv.org/pdf/2506.05442", "abs": "https://arxiv.org/abs/2506.05442", "authors": ["Hao Jiang", "Chuan Hu", "Yukang Shi", "Yuan He", "Ke Wang", "Xi Zhang", "Zhipeng Zhang"], "title": "Structured Labeling Enables Faster Vision-Language Models for End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) offer a promising approach to end-to-end\nautonomous driving due to their human-like reasoning capabilities. However,\ntroublesome gaps remains between current VLMs and real-world autonomous driving\napplications. One major limitation is that existing datasets with loosely\nformatted language descriptions are not machine-friendly and may introduce\nredundancy. Additionally, high computational cost and massive scale of VLMs\nhinder the inference speed and real-world deployment. To bridge the gap, this\npaper introduces a structured and concise benchmark dataset, NuScenes-S, which\nis derived from the NuScenes dataset and contains machine-friendly structured\nrepresentations. Moreover, we present FastDrive, a compact VLM baseline with\n0.9B parameters. In contrast to existing VLMs with over 7B parameters and\nunstructured language processing(e.g., LLaVA-1.5), FastDrive understands\nstructured and concise descriptions and generates machine-friendly driving\ndecisions with high efficiency. Extensive experiments show that FastDrive\nachieves competitive performance on structured dataset, with approximately 20%\naccuracy improvement on decision-making tasks, while surpassing massive\nparameter baseline in inference speed with over 10x speedup. Additionally,\nablation studies further focus on the impact of scene annotations (e.g.,\nweather, time of day) on decision-making tasks, demonstrating their importance\non decision-making tasks in autonomous driving.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u6570\u636e\u96c6NuScenes-S\u548c\u7d27\u51d1\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578bFastDrive\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5197\u4f59\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\u5b58\u5728\u5197\u4f59\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7ed3\u6784\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u7ed3\u6784\u5316\u6570\u636e\u96c6NuScenes-S\u548c\u7d27\u51d1\u578b\u6a21\u578bFastDrive\uff080.9B\u53c2\u6570\uff09\uff0c\u4e13\u6ce8\u4e8e\u673a\u5668\u53cb\u597d\u7684\u7ed3\u6784\u5316\u63cf\u8ff0\u548c\u9ad8\u6548\u63a8\u7406\u3002", "result": "FastDrive\u5728\u7ed3\u6784\u5316\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u51b3\u7b56\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u534720%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534710\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u7ed3\u6784\u5316\u6570\u636e\u548c\u7d27\u51d1\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u573a\u666f\u6807\u6ce8\u5bf9\u51b3\u7b56\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.05443", "pdf": "https://arxiv.org/pdf/2506.05443", "abs": "https://arxiv.org/abs/2506.05443", "authors": ["Yiyu Lin", "Yan Wang", "You Zhou", "Xinye Ni", "Jiahui Wu", "Sen Yang"], "title": "UniPTMs: The First Unified Multi-type PTM Site Prediction Model via Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical Contrastive Loss", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "comment": null, "summary": "As a core mechanism of epigenetic regulation in eukaryotes, protein\npost-translational modifications (PTMs) require precise prediction to decipher\ndynamic life activity networks. To address the limitations of existing deep\nlearning models in cross-modal feature fusion, domain generalization, and\narchitectural optimization, this study proposes UniPTMs: the first unified\nframework for multi-type PTM prediction. The framework innovatively establishes\na \"Master-Slave\" dual-path collaborative architecture: The master path\ndynamically integrates high-dimensional representations of protein sequences,\nstructures, and evolutionary information through a Bidirectional Gated\nCross-Attention (BGCA) module, while the slave path optimizes feature\ndiscrepancies and recalibration between structural and traditional features\nusing a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale\nAdaptive convolutional Pyramid (MACP) for capturing local feature patterns and\na Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level\nfeature integration across paths, the framework employs a Hierarchical Dynamic\nWeighting Fusion (HDWF) mechanism to intelligently aggregate multimodal\nfeatures. Enhanced by a novel Hierarchical Contrastive loss function for\nfeature consistency optimization, UniPTMs demonstrates significant performance\nimprovements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art\nmodels across five modification types and transcends the Single-Type Prediction\nParadigm. To strike a balance between model complexity and performance, we have\nalso developed a lightweight variant named UniPTMs-mini.", "AI": {"tldr": "UniPTMs\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u7c7b\u578b\u86cb\u767d\u8d28\u7ffb\u8bd1\u540e\u4fee\u9970\uff08PTM\uff09\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u8def\u5f84\u534f\u4f5c\u67b6\u6784\u548c\u591a\u79cd\u6a21\u5757\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\u3001\u9886\u57df\u6cdb\u5316\u548c\u67b6\u6784\u4f18\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684PTM\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faUniPTMs\u6846\u67b6\uff0c\u91c7\u7528\u4e3b\u4ece\u53cc\u8def\u5f84\u534f\u4f5c\u67b6\u6784\uff0c\u7ed3\u5408BGCA\u3001LDFN\u3001MACP\u3001BHGFN\u548cHDWF\u7b49\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u5206\u5c42\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\u3002", "result": "UniPTMs\u5728\u4e94\u79cd\u4fee\u9970\u7c7b\u578b\u4e0a\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08MCC\u63d0\u9ad83.2%-11.4%\uff0cAP\u63d0\u9ad84.2%-14.3%\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u53d8\u4f53UniPTMs-mini\u3002", "conclusion": "UniPTMs\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u591a\u6a21\u5757\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86PTM\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u7684\u5e73\u8861\u65b9\u6848\u3002"}}
{"id": "2506.05445", "pdf": "https://arxiv.org/pdf/2506.05445", "abs": "https://arxiv.org/abs/2506.05445", "authors": ["Thanh Vinh Vo", "Young Lee", "Haozhe Ma", "Chien Lu", "Tze-Yun Leong"], "title": "Causal Policy Learning in Reinforcement Learning: Backdoor-Adjusted Soft Actor-Critic", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Hidden confounders that influence both states and actions can bias policy\nlearning in reinforcement learning (RL), leading to suboptimal or\nnon-generalizable behavior. Most RL algorithms ignore this issue, learning\npolicies from observational trajectories based solely on statistical\nassociations rather than causal effects. We propose DoSAC (Do-Calculus Soft\nActor-Critic with Backdoor Adjustment), a principled extension of the SAC\nalgorithm that corrects for hidden confounding via causal intervention\nestimation. DoSAC estimates the interventional policy $\\pi(a | \\mathrm{do}(s))$\nusing the backdoor criterion, without requiring access to true confounders or\ncausal labels. To achieve this, we introduce a learnable Backdoor Reconstructor\nthat infers pseudo-past variables (previous state and action) from the current\nstate to enable backdoor adjustment from observational data. This module is\nintegrated into a soft actor-critic framework to compute both the\ninterventional policy and its entropy. Empirical results on continuous control\nbenchmarks show that DoSAC outperforms baselines under confounded settings,\nwith improved robustness, generalization, and policy reliability.", "AI": {"tldr": "DoSAC\u662f\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u5e72\u9884\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u540e\u95e8\u8c03\u6574\u89e3\u51b3\u9690\u85cf\u6df7\u6dc6\u53d8\u91cf\u95ee\u9898\uff0c\u63d0\u5347\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u9690\u85cf\u6df7\u6dc6\u53d8\u91cf\u4f1a\u5f71\u54cd\u72b6\u6001\u548c\u52a8\u4f5c\uff0c\u5bfc\u81f4\u7b56\u7565\u5b66\u4e60\u504f\u5dee\uff0c\u4f20\u7edfRL\u7b97\u6cd5\u5ffd\u89c6\u6b64\u95ee\u9898\uff0c\u4ec5\u4f9d\u8d56\u7edf\u8ba1\u5173\u8054\u800c\u975e\u56e0\u679c\u6548\u5e94\u3002", "method": "\u63d0\u51faDoSAC\u7b97\u6cd5\uff0c\u5229\u7528\u540e\u95e8\u51c6\u5219\u4f30\u8ba1\u5e72\u9884\u7b56\u7565\uff0c\u65e0\u9700\u771f\u5b9e\u6df7\u6dc6\u53d8\u91cf\u6216\u56e0\u679c\u6807\u7b7e\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u540e\u95e8\u91cd\u6784\u5668\u63a8\u65ad\u4f2a\u8fc7\u53bb\u53d8\u91cf\u3002", "result": "\u5728\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDoSAC\u5728\u6df7\u6dc6\u73af\u5883\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u7b56\u7565\u53ef\u9760\u6027\u3002", "conclusion": "DoSAC\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u6709\u6548\u89e3\u51b3\u9690\u85cf\u6df7\u6dc6\u95ee\u9898\uff0c\u4e3aRL\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.05446", "pdf": "https://arxiv.org/pdf/2506.05446", "abs": "https://arxiv.org/abs/2506.05446", "authors": ["Dror Ivry", "Oran Nahum"], "title": "Sentinel: SOTA model to protect against prompt injections", "categories": ["cs.CR", "cs.AI"], "comment": "6 pages, 2 tables", "summary": "Large Language Models (LLMs) are increasingly powerful but remain vulnerable\nto prompt injection attacks, where malicious inputs cause the model to deviate\nfrom its intended instructions. This paper introduces Sentinel, a novel\ndetection model, qualifire/prompt-injection-sentinel, based on the\n\\answerdotai/ModernBERT-large architecture. By leveraging ModernBERT's advanced\nfeatures and fine-tuning on an extensive and diverse dataset comprising a few\nopen-source and private collections, Sentinel achieves state-of-the-art\nperformance. This dataset amalgamates varied attack types, from role-playing\nand instruction hijacking to attempts to generate biased content, alongside a\nbroad spectrum of benign instructions, with private datasets specifically\ntargeting nuanced error correction and real-world misclassifications. On a\ncomprehensive, unseen internal test set, Sentinel demonstrates an average\naccuracy of 0.987 and an F1-score of 0.980. Furthermore, when evaluated on\npublic benchmarks, it consistently outperforms strong baselines like\nprotectai/deberta-v3-base-prompt-injection-v2. This work details Sentinel's\narchitecture, its meticulous dataset curation, its training methodology, and a\nthorough evaluation, highlighting its superior detection capabilities.", "AI": {"tldr": "Sentinel\u662f\u4e00\u79cd\u57fa\u4e8eModernBERT-large\u67b6\u6784\u7684\u65b0\u578b\u68c0\u6d4b\u6a21\u578b\uff0c\u7528\u4e8e\u9632\u5fa1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6613\u53d7\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u9700\u8981\u9ad8\u6548\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528ModernBERT-large\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u5185\u90e8\u6d4b\u8bd5\u96c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u8fbe0.987\uff0cF1\u5206\u65700.980\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "Sentinel\u5728\u68c0\u6d4b\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.05447", "pdf": "https://arxiv.org/pdf/2506.05447", "abs": "https://arxiv.org/abs/2506.05447", "authors": ["Andrei Mircea", "Supriyo Chakraborty", "Nima Chitsazan", "Irina Rish", "Ekaterina Lobacheva"], "title": "Training Dynamics Underlying Language Model Scaling Laws: Loss Deceleration and Zero-Sum Learning", "categories": ["cs.LG", "cs.AI", "I.2.7"], "comment": "Published as a conference paper at ACL 2025", "summary": "This work aims to understand how scaling improves language models,\nspecifically in terms of training dynamics. We find that language models\nundergo loss deceleration early in training; an abrupt slowdown in the rate of\nloss improvement, resulting in piecewise linear behaviour of the loss curve in\nlog-log space. Scaling up the model mitigates this transition by (1) decreasing\nthe loss at which deceleration occurs, and (2) improving the log-log rate of\nloss improvement after deceleration. We attribute loss deceleration to a type\nof degenerate training dynamics we term zero-sum learning (ZSL). In ZSL,\nper-example gradients become systematically opposed, leading to destructive\ninterference in per-example changes in loss. As a result, improving loss on one\nsubset of examples degrades it on another, bottlenecking overall progress. Loss\ndeceleration and ZSL provide new insights into the training dynamics underlying\nlanguage model scaling laws, and could potentially be targeted directly to\nimprove language models independent of scale. We make our code and artefacts\navailable at: https://github.com/mirandrom/zsl", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u65e9\u671f\u4f1a\u51fa\u73b0\u635f\u5931\u51cf\u901f\u73b0\u8c61\uff0c\u6a21\u578b\u89c4\u6a21\u6269\u5927\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u73b0\u8c61\u3002", "motivation": "\u7406\u89e3\u89c4\u6a21\u6269\u5c55\u5982\u4f55\u6539\u5584\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u52a8\u6001\u3002", "method": "\u5206\u6790\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u635f\u5931\u51cf\u901f\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u96f6\u548c\u5b66\u4e60\uff08ZSL\uff09\u6982\u5ff5\u89e3\u91ca\u5176\u6210\u56e0\u3002", "result": "\u6a21\u578b\u89c4\u6a21\u6269\u5927\u80fd\u964d\u4f4e\u635f\u5931\u51cf\u901f\u53d1\u751f\u7684\u635f\u5931\u503c\uff0c\u5e76\u6539\u5584\u51cf\u901f\u540e\u7684\u635f\u5931\u6539\u8fdb\u901f\u7387\u3002", "conclusion": "\u635f\u5931\u51cf\u901f\u548cZSL\u4e3a\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u89c4\u5f8b\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u672a\u6765\u53ef\u9488\u5bf9\u6027\u4f18\u5316\u3002"}}
{"id": "2506.05453", "pdf": "https://arxiv.org/pdf/2506.05453", "abs": "https://arxiv.org/abs/2506.05453", "authors": ["Hongbo Zhao", "Fei Zhu", "Rundong Wang", "Gaofeng Meng", "Zhaoxiang Zhang"], "title": "MLLM-CL: Continual Learning for Multimodal Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent Multimodal Large Language Models (MLLMs) excel in vision-language\nunderstanding but face challenges in adapting to dynamic real-world scenarios\nthat require continuous integration of new knowledge and skills. While\ncontinual learning (CL) offers a potential solution, existing benchmarks and\nmethods suffer from critical limitations. In this paper, we introduce MLLM-CL,\na novel benchmark encompassing domain and ability continual learning, where the\nformer focuses on independently and identically distributed (IID) evaluation\nacross evolving mainstream domains, whereas the latter evaluates on non-IID\nscenarios with emerging model ability. Methodologically, we propose preventing\ncatastrophic interference through parameter isolation, along with an MLLM-based\nrouting mechanism. Extensive experiments demonstrate that our approach can\nintegrate domain-specific knowledge and functional abilities with minimal\nforgetting, significantly outperforming existing methods.", "AI": {"tldr": "MLLM-CL\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\uff0c\u901a\u8fc7\u53c2\u6570\u9694\u79bb\u548c\u8def\u7531\u673a\u5236\u89e3\u51b3\u52a8\u6001\u573a\u666f\u4e2d\u7684\u77e5\u8bc6\u6574\u5408\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u4e2d\u6574\u5408\u65b0\u77e5\u8bc6\u548c\u6280\u80fd\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u53c2\u6570\u9694\u79bb\u548cMLLM\u8def\u7531\u673a\u5236\uff0c\u9632\u6b62\u707e\u96be\u6027\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u6574\u5408\u9886\u57df\u77e5\u8bc6\u548c\u529f\u80fd\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MLLM-CL\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.05454", "pdf": "https://arxiv.org/pdf/2506.05454", "abs": "https://arxiv.org/abs/2506.05454", "authors": ["Liang Zhang", "Bingcong Li", "Kiran Koshy Thekumparampil", "Sewoong Oh", "Michael Muehlebach", "Niao He"], "title": "Zeroth-Order Optimization Finds Flat Minima", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "comment": null, "summary": "Zeroth-order methods are extensively used in machine learning applications\nwhere gradients are infeasible or expensive to compute, such as black-box\nattacks, reinforcement learning, and language model fine-tuning. Existing\noptimization theory focuses on convergence to an arbitrary stationary point,\nbut less is known on the implicit regularization that provides a fine-grained\ncharacterization on which particular solutions are finally reached. We show\nthat zeroth-order optimization with the standard two-point estimator favors\nsolutions with small trace of Hessian, which is widely used in previous work to\ndistinguish between sharp and flat minima. We further provide convergence rates\nof zeroth-order optimization to approximate flat minima for convex and\nsufficiently smooth functions, where flat minima are defined as the minimizers\nthat achieve the smallest trace of Hessian among all optimal solutions.\nExperiments on binary classification tasks with convex losses and language\nmodel fine-tuning support our theoretical findings.", "AI": {"tldr": "\u96f6\u9636\u65b9\u6cd5\u5728\u68af\u5ea6\u4e0d\u53ef\u884c\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u65f6\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u672c\u6587\u7814\u7a76\u5176\u9690\u5f0f\u6b63\u5219\u5316\u7279\u6027\uff0c\u53d1\u73b0\u6807\u51c6\u4e24\u70b9\u4f30\u8ba1\u5668\u504f\u597dHessian\u77e9\u9635\u8ff9\u5c0f\u7684\u89e3\uff0c\u5373\u5e73\u5766\u6781\u5c0f\u503c\u3002", "motivation": "\u7814\u7a76\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u5728\u9690\u5f0f\u6b63\u5219\u5316\u65b9\u9762\u7684\u7279\u6027\uff0c\u586b\u8865\u73b0\u6709\u7406\u8bba\u5bf9\u6700\u7ec8\u89e3\u9009\u62e9\u673a\u5236\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u4e24\u70b9\u4f30\u8ba1\u5668\u7684\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u5bf9Hessian\u77e9\u9635\u8ff9\u5c0f\u7684\u89e3\u7684\u504f\u597d\uff0c\u5e76\u63d0\u4f9b\u6536\u655b\u901f\u7387\u7406\u8bba\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u96f6\u9636\u4f18\u5316\u503e\u5411\u4e8e\u5e73\u5766\u6781\u5c0f\u503c\uff0c\u5b9e\u9a8c\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u548c\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u53d1\u73b0\u3002", "conclusion": "\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u9690\u5f0f\u504f\u597d\u5e73\u5766\u6781\u5c0f\u503c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2506.05490", "pdf": "https://arxiv.org/pdf/2506.05490", "abs": "https://arxiv.org/abs/2506.05490", "authors": ["Mohammed Almutairi"], "title": "Sentiment Analysis in Learning Management Systems Understanding Student Feedback at Scale", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "10 pages, 10 figures", "summary": "During the wake of the Covid-19 pandemic, the educational paradigm has\nexperienced a major change from in person learning traditional to online\nplatforms. The change of learning convention has impacted the teacher-student\nespecially in non-verbal communication. The absent of non-verbal communication\nhas led to a reliance on verbal feedback which diminished the efficacy of the\neducational experience. This paper explores the integration of sentiment\nanalysis into learning management systems (LMS) to bridge the student-teacher's\ngap by offering an alternative approach to interpreting student feedback beyond\nits verbal context. The research involves data preparation, feature selection,\nand the development of a deep neural network model encompassing word embedding,\nLSTM, and attention mechanisms. This model is compared against a logistic\nregression baseline to evaluate its efficacy in understanding student feedback.\nThe study aims to bridge the communication gap between instructors and students\nin online learning environments, offering insights into the emotional context\nof student feedback and ultimately improving the quality of online education.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u65b0\u51a0\u75ab\u60c5\u671f\u95f4\uff0c\u6559\u80b2\u6a21\u5f0f\u4ece\u9762\u5bf9\u9762\u8f6c\u5411\u5728\u7ebf\u5e73\u53f0\u540e\uff0c\u975e\u8bed\u8a00\u6c9f\u901a\u7f3a\u5931\u5bf9\u5e08\u751f\u4e92\u52a8\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u60c5\u611f\u5206\u6790\u6280\u672f\u6539\u8fdb\u5b66\u4e60\u7ba1\u7406\u7cfb\u7edf\uff08LMS\uff09\u4ee5\u5f25\u8865\u8fd9\u4e00\u6c9f\u901a\u9e3f\u6c9f\u3002", "motivation": "\u75ab\u60c5\u671f\u95f4\u5728\u7ebf\u6559\u80b2\u7684\u666e\u53ca\u5bfc\u81f4\u5e08\u751f\u975e\u8bed\u8a00\u6c9f\u901a\u7f3a\u5931\uff0c\u4ec5\u4f9d\u8d56\u8bed\u8a00\u53cd\u9988\u964d\u4f4e\u4e86\u6559\u80b2\u6548\u679c\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u60c5\u611f\u5206\u6790\u6280\u672f\u63d0\u5347\u5728\u7ebf\u6559\u80b2\u4e2d\u5e08\u751f\u6c9f\u901a\u7684\u8d28\u91cf\u3002", "method": "\u7814\u7a76\u5305\u62ec\u6570\u636e\u51c6\u5907\u3001\u7279\u5f81\u9009\u62e9\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u8bcd\u5d4c\u5165\u3001LSTM\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4e0e\u903b\u8f91\u56de\u5f52\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u7406\u89e3\u5b66\u751f\u53cd\u9988\u7684\u60c5\u611f\u80cc\u666f\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u903b\u8f91\u56de\u5f52\u6a21\u578b\uff0c\u4e3a\u5728\u7ebf\u6559\u80b2\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u6c9f\u901a\u5de5\u5177\u3002", "conclusion": "\u60c5\u611f\u5206\u6790\u6280\u672f\u53ef\u4ee5\u5f25\u8865\u5728\u7ebf\u6559\u80b2\u4e2d\u5e08\u751f\u6c9f\u901a\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u6559\u80b2\u4f53\u9a8c\u7684\u8d28\u91cf\u3002"}}
{"id": "2506.05497", "pdf": "https://arxiv.org/pdf/2506.05497", "abs": "https://arxiv.org/abs/2506.05497", "authors": ["Sima Noorani", "Shayan Kiyani", "George Pappas", "Hamed Hassani"], "title": "Conformal Prediction Beyond the Seen: A Missing Mass Perspective for Uncertainty Quantification in Generative Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Uncertainty quantification (UQ) is essential for safe deployment of\ngenerative AI models such as large language models (LLMs), especially in high\nstakes applications. Conformal prediction (CP) offers a principled uncertainty\nquantification framework, but classical methods focus on regression and\nclassification, relying on geometric distances or softmax scores: tools that\npresuppose structured outputs. We depart from this paradigm by studying CP in a\nquery only setting, where prediction sets must be constructed solely from\nfinite queries to a black box generative model, introducing a new trade off\nbetween coverage, test time query budget, and informativeness. We introduce\nConformal Prediction with Query Oracle (CPQ), a framework characterizing the\noptimal interplay between these objectives. Our finite sample algorithm is\nbuilt on two core principles: one governs the optimal query policy, and the\nother defines the optimal mapping from queried samples to prediction sets.\nRemarkably, both are rooted in the classical missing mass problem in\nstatistics. Specifically, the optimal query policy depends on the rate of\ndecay, or the derivative, of the missing mass, for which we develop a novel\nestimator. Meanwhile, the optimal mapping hinges on the missing mass itself,\nwhich we estimate using Good Turing estimators. We then turn our focus to\nimplementing our method for language models, where outputs are vast, variable,\nand often under specified. Fine grained experiments on three real world open\nended tasks and two LLMs, show CPQ applicability to any black box LLM and\nhighlight: (1) individual contribution of each principle to CPQ performance,\nand (2) CPQ ability to yield significantly more informative prediction sets\nthan existing conformal methods for language uncertainty quantification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCPQ\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4ec5\u67e5\u8be2\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u901a\u8fc7\u4f18\u5316\u67e5\u8be2\u7b56\u7565\u548c\u9884\u6d4b\u96c6\u6620\u5c04\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210AI\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6548\u679c\u3002", "motivation": "\u751f\u6210AI\u6a21\u578b\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u9700\u8981\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u65e0\u6cd5\u76f4\u63a5\u9002\u7528\u4e8e\u67e5\u8be2\u8bbe\u7f6e\u3002", "method": "\u63d0\u51faCPQ\u6846\u67b6\uff0c\u57fa\u4e8e\u7edf\u8ba1\u5b66\u4e2d\u7684\u7f3a\u5931\u8d28\u91cf\u95ee\u9898\uff0c\u8bbe\u8ba1\u6700\u4f18\u67e5\u8be2\u7b56\u7565\u548c\u9884\u6d4b\u96c6\u6620\u5c04\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528Good-Turing\u4f30\u8ba1\u5668\u8fdb\u884c\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCPQ\u80fd\u591f\u663e\u8457\u63d0\u5347\u9884\u6d4b\u96c6\u7684\u4fe1\u606f\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u8bed\u8a00\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "conclusion": "CPQ\u4e3a\u9ed1\u76d2\u751f\u6210\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.05502", "pdf": "https://arxiv.org/pdf/2506.05502", "abs": "https://arxiv.org/abs/2506.05502", "authors": ["Ya Jiang", "Chuxiong Wu", "Massieh Kordi Boroujeny", "Brian Mark", "Kai Zeng"], "title": "StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": "camera-ready version", "summary": "Watermarking for large language models (LLMs) offers a promising approach to\nidentifying AI-generated text. Existing approaches, however, either compromise\nthe distribution of original generated text by LLMs or are limited to embedding\nzero-bit information that only allows for watermark detection but ignores\nidentification. We present StealthInk, a stealthy multi-bit watermarking scheme\nthat preserves the original text distribution while enabling the embedding of\nprovenance data, such as userID, TimeStamp, and modelID, within LLM-generated\ntext. This enhances fast traceability without requiring access to the language\nmodel's API or prompts. We derive a lower bound on the number of tokens\nnecessary for watermark detection at a fixed equal error rate, which provides\ninsights on how to enhance the capacity. Comprehensive empirical evaluations\nacross diverse tasks highlight the stealthiness, detectability, and resilience\nof StealthInk, establishing it as an effective solution for LLM watermarking\napplications.", "AI": {"tldr": "StealthInk\u662f\u4e00\u79cd\u9690\u853d\u7684\u591a\u4f4d\u6c34\u5370\u65b9\u6848\uff0c\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\uff0c\u65e2\u80fd\u4fdd\u6301\u539f\u59cb\u6587\u672c\u5206\u5e03\uff0c\u53c8\u80fd\u5d4c\u5165\u6765\u6e90\u6570\u636e\uff08\u5982\u7528\u6237ID\u3001\u65f6\u95f4\u6233\u7b49\uff09\uff0c\u589e\u5f3a\u53ef\u8ffd\u6eaf\u6027\u3002", "motivation": "\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u8981\u4e48\u7834\u574f\u751f\u6210\u6587\u672c\u7684\u539f\u59cb\u5206\u5e03\uff0c\u8981\u4e48\u4ec5\u652f\u6301\u96f6\u4f4d\u4fe1\u606f\u5d4c\u5165\uff0c\u65e0\u6cd5\u5b9e\u73b0\u8bc6\u522b\u529f\u80fd\u3002", "method": "\u63d0\u51faStealthInk\u65b9\u6848\uff0c\u901a\u8fc7\u5d4c\u5165\u591a\u4f4d\u6c34\u5370\u4fe1\u606f\uff08\u5982\u7528\u6237ID\u3001\u65f6\u95f4\u6233\u7b49\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6587\u672c\u5206\u5e03\u4e0d\u53d8\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eStealthInk\u5177\u6709\u9690\u853d\u6027\u3001\u53ef\u68c0\u6d4b\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u5e94\u7528\u3002", "conclusion": "StealthInk\u662f\u4e00\u79cd\u6709\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5feb\u901f\u6eaf\u6e90\u4e14\u65e0\u9700\u8bbf\u95ee\u6a21\u578bAPI\u6216\u63d0\u793a\u3002"}}
{"id": "2506.05508", "pdf": "https://arxiv.org/pdf/2506.05508", "abs": "https://arxiv.org/abs/2506.05508", "authors": ["Tiyasa Mitra", "Ritika Borkar", "Nidhi Bhatia", "Ramon Matas", "Shivam Raj", "Dheevatsa Mudigere", "Ritchie Zhao", "Maximilian Golub", "Arpan Dutta", "Sailaja Madduri", "Dharmesh Jani", "Brian Pharris", "Bita Darvish Rouhani"], "title": "Beyond the Buzz: A Pragmatic Take on Inference Disaggregation", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "As inference scales to multi-node deployments, disaggregation - splitting\ninference into distinct phases - offers a promising path to improving the\nthroughput-interactivity Pareto frontier. Despite growing enthusiasm and a\nsurge of open-source efforts, practical deployment of disaggregated serving\nremains limited due to the complexity of the optimization search space and\nsystem-level coordination. In this paper, we present the first systematic study\nof disaggregated inference at scale, evaluating hundreds of thousands of design\npoints across diverse workloads and hardware configurations. We find that\ndisaggregation is most effective for prefill-heavy traffic patterns and larger\nmodels. Our results highlight the critical role of dynamic rate matching and\nelastic scaling in achieving Pareto-optimal performance. Our findings offer\nactionable insights for efficient disaggregated deployments to navigate the\ntrade-off between system throughput and interactivity.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5206\u5e03\u5f0f\u63a8\u7406\u7684\u89c4\u6a21\u5316\u5e94\u7528\uff0c\u53d1\u73b0\u5176\u5bf9\u9884\u586b\u5145\u6d41\u91cf\u548c\u5927\u6a21\u578b\u6700\u6709\u6548\uff0c\u5e76\u5f3a\u8c03\u4e86\u52a8\u6001\u901f\u7387\u5339\u914d\u548c\u5f39\u6027\u6269\u5c55\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u63a8\u7406\u6269\u5c55\u5230\u591a\u8282\u70b9\u90e8\u7f72\uff0c\u5206\u5e03\u5f0f\u63a8\u7406\uff08\u5c06\u63a8\u7406\u5206\u4e3a\u4e0d\u540c\u9636\u6bb5\uff09\u4e3a\u63d0\u9ad8\u541e\u5410\u91cf-\u4ea4\u4e92\u6027Pareto\u8fb9\u754c\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4f18\u5316\u641c\u7d22\u7a7a\u95f4\u548c\u7cfb\u7edf\u534f\u8c03\u7684\u590d\u6742\u6027\uff0c\u5b9e\u9645\u90e8\u7f72\u4ecd\u53d7\u9650\u3002", "method": "\u672c\u6587\u9996\u6b21\u5bf9\u89c4\u6a21\u5316\u5206\u5e03\u5f0f\u63a8\u7406\u8fdb\u884c\u4e86\u7cfb\u7edf\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u6570\u5341\u4e07\u4e2a\u8bbe\u8ba1\u70b9\uff0c\u6db5\u76d6\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\u548c\u786c\u4ef6\u914d\u7f6e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5206\u5e03\u5f0f\u63a8\u7406\u5bf9\u9884\u586b\u5145\u6d41\u91cf\u6a21\u5f0f\u548c\u5927\u6a21\u578b\u6548\u679c\u6700\u4f73\uff0c\u52a8\u6001\u901f\u7387\u5339\u914d\u548c\u5f39\u6027\u6269\u5c55\u662f\u5b9e\u73b0Pareto\u6700\u4f18\u6027\u80fd\u7684\u5173\u952e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u9ad8\u6548\u90e8\u7f72\u5206\u5e03\u5f0f\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\uff0c\u5e2e\u52a9\u6743\u8861\u7cfb\u7edf\u541e\u5410\u91cf\u548c\u4ea4\u4e92\u6027\u3002"}}
{"id": "2506.05515", "pdf": "https://arxiv.org/pdf/2506.05515", "abs": "https://arxiv.org/abs/2506.05515", "authors": ["Adrien Cort\u00e9s", "R\u00e9mi Rehm", "Victor Letzelter"], "title": "Winner-takes-all for Multivariate Probabilistic Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "ICML 2025", "summary": "We introduce TimeMCL, a method leveraging the Multiple Choice Learning (MCL)\nparadigm to forecast multiple plausible time series futures. Our approach\nemploys a neural network with multiple heads and utilizes the Winner-Takes-All\n(WTA) loss to promote diversity among predictions. MCL has recently gained\nattention due to its simplicity and ability to address ill-posed and ambiguous\ntasks. We propose an adaptation of this framework for time-series forecasting,\npresenting it as an efficient method to predict diverse futures, which we\nrelate to its implicit quantization objective. We provide insights into our\napproach using synthetic data and evaluate it on real-world time series,\ndemonstrating its promising performance at a light computational cost.", "AI": {"tldr": "TimeMCL\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u9009\u62e9\u5b66\u4e60\uff08MCL\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u65f6\u95f4\u5e8f\u5217\u7684\u591a\u79cd\u53ef\u80fd\u672a\u6765\u3002\u5b83\u901a\u8fc7\u591a\u5934\u90e8\u795e\u7ecf\u7f51\u7edc\u548cWTA\u635f\u5931\u51fd\u6570\u63d0\u5347\u9884\u6d4b\u591a\u6837\u6027\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u6027\u80fd\u4f18\u5f02\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u901a\u5e38\u5b58\u5728\u6a21\u7cca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u591a\u79cd\u53ef\u80fd\u7684\u672a\u6765\u3002TimeMCL\u65e8\u5728\u901a\u8fc7MCL\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u5934\u90e8\u795e\u7ecf\u7f51\u7edc\u548cWinner-Takes-All\uff08WTA\uff09\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u4fc3\u8fdb\u9884\u6d4b\u7684\u591a\u6837\u6027\uff0c\u5e76\u9690\u542b\u91cf\u5316\u76ee\u6807\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u65f6\u95f4\u5e8f\u5217\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u73b0\u51fa\u8272\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "TimeMCL\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u591a\u6837\u5316\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6a21\u7cca\u6027\u4efb\u52a1\u3002"}}
{"id": "2506.05516", "pdf": "https://arxiv.org/pdf/2506.05516", "abs": "https://arxiv.org/abs/2506.05516", "authors": ["Boyuan Deng", "Luca Rossini", "Jin Wang", "Weijie Wang", "Nikolaos Tsagarakis"], "title": "Learning to Recover: Dynamic Reward Shaping with Wheel-Leg Coordination for Fallen Robots", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Adaptive recovery from fall incidents are essential skills for the practical\ndeployment of wheeled-legged robots, which uniquely combine the agility of legs\nwith the speed of wheels for rapid recovery. However, traditional methods\nrelying on preplanned recovery motions, simplified dynamics or sparse rewards\noften fail to produce robust recovery policies. This paper presents a\nlearning-based framework integrating Episode-based Dynamic Reward Shaping and\ncurriculum learning, which dynamically balances exploration of diverse recovery\nmaneuvers with precise posture refinement. An asymmetric actor-critic\narchitecture accelerates training by leveraging privileged information in\nsimulation, while noise-injected observations enhance robustness against\nuncertainties. We further demonstrate that synergistic wheel-leg coordination\nreduces joint torque consumption by 15.8% and 26.2% and improves stabilization\nthrough energy transfer mechanisms. Extensive evaluations on two distinct\nquadruped platforms achieve recovery success rates up to 99.1% and 97.8%\nwithout platform-specific tuning. The supplementary material is available at\nhttps://boyuandeng.github.io/L2R-WheelLegCoordination/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u5956\u52b1\u5851\u5f62\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u7528\u4e8e\u8f6e\u817f\u673a\u5668\u4eba\u7684\u81ea\u9002\u5e94\u8dcc\u5012\u6062\u590d\u3002\u901a\u8fc7\u975e\u5bf9\u79f0\u7684actor-critic\u67b6\u6784\u548c\u566a\u58f0\u6ce8\u5165\u89c2\u5bdf\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5173\u8282\u626d\u77e9\u6d88\u8017\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8fbe99.1%\u7684\u6062\u590d\u6210\u529f\u7387\u3002", "motivation": "\u8f6e\u817f\u673a\u5668\u4eba\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9700\u8981\u5177\u5907\u81ea\u9002\u5e94\u8dcc\u5012\u6062\u590d\u80fd\u529b\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u89c4\u5212\u52a8\u4f5c\u6216\u7b80\u5316\u52a8\u529b\u5b66\uff0c\u96be\u4ee5\u751f\u6210\u9c81\u68d2\u7684\u6062\u590d\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u4e86\u52a8\u6001\u5956\u52b1\u5851\u5f62\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u91c7\u7528\u975e\u5bf9\u79f0actor-critic\u67b6\u6784\u548c\u566a\u58f0\u6ce8\u5165\u89c2\u5bdf\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e8615.8%\u548c26.2%\u7684\u5173\u8282\u626d\u77e9\u6d88\u8017\uff0c\u5e76\u5728\u4e24\u79cd\u56db\u8db3\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e8699.1%\u548c97.8%\u7684\u6062\u590d\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8f6e\u817f\u673a\u5668\u4eba\u7684\u6062\u590d\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u5e73\u53f0\u8c03\u6574\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2506.05523", "pdf": "https://arxiv.org/pdf/2506.05523", "abs": "https://arxiv.org/abs/2506.05523", "authors": ["Zikui Cai", "Andrew Wang", "Anirudh Satheesh", "Ankit Nakhawa", "Hyunwoo Jae", "Keenan Powell", "Minghui Liu", "Neel Jay", "Sungbin Oh", "Xiyao Wang", "Yongyuan Liang", "Tom Goldstein", "Furong Huang"], "title": "MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite rapid advances in vision-language models (VLMs), current benchmarks\nfor multimodal reasoning fall short in three key dimensions. First, they\noverwhelmingly rely on static images, failing to capture the temporal\ncomplexity of real-world environments. Second, they narrowly focus on\nmathematical problem-solving, neglecting the broader spectrum of reasoning\nskills -- including abstract, physical, planning, spatial, and temporal\ncapabilities -- required for robust multimodal intelligence. Third, many\nbenchmarks quickly saturate, offering limited headroom for diagnosing failure\nmodes or measuring continued progress. We introduce MORSE-500 (Multimodal\nReasoning Stress-test Environment), a video benchmark composed of 500 fully\nscripted clips with embedded questions spanning six complementary reasoning\ncategories. Each instance is programmatically generated using deterministic\nPython scripts (via Manim, Matplotlib, MoviePy), generative video models, and\ncurated real footage. This script-driven design allows fine-grained control\nover visual complexity, distractor density, and temporal dynamics -- enabling\ndifficulty to be scaled systematically as models improve. Unlike static\nbenchmarks that become obsolete once saturated, MORSE-500 is built to evolve:\nits controllable generation pipeline supports the creation of arbitrarily\nchallenging new instances, making it ideally suited for stress-testing\nnext-generation models. Initial experiments with state-of-the-art systems --\nincluding various Gemini 2.5 Pro and OpenAI o3 which represent the strongest\navailable at the time, alongside strong open-source models -- reveal\nsubstantial performance gaps across all categories, with particularly large\ndeficits in abstract and planning tasks. We release the full dataset,\ngeneration scripts, and evaluation harness to support transparent,\nreproducible, and forward-looking multimodal reasoning research.", "AI": {"tldr": "MORSE-500\u662f\u4e00\u4e2a\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u7684\u4e09\u4e2a\u4e0d\u8db3\uff1a\u9759\u6001\u56fe\u50cf\u3001\u72ed\u7a84\u7684\u6570\u5b66\u95ee\u9898\u8303\u56f4\u4ee5\u53ca\u5feb\u901f\u9971\u548c\u7684\u95ee\u9898\u3002\u5b83\u5305\u542b500\u4e2a\u811a\u672c\u751f\u6210\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u8986\u76d6\u516d\u79cd\u63a8\u7406\u7c7b\u522b\uff0c\u652f\u6301\u52a8\u6001\u96be\u5ea6\u8c03\u6574\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u7684\u65f6\u5e8f\u590d\u6742\u6027\uff0c\u4e14\u5c40\u9650\u4e8e\u6570\u5b66\u95ee\u9898\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6a21\u6001\u667a\u80fd\u6240\u9700\u7684\u5e7f\u6cdb\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7Python\u811a\u672c\uff08Manim\u3001Matplotlib\u3001MoviePy\uff09\u3001\u751f\u6210\u89c6\u9891\u6a21\u578b\u548c\u771f\u5b9e\u7d20\u6750\uff0c\u7a0b\u5e8f\u5316\u751f\u6210500\u4e2a\u89c6\u9891\u7247\u6bb5\uff0c\u5d4c\u5165\u516d\u7c7b\u63a8\u7406\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\uff08\u5982Gemini 2.5 Pro\u548cOpenAI o3\uff09\u5728\u6240\u6709\u63a8\u7406\u7c7b\u522b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u62bd\u8c61\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u5dee\u8ddd\u663e\u8457\u3002", "conclusion": "MORSE-500\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u652f\u6301\u672a\u6765\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u7684\u900f\u660e\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2506.05565", "pdf": "https://arxiv.org/pdf/2506.05565", "abs": "https://arxiv.org/abs/2506.05565", "authors": ["Feliks Ba\u0144ka", "Jaros\u0142aw A. Chudziak"], "title": "Applying Informer for Option Pricing: A Transformer-Based Approach", "categories": ["cs.CE", "cs.AI", "cs.LG", "q-fin.CP", "91G60, 68T07", "I.2.6; J.4"], "comment": "8 pages, 3 tables, 7 figures. Accepted at the 17th International\n  Conference on Agents and Artificial Intelligence (ICAART 2025). Final version\n  published in Proceedings of ICAART 2025 (Vol. 3), pages 1270-1277", "summary": "Accurate option pricing is essential for effective trading and risk\nmanagement in financial markets, yet it remains challenging due to market\nvolatility and the limitations of traditional models like Black-Scholes. In\nthis paper, we investigate the application of the Informer neural network for\noption pricing, leveraging its ability to capture long-term dependencies and\ndynamically adjust to market fluctuations. This research contributes to the\nfield of financial forecasting by introducing Informer's efficient architecture\nto enhance prediction accuracy and provide a more adaptable and resilient\nframework compared to existing methods. Our results demonstrate that Informer\noutperforms traditional approaches in option pricing, advancing the\ncapabilities of data-driven financial forecasting in this domain.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Informer\u795e\u7ecf\u7f51\u7edc\u5728\u671f\u6743\u5b9a\u4ef7\u4e2d\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u671f\u6743\u5b9a\u4ef7\u6a21\u578b\uff08\u5982Black-Scholes\uff09\u56e0\u5e02\u573a\u6ce2\u52a8\u6027\u548c\u5c40\u9650\u6027\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528Informer\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u5e76\u52a8\u6001\u9002\u5e94\u5e02\u573a\u6ce2\u52a8\u3002", "result": "Informer\u5728\u671f\u6743\u5b9a\u4ef7\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "Informer\u4e3a\u91d1\u878d\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u6570\u636e\u9a71\u52a8\u91d1\u878d\u9884\u6d4b\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.05566", "pdf": "https://arxiv.org/pdf/2506.05566", "abs": "https://arxiv.org/abs/2506.05566", "authors": ["Chenhui Deng", "Yun-Da Tsai", "Guan-Ting Liu", "Zhongzhi Yu", "Haoxing Ren"], "title": "ScaleRTL: Scaling LLMs with Reasoning Data and Test-Time Compute for Accurate RTL Code Generation", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled near-human\nperformance on software coding benchmarks, but their effectiveness in RTL code\ngeneration remains limited due to the scarcity of high-quality training data.\nWhile prior efforts have fine-tuned LLMs for RTL tasks, they do not\nfundamentally overcome the data bottleneck and lack support for test-time\nscaling due to their non-reasoning nature. In this work, we introduce ScaleRTL,\nthe first reasoning LLM for RTL coding that scales up both high-quality\nreasoning data and test-time compute. Specifically, we curate a diverse set of\nlong chain-of-thought reasoning traces averaging 56K tokens each, resulting in\na dataset of 3.5B tokens that captures rich RTL knowledge. Fine-tuning a\ngeneral-purpose reasoning model on this corpus yields ScaleRTL that is capable\nof deep RTL reasoning. Subsequently, we further enhance the performance of\nScaleRTL through a novel test-time scaling strategy that extends the reasoning\nprocess via iteratively reflecting on and self-correcting previous reasoning\nsteps. Experimental results show that ScaleRTL achieves state-of-the-art\nperformance on VerilogEval and RTLLM, outperforming 18 competitive baselines by\nup to 18.4% on VerilogEval and 12.7% on RTLLM.", "AI": {"tldr": "ScaleRTL\u662f\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8eRTL\u4ee3\u7801\u751f\u6210\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u63a8\u7406\u6570\u636e\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u5728RTL\u4ee3\u7801\u751f\u6210\u4e2d\u56e0\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u975e\u63a8\u7406\u6027\u8d28\u800c\u8868\u73b0\u53d7\u9650\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b3.5B\u4ee4\u724c\u7684\u591a\u6837\u5316\u957f\u94fe\u63a8\u7406\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u548c\u4fee\u6b63\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728VerilogEval\u548cRTLLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cScaleRTL\u6027\u80fd\u4f18\u4e8e18\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u5206\u522b\u63d0\u534718.4%\u548c12.7%\u3002", "conclusion": "ScaleRTL\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u63a8\u7406\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u4e86RTL\u4ee3\u7801\u751f\u6210\u7684\u6027\u80fd\uff0c\u4e3a\u9886\u57df\u5185\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.05568", "pdf": "https://arxiv.org/pdf/2506.05568", "abs": "https://arxiv.org/abs/2506.05568", "authors": ["Arian Raje", "Baris Askin", "Divyansh Jhunjhunwala", "Gauri Joshi"], "title": "Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have not yet effectively leveraged the vast\namounts of edge-device data, and federated learning (FL) offers a promising\nparadigm to collaboratively fine-tune LLMs without transferring private edge\ndata to the cloud. To operate within the computation and communication\nconstraints of edge devices, recent literature on federated fine-tuning of LLMs\nproposes the use of low-rank adaptation (LoRA) and similar parameter-efficient\nmethods. However, LoRA-based methods suffer from accuracy degradation in FL\nsettings, primarily because of data and computational heterogeneity across\nclients. We propose \\textsc{Ravan}, an adaptive multi-head LoRA method that\nbalances parameter efficiency and model expressivity by reparameterizing the\nweight updates as the sum of multiple LoRA heads\n$s_i\\textbf{B}_i\\textbf{H}_i\\textbf{A}_i$ in which only the core matrices\n$\\textbf{H}_i$ and their lightweight scaling factors $s_i$ are trained. These\ntrainable scaling factors let the optimization focus on the most useful heads,\nrecovering a higher-rank approximation of the full update without increasing\nthe number of communicated parameters since clients upload $s_i\\textbf{H}_i$\ndirectly. Experiments on vision and language benchmarks show that\n\\textsc{Ravan} improves test accuracy by 2-8\\% over prior parameter-efficient\nbaselines, making it a robust and scalable solution for federated fine-tuning\nof LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRavan\u7684\u81ea\u9002\u5e94\u591a\u5934LoRA\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u9ad8\u6548\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5f02\u6784\u6027\u548c\u8ba1\u7b97\u5f02\u6784\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e862-8%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c1a\u672a\u6709\u6548\u5229\u7528\u8fb9\u7f18\u8bbe\u5907\u6570\u636e\uff0c\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4f20\u8f93\u79c1\u6709\u6570\u636e\u7684\u534f\u4f5c\u5fae\u8c03\u8303\u5f0f\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684LoRA\u65b9\u6cd5\u5728FL\u4e2d\u56e0\u6570\u636e\u5f02\u6784\u6027\u548c\u8ba1\u7b97\u5f02\u6784\u6027\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u63d0\u51faRavan\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6743\u91cd\u66f4\u65b0\u91cd\u65b0\u53c2\u6570\u5316\u4e3a\u591a\u4e2aLoRA\u5934\u7684\u52a0\u6743\u548c\uff08s_iB_iH_iA_i\uff09\uff0c\u4ec5\u8bad\u7ec3\u6838\u5fc3\u77e9\u9635H_i\u548c\u8f7b\u91cf\u7ea7\u7f29\u653e\u56e0\u5b50s_i\uff0c\u4f18\u5316\u805a\u7126\u4e8e\u6700\u6709\u7528\u7684\u5934\uff0c\u6062\u590d\u66f4\u9ad8\u79e9\u7684\u66f4\u65b0\u8fd1\u4f3c\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRavan\u6bd4\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u57fa\u7ebf\u63d0\u5347\u4e862-8%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u3002", "conclusion": "Ravan\u662f\u4e00\u79cd\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8eLLMs\u7684\u8054\u90a6\u5fae\u8c03\u3002"}}
{"id": "2506.05577", "pdf": "https://arxiv.org/pdf/2506.05577", "abs": "https://arxiv.org/abs/2506.05577", "authors": ["Saptarshi Nath", "Christos Peridis", "Eseoghene Benjamin", "Xinran Liu", "Soheil Kolouri", "Peter Kinnell", "Zexin Li", "Cong Liu", "Shirin Dora", "Andrea Soltoggio"], "title": "Collaborative Learning in Agentic Systems: A Collective AI is Greater Than the Sum of Its Parts", "categories": ["cs.LG", "cs.AI", "cs.MA", "I.2.6; I.2.11"], "comment": "36 pages, 21 figures, 6 tables. Preprint", "summary": "Agentic AI has gained significant interest as a research paradigm focused on\nautonomy, self-directed learning, and long-term reliability of decision making.\nReal-world agentic systems operate in decentralized settings on a large set of\ntasks or data distributions with constraints such as limited bandwidth,\nasynchronous execution, and the absence of a centralized model or even common\nobjectives. We posit that exploiting previously learned skills, task\nsimilarities, and communication capabilities in a collective of agentic AI are\nchallenging but essential elements to enabling scalability, open-endedness, and\nbeneficial collaborative learning dynamics. In this paper, we introduce Modular\nSharing and Composition in Collective Learning (MOSAIC), an agentic algorithm\nthat allows multiple agents to independently solve different tasks while also\nidentifying, sharing, and reusing useful machine-learned knowledge, without\ncoordination, synchronization, or centralized control. MOSAIC combines three\nmechanisms: (1) modular policy composition via neural network masks, (2) cosine\nsimilarity estimation using Wasserstein embeddings for knowledge selection, and\n(3) asynchronous communication and policy integration. Results on a set of RL\nbenchmarks show that MOSAIC has a greater sample efficiency than isolated\nlearners, i.e., it learns significantly faster, and in some cases, finds\nsolutions to tasks that cannot be solved by isolated learners. The\ncollaborative learning and sharing dynamics are also observed to result in the\nemergence of ideal curricula of tasks, from easy to hard. These findings\nsupport the case for collaborative learning in agentic systems to achieve\nbetter and continuously evolving performance both at the individual and\ncollective levels.", "AI": {"tldr": "MOSAIC\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7b97\u6cd5\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u5171\u4eab\u548c\u7ec4\u5408\u5b66\u4e60\uff0c\u5b9e\u73b0\u65e0\u534f\u8c03\u3001\u5f02\u6b65\u7684\u77e5\u8bc6\u5171\u4eab\u4e0e\u91cd\u7528\uff0c\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u7387\u548c\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u53bb\u4e2d\u5fc3\u5316\u3001\u5f02\u6b65\u3001\u65e0\u5171\u540c\u76ee\u6807\u7684\u590d\u6742\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u95f4\u7684\u77e5\u8bc6\u5171\u4eab\u4e0e\u534f\u4f5c\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u3002", "method": "\u7ed3\u5408\u6a21\u5757\u5316\u7b56\u7565\u7ec4\u5408\u3001Wasserstein\u5d4c\u5165\u7684\u4f59\u5f26\u76f8\u4f3c\u6027\u4f30\u8ba1\u548c\u5f02\u6b65\u901a\u4fe1\u673a\u5236\uff0c\u5b9e\u73b0\u77e5\u8bc6\u7684\u9009\u62e9\u3001\u5171\u4eab\u4e0e\u96c6\u6210\u3002", "result": "\u5728RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMOSAIC\u6bd4\u5b64\u7acb\u5b66\u4e60\u5668\u5b66\u4e60\u66f4\u5feb\uff0c\u80fd\u89e3\u51b3\u5b64\u7acb\u5b66\u4e60\u5668\u65e0\u6cd5\u5b8c\u6210\u7684\u4efb\u52a1\uff0c\u5e76\u89c2\u5bdf\u5230\u4efb\u52a1\u96be\u5ea6\u7531\u6613\u5230\u96be\u7684\u7406\u60f3\u8bfe\u7a0b\u3002", "conclusion": "\u534f\u4f5c\u5b66\u4e60\u5728\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u80fd\u5b9e\u73b0\u4e2a\u4f53\u548c\u96c6\u4f53\u6027\u80fd\u7684\u6301\u7eed\u4f18\u5316\uff0c\u652f\u6301\u5f00\u653e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.05582", "pdf": "https://arxiv.org/pdf/2506.05582", "abs": "https://arxiv.org/abs/2506.05582", "authors": ["Azza Abouzied", "Firoj Alam", "Raian Ali", "Paolo Papotti"], "title": "Combating Misinformation in the Arab World: Challenges & Opportunities", "categories": ["cs.CL", "cs.AI", "cs.SI", "68T50", "I.2.7"], "comment": "disinformation, misinformation, factuality, harmfulness, fake news", "summary": "Misinformation and disinformation pose significant risks globally, with the\nArab region facing unique vulnerabilities due to geopolitical instabilities,\nlinguistic diversity, and cultural nuances. We explore these challenges through\nthe key facets of combating misinformation: detection, tracking, mitigation and\ncommunity-engagement. We shed light on how connecting with grass-roots\nfact-checking organizations, understanding cultural norms, promoting social\ncorrection, and creating strong collaborative information networks can create\nopportunities for a more resilient information ecosystem in the Arab world.", "AI": {"tldr": "\u63a2\u8ba8\u963f\u62c9\u4f2f\u5730\u533a\u56e0\u5730\u7f18\u653f\u6cbb\u3001\u8bed\u8a00\u591a\u6837\u6027\u548c\u6587\u5316\u5dee\u5f02\u800c\u9762\u4e34\u7684\u865a\u5047\u4fe1\u606f\u6311\u6218\uff0c\u63d0\u51fa\u901a\u8fc7\u68c0\u6d4b\u3001\u8ffd\u8e2a\u3001\u7f13\u89e3\u548c\u793e\u533a\u53c2\u4e0e\u6765\u6784\u5efa\u66f4\u5177\u97e7\u6027\u7684\u4fe1\u606f\u751f\u6001\u7cfb\u7edf\u3002", "motivation": "\u963f\u62c9\u4f2f\u5730\u533a\u56e0\u72ec\u7279\u7684\u5730\u7f18\u653f\u6cbb\u548c\u6587\u5316\u80cc\u666f\uff0c\u66f4\u5bb9\u6613\u53d7\u5230\u865a\u5047\u4fe1\u606f\u7684\u5f71\u54cd\uff0c\u4e9f\u9700\u9488\u5bf9\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8fde\u63a5\u57fa\u5c42\u4e8b\u5b9e\u6838\u67e5\u7ec4\u7ec7\u3001\u7406\u89e3\u6587\u5316\u89c4\u8303\u3001\u63a8\u52a8\u793e\u4f1a\u7ea0\u6b63\u548c\u5efa\u7acb\u534f\u4f5c\u4fe1\u606f\u7f51\u7edc\u6765\u5e94\u5bf9\u865a\u5047\u4fe1\u606f\u3002", "result": "\u4e3a\u963f\u62c9\u4f2f\u4e16\u754c\u6784\u5efa\u66f4\u5177\u97e7\u6027\u7684\u4fe1\u606f\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002", "conclusion": "\u7ed3\u5408\u672c\u5730\u5316\u7b56\u7565\u548c\u591a\u65b9\u534f\u4f5c\uff0c\u53ef\u4ee5\u6709\u6548\u5e94\u5bf9\u963f\u62c9\u4f2f\u5730\u533a\u7684\u865a\u5047\u4fe1\u606f\u95ee\u9898\u3002"}}
{"id": "2506.05583", "pdf": "https://arxiv.org/pdf/2506.05583", "abs": "https://arxiv.org/abs/2506.05583", "authors": ["Nien-Shao Wang", "Duygu Nur Yaldiz", "Yavuz Faruk Bakman", "Sai Praneeth Karimireddy"], "title": "Conformal Prediction Adaptive to Unknown Subpopulation Shifts", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "20 pages, 6 figures, 5 tables, submitted to NeurIPS 2025", "summary": "Conformal prediction is widely used to equip black-box machine learning\nmodels with uncertainty quantification enjoying formal coverage guarantees.\nHowever, these guarantees typically break down in the presence of distribution\nshifts, where the data distribution at test time differs from the training (or\ncalibration-time) distribution. In this work, we address subpopulation shifts,\nwhere the test environment exhibits an unknown and differing mixture of\nsubpopulations compared to the calibration data. We propose new methods that\nprovably adapt conformal prediction to such shifts, ensuring valid coverage\nwithout requiring explicit knowledge of subpopulation structure. Our algorithms\nscale to high-dimensional settings and perform effectively in realistic machine\nlearning tasks. Extensive experiments on vision (with vision transformers) and\nlanguage (with large language models) benchmarks demonstrate that our methods\nreliably maintain coverage and controls risk in scenarios where standard\nconformal prediction fails.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u65b0\u65b9\u6cd5\uff0c\u4f7f\u4fdd\u5f62\u9884\u6d4b\u80fd\u591f\u5728\u5b50\u7fa4\u4f53\u5206\u5e03\u504f\u79fb\u4e0b\u4fdd\u6301\u6709\u6548\u8986\u76d6\uff0c\u65e0\u9700\u663e\u5f0f\u5b50\u7fa4\u4f53\u7ed3\u6784\u77e5\u8bc6\u3002", "motivation": "\u6807\u51c6\u4fdd\u5f62\u9884\u6d4b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u5931\u6548\uff0c\u5c24\u5176\u662f\u5b50\u7fa4\u4f53\u504f\u79fb\u65f6\uff0c\u8986\u76d6\u4fdd\u8bc1\u88ab\u7834\u574f\u3002", "method": "\u63d0\u51fa\u65b0\u7b97\u6cd5\uff0c\u9002\u5e94\u672a\u77e5\u5b50\u7fa4\u4f53\u6df7\u5408\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u786e\u4fdd\u6709\u6548\u8986\u76d6\u3002", "result": "\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u6807\u51c6\u65b9\u6cd5\u5931\u6548\u65f6\u4ecd\u80fd\u7ef4\u6301\u8986\u76d6\u548c\u63a7\u5236\u98ce\u9669\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u5b50\u7fa4\u4f53\u504f\u79fb\u4e0b\u53ef\u9760\u5730\u7ef4\u6301\u4fdd\u5f62\u9884\u6d4b\u7684\u8986\u76d6\u4fdd\u8bc1\u3002"}}
{"id": "2506.05586", "pdf": "https://arxiv.org/pdf/2506.05586", "abs": "https://arxiv.org/abs/2506.05586", "authors": ["Isha Puri", "Amit Dhurandhar", "Tejaswini Pedapati", "Kartikeyan Shanmugam", "Dennis Wei", "Kush R. Varshney"], "title": "CoFrNets: Interpretable Neural Architecture Inspired by Continued Fractions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In recent years there has been a considerable amount of research on local\npost hoc explanations for neural networks. However, work on building\ninterpretable neural architectures has been relatively sparse. In this paper,\nwe present a novel neural architecture, CoFrNet, inspired by the form of\ncontinued fractions which are known to have many attractive properties in\nnumber theory, such as fast convergence of approximations to real numbers. We\nshow that CoFrNets can be efficiently trained as well as interpreted leveraging\ntheir particular functional form. Moreover, we prove that such architectures\nare universal approximators based on a proof strategy that is different than\nthe typical strategy used to prove universal approximation results for neural\nnetworks based on infinite width (or depth), which is likely to be of\nindependent interest. We experiment on nonlinear synthetic functions and are\nable to accurately model as well as estimate feature attributions and even\nhigher order terms in some cases, which is a testament to the representational\npower as well as interpretability of such architectures. To further showcase\nthe power of CoFrNets, we experiment on seven real datasets spanning tabular,\ntext and image modalities, and show that they are either comparable or\nsignificantly better than other interpretable models and multilayer\nperceptrons, sometimes approaching the accuracies of state-of-the-art models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784CoFrNet\uff0c\u57fa\u4e8e\u8fde\u5206\u6570\u7684\u5f62\u5f0f\uff0c\u5177\u6709\u9ad8\u6548\u8bad\u7ec3\u548c\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u5c40\u90e8\u540e\u9a8c\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u7684\u7814\u7a76\u8f83\u591a\uff0c\u4f46\u6784\u5efa\u53ef\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u5de5\u4f5c\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faCoFrNet\uff0c\u4e00\u79cd\u57fa\u4e8e\u8fde\u5206\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5229\u7528\u5176\u7279\u6b8a\u51fd\u6570\u5f62\u5f0f\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u89e3\u91ca\u3002", "result": "CoFrNet\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u51c6\u786e\u5efa\u6a21\u5e76\u4f30\u8ba1\u7279\u5f81\u8d21\u732e\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f30\u8ba1\u9ad8\u9636\u9879\u3002", "conclusion": "CoFrNet\u4e0d\u4ec5\u5177\u6709\u5f3a\u5927\u7684\u8868\u793a\u80fd\u529b\uff0c\u8fd8\u5177\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u53ef\u89e3\u91ca\u6a21\u578b\u548c\u591a\u5c42\u611f\u77e5\u673a\u3002"}}
{"id": "2506.05593", "pdf": "https://arxiv.org/pdf/2506.05593", "abs": "https://arxiv.org/abs/2506.05593", "authors": ["David Palzer", "Matthew Maciejewski", "Eric Fosler-Lussier"], "title": "Improving Neural Diarization through Speaker Attribute Attractors and Local Dependency Modeling", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP), Seoul, Korea, Republic of, 2024, pp.\n  11911-11915", "summary": "In recent years, end-to-end approaches have made notable progress in\naddressing the challenge of speaker diarization, which involves segmenting and\nidentifying speakers in multi-talker recordings. One such approach,\nEncoder-Decoder Attractors (EDA), has been proposed to handle variable speaker\ncounts as well as better guide the network during training. In this study, we\nextend the attractor paradigm by moving beyond direct speaker modeling and\ninstead focus on representing more detailed `speaker attributes' through a\nmulti-stage process of intermediate representations. Additionally, we enhance\nthe architecture by replacing transformers with conformers, a\nconvolution-augmented transformer, to model local dependencies. Experiments\ndemonstrate improved diarization performance on the CALLHOME dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u7aef\u5230\u7aef\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8868\u793a\u548c\u5377\u79ef\u589e\u5f3a\u7684\u53d8\u6362\u5668\uff08conformer\uff09\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\uff08\u5982EDA\uff09\u5728\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u5316\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u5bf9\u8bf4\u8bdd\u4eba\u5c5e\u6027\u7684\u5efa\u6a21\u548c\u5c40\u90e8\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u6269\u5c55\u4e86\u5438\u5f15\u5b50\u8303\u5f0f\uff0c\u5f15\u5165\u591a\u9636\u6bb5\u4e2d\u95f4\u8868\u793a\u4ee5\u6355\u6349\u66f4\u8be6\u7ec6\u7684\u8bf4\u8bdd\u4eba\u5c5e\u6027\uff0c\u5e76\u7528conformer\u66ff\u4ee3transformer\u4ee5\u589e\u5f3a\u5c40\u90e8\u4f9d\u8d56\u5efa\u6a21\u3002", "result": "\u5728CALLHOME\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u5316\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u591a\u9636\u6bb5\u8868\u793a\u548cconformer\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u5316\u7684\u6548\u679c\u3002"}}
{"id": "2506.05596", "pdf": "https://arxiv.org/pdf/2506.05596", "abs": "https://arxiv.org/abs/2506.05596", "authors": ["Jes Frellsen", "Maher M. Kassem", "Tone Bengtsen", "Lars Olsen", "Kresten Lindorff-Larsen", "Jesper Ferkinghoff-Borg", "Wouter Boomsma"], "title": "Zero-shot protein stability prediction by inverse folding models: a free energy interpretation", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "stat.ML"], "comment": null, "summary": "Inverse folding models have proven to be highly effective zero-shot\npredictors of protein stability. Despite this success, the link between the\namino acid preferences of an inverse folding model and the free-energy\nconsiderations underlying thermodynamic stability remains incompletely\nunderstood. A better understanding would be of interest not only from a\ntheoretical perspective, but also potentially provide the basis for stronger\nzero-shot stability prediction. In this paper, we take steps to clarify the\nfree-energy foundations of inverse folding models. Our derivation reveals the\nstandard practice of likelihood ratios as a simplistic approximation and\nsuggests several paths towards better estimates of the relative stability. We\nempirically assess these approaches and demonstrate that considerable gains in\nzero-shot performance can be achieved with fairly simple means.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u9006\u6298\u53e0\u6a21\u578b\u4e0e\u86cb\u767d\u8d28\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u81ea\u7531\u80fd\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u4f3c\u7136\u6bd4\u65b9\u6cd5\u7684\u7b80\u5316\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3\u9006\u6298\u53e0\u6a21\u578b\u7684\u6c28\u57fa\u9178\u504f\u597d\u4e0e\u70ed\u529b\u5b66\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u81ea\u7531\u80fd\u5173\u7cfb\uff0c\u4ee5\u63d0\u5347\u96f6\u6837\u672c\u7a33\u5b9a\u6027\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u63ed\u793a\u4f3c\u7136\u6bd4\u65b9\u6cd5\u7684\u7b80\u5316\u6027\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\uff0c\u968f\u540e\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7b80\u5355\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u4e3a\u9006\u6298\u53e0\u6a21\u578b\u7684\u81ea\u7531\u80fd\u57fa\u7840\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u7406\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u6539\u8fdb\u7a33\u5b9a\u6027\u9884\u6d4b\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.05598", "pdf": "https://arxiv.org/pdf/2506.05598", "abs": "https://arxiv.org/abs/2506.05598", "authors": ["Michael J Ryan", "Omar Shaikh", "Aditri Bhagirath", "Daniel Frees", "William Held", "Diyi Yang"], "title": "SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Recent calls for pluralistic alignment of Large Language Models (LLMs)\nencourage adapting models to diverse user preferences. However, most prior work\non personalized reward models heavily rely on additional identity information,\nsuch as demographic details or a predefined set of preference categories. To\nthis end, we introduce SynthesizeMe, an approach to inducing synthetic user\npersonas from user interactions for personalized reward modeling. SynthesizeMe\nfirst generates and verifies reasoning to explain user preferences, then\ninduces synthetic user personas from that reasoning, and finally filters to\ninformative prior user interactions in order to build personalized prompts for\na particular user. We show that using SynthesizeMe induced prompts improves\npersonalized LLM-as-a-judge accuracy by 4.4% on Chatbot Arena. Combining\nSynthesizeMe derived prompts with a reward model achieves top performance on\nPersonalRewardBench: a new curation of user-stratified interactions with\nchatbots collected from 854 users of Chatbot Arena and PRISM.", "AI": {"tldr": "SynthesizeMe\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u751f\u6210\u5408\u6210\u7528\u6237\u89d2\u8272\uff0c\u63d0\u5347\u4e2a\u6027\u5316\u5956\u52b1\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u8eab\u4efd\u4fe1\u606f\uff0c\u800cSynthesizeMe\u65e8\u5728\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u76f4\u63a5\u751f\u6210\u4e2a\u6027\u5316\u89d2\u8272\u3002", "method": "\u751f\u6210\u5e76\u9a8c\u8bc1\u7528\u6237\u504f\u597d\u7684\u89e3\u91ca\uff0c\u8bf1\u5bfc\u5408\u6210\u89d2\u8272\uff0c\u7b5b\u9009\u4fe1\u606f\u4ea4\u4e92\u4ee5\u6784\u5efa\u4e2a\u6027\u5316\u63d0\u793a\u3002", "result": "\u5728Chatbot Arena\u4e0a\u63d0\u53474.4%\u7684\u51c6\u786e\u6027\uff0c\u5728PersonalRewardBench\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "SynthesizeMe\u6709\u6548\u63d0\u5347\u4e2a\u6027\u5316\u5956\u52b1\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u4f9d\u8d56\u989d\u5916\u8eab\u4efd\u4fe1\u606f\u3002"}}
{"id": "2506.05605", "pdf": "https://arxiv.org/pdf/2506.05605", "abs": "https://arxiv.org/abs/2506.05605", "authors": ["Julia Barnett", "Kimon Kieslich", "Jasmine Sinchai", "Nicholas Diakopoulos"], "title": "Scenarios in Computing Research: A Systematic Review of the Use of Scenario Methods for Exploring the Future of Computing Technologies in Society", "categories": ["cs.HC", "cs.AI"], "comment": "10 pages, 3 figures. Currently under review", "summary": "Scenario building is an established method to anticipate the future of\nemerging technologies. Its primary goal is to use narratives to map future\ntrajectories of technology development and sociotechnical adoption. Following\nthis process, risks and benefits can be identified early on, and strategies can\nbe developed that strive for desirable futures. In recent years, computer\nscience has adopted this method and applied it to various technologies,\nincluding Artificial Intelligence (AI). Because computing technologies play\nsuch an important role in shaping modern societies, it is worth exploring how\nscenarios are being used as an anticipatory tool in the field -- and what\npossible traditional uses of scenarios are not yet covered but have the\npotential to enrich the field. We address this gap by conducting a systematic\nliterature review on the use of scenario building methods in computer science\nover the last decade (n = 59). We guide the review along two main questions.\nFirst, we aim to uncover how scenarios are used in computing literature,\nfocusing especially on the rationale for why scenarios are used. Second, in\nfollowing the potential of scenario building to enhance inclusivity in\nresearch, we dive deeper into the participatory element of the existing\nscenario building literature in computer science.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u60c5\u666f\u6784\u5efa\u4f5c\u4e3a\u9884\u6d4b\u65b0\u5174\u6280\u672f\u672a\u6765\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u5206\u6790\u4e86\u8fd1\u5341\u5e74\u7684\u4f7f\u7528\u60c5\u51b5\u3002", "motivation": "\u63a2\u7d22\u60c5\u666f\u6784\u5efa\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5982\u4f55\u901a\u8fc7\u60c5\u666f\u6784\u5efa\u589e\u5f3a\u7814\u7a76\u7684\u5305\u5bb9\u6027\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff08n = 59\uff09\uff0c\u5206\u6790\u8fd1\u5341\u5e74\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u60c5\u666f\u6784\u5efa\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u7528\u9014\u548c\u53c2\u4e0e\u6027\u3002", "result": "\u63ed\u793a\u4e86\u60c5\u666f\u6784\u5efa\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684\u4f7f\u7528\u65b9\u5f0f\u53ca\u5176\u6f5c\u5728\u4ef7\u503c\uff0c\u5c24\u5176\u662f\u53c2\u4e0e\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "\u60c5\u666f\u6784\u5efa\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u53c2\u4e0e\u6027\u4ee5\u589e\u5f3a\u7814\u7a76\u7684\u5305\u5bb9\u6027\u3002"}}
{"id": "2506.05615", "pdf": "https://arxiv.org/pdf/2506.05615", "abs": "https://arxiv.org/abs/2506.05615", "authors": ["Ruipeng Zhang", "Ya-Chien Chang", "Sicun Gao"], "title": "When Maximum Entropy Misleads Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The Maximum Entropy Reinforcement Learning (MaxEnt RL) framework is a leading\napproach for achieving efficient learning and robust performance across many RL\ntasks. However, MaxEnt methods have also been shown to struggle with\nperformance-critical control problems in practice, where non-MaxEnt algorithms\ncan successfully learn. In this work, we analyze how the trade-off between\nrobustness and optimality affects the performance of MaxEnt algorithms in\ncomplex control tasks: while entropy maximization enhances exploration and\nrobustness, it can also mislead policy optimization, leading to failure in\ntasks that require precise, low-entropy policies. Through experiments on a\nvariety of control problems, we concretely demonstrate this misleading effect.\nOur analysis leads to better understanding of how to balance reward design and\nentropy maximization in challenging control problems.", "AI": {"tldr": "MaxEnt RL\u6846\u67b6\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u9762\u4e34\u7a33\u5065\u6027\u4e0e\u6700\u4f18\u6027\u7684\u6743\u8861\u95ee\u9898\uff0c\u71b5\u6700\u5927\u5316\u53ef\u80fd\u8bef\u5bfc\u7b56\u7565\u4f18\u5316\u3002", "motivation": "\u7814\u7a76MaxEnt RL\u5728\u6027\u80fd\u5173\u952e\u63a7\u5236\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u7a33\u5065\u6027\u4e0e\u6700\u4f18\u6027\u7684\u6743\u8861\u3002", "method": "\u901a\u8fc7\u591a\u79cd\u63a7\u5236\u95ee\u9898\u7684\u5b9e\u9a8c\uff0c\u5206\u6790\u71b5\u6700\u5927\u5316\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u8bef\u5bfc\u6548\u5e94\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u71b5\u6700\u5927\u5316\u53ef\u80fd\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u786e\u4f4e\u71b5\u7b56\u7565\u7684\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5e73\u8861\u5956\u52b1\u8bbe\u8ba1\u4e0e\u71b5\u6700\u5927\u5316\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u7406\u89e3\u3002"}}
{"id": "2506.05617", "pdf": "https://arxiv.org/pdf/2506.05617", "abs": "https://arxiv.org/abs/2506.05617", "authors": ["Antonia van Betteray", "Matthias Rottmann", "Karsten Kahl"], "title": "LFA applied to CNNs: Efficient Singular Value Decomposition of Convolutional Mappings by Local Fourier Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The singular values of convolutional mappings encode interesting spectral\nproperties, which can be used, e.g., to improve generalization and robustness\nof convolutional neural networks as well as to facilitate model compression.\nHowever, the computation of singular values is typically very\nresource-intensive. The naive approach involves unrolling the convolutional\nmapping along the input and channel dimensions into a large and sparse\ntwo-dimensional matrix, making the exact calculation of all singular values\ninfeasible due to hardware limitations. In particular, this is true for\nmatrices that represent convolutional mappings with large inputs and a high\nnumber of channels. Existing efficient methods leverage the Fast Fourier\ntransformation (FFT) to transform convolutional mappings into the frequency\ndomain, enabling the computation of singular values for matrices representing\nconvolutions with larger input and channel dimensions. For a constant number of\nchannels in a given convolution, an FFT can compute N singular values in O(N\nlog N) complexity. In this work, we propose an approach of complexity O(N)\nbased on local Fourier analysis, which additionally exploits the shift\ninvariance of convolutional operators. We provide a theoretical analysis of our\nalgorithm's runtime and validate its efficiency through numerical experiments.\nOur results demonstrate that our proposed method is scalable and offers a\npractical solution to calculate the entire set of singular values - along with\nthe corresponding singular vectors if needed - for high-dimensional\nconvolutional mappings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u5085\u91cc\u53f6\u5206\u6790\u7684\u4f4e\u590d\u6742\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u8ba1\u7b97\u5377\u79ef\u6620\u5c04\u7684\u5947\u5f02\u503c\u3002", "motivation": "\u5377\u79ef\u6620\u5c04\u7684\u5947\u5f02\u503c\u5177\u6709\u91cd\u8981\u7684\u8c31\u7279\u6027\uff0c\u4f46\u4f20\u7edf\u8ba1\u7b97\u65b9\u6cd5\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u96be\u4ee5\u9002\u7528\u4e8e\u9ad8\u7ef4\u8f93\u5165\u548c\u591a\u901a\u9053\u573a\u666f\u3002", "method": "\u5229\u7528\u5c40\u90e8\u5085\u91cc\u53f6\u5206\u6790\u548c\u5377\u79ef\u7b97\u5b50\u7684\u5e73\u79fb\u4e0d\u53d8\u6027\uff0c\u63d0\u51fa\u590d\u6742\u5ea6\u4e3aO(N)\u7684\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u5377\u79ef\u6620\u5c04\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8ba1\u7b97\u9ad8\u7ef4\u5377\u79ef\u6620\u5c04\u7684\u5947\u5f02\u503c\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.05628", "pdf": "https://arxiv.org/pdf/2506.05628", "abs": "https://arxiv.org/abs/2506.05628", "authors": ["Jiri Navratil", "Jarret Ross", "Payel Das", "Youssef Mroueh", "Samuel C Hoffman", "Vijil Chenthamarakshan", "Brian Belgodere"], "title": "GP-MoLFormer-Sim: Test Time Molecular Optimization through Contextual Similarity Guidance", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages main article, 21 pages total", "summary": "The ability to design molecules while preserving similarity to a target\nmolecule and/or property is crucial for various applications in drug discovery,\nchemical design, and biology. We introduce in this paper an efficient\ntraining-free method for navigating and sampling from the molecular space with\na generative Chemical Language Model (CLM), while using the molecular\nsimilarity to the target as a guide. Our method leverages the contextual\nrepresentations learned from the CLM itself to estimate the molecular\nsimilarity, which is then used to adjust the autoregressive sampling strategy\nof the CLM. At each step of the decoding process, the method tracks the\ndistance of the current generations from the target and updates the logits to\nencourage the preservation of similarity in generations. We implement the\nmethod using a recently proposed $\\sim$47M parameter SMILES-based CLM,\nGP-MoLFormer, and therefore refer to the method as GP-MoLFormer-Sim, which\nenables a test-time update of the deep generative policy to reflect the\ncontextual similarity to a set of guide molecules. The method is further\nintegrated into a genetic algorithm (GA) and tested on a set of standard\nmolecular optimization benchmarks involving property optimization, molecular\nrediscovery, and structure-based drug design. Results show that,\nGP-MoLFormer-Sim, combined with GA (GP-MoLFormer-Sim+GA) outperforms existing\ntraining-free baseline methods, when the oracle remains black-box. The findings\nin this work are a step forward in understanding and guiding the generative\nmechanisms of CLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5206\u5b50\u751f\u6210\u65b9\u6cd5GP-MoLFormer-Sim\uff0c\u901a\u8fc7\u5316\u5b66\u8bed\u8a00\u6a21\u578b\uff08CLM\uff09\u548c\u5206\u5b50\u76f8\u4f3c\u6027\u6307\u5bfc\uff0c\u4f18\u5316\u5206\u5b50\u8bbe\u8ba1\u3002\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\uff0c\u5728\u591a\u4e2a\u5206\u5b50\u4f18\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u836f\u7269\u53d1\u73b0\u548c\u5316\u5b66\u8bbe\u8ba1\u4e2d\uff0c\u8bbe\u8ba1\u76f8\u4f3c\u6027\u9ad8\u4e14\u4fdd\u7559\u76ee\u6807\u6027\u8d28\u7684\u5206\u5b50\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7CLM\u548c\u76f8\u4f3c\u6027\u6307\u5bfc\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5206\u5b50\u751f\u6210\u3002", "method": "\u5229\u7528CLM\u7684\u4e0a\u4e0b\u6587\u8868\u793a\u4f30\u8ba1\u5206\u5b50\u76f8\u4f3c\u6027\uff0c\u8c03\u6574\u81ea\u56de\u5f52\u91c7\u6837\u7b56\u7565\u3002\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\uff0c\u5728GP-MoLFormer\u6a21\u578b\u4e0a\u5b9e\u73b0GP-MoLFormer-Sim\u65b9\u6cd5\u3002", "result": "GP-MoLFormer-Sim+GA\u5728\u5206\u5b50\u4f18\u5316\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u9ed1\u76d2\u6761\u4ef6\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u548c\u6307\u5bfcCLM\u7684\u751f\u6210\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63a8\u52a8\u4e86\u5206\u5b50\u8bbe\u8ba1\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.05634", "pdf": "https://arxiv.org/pdf/2506.05634", "abs": "https://arxiv.org/abs/2506.05634", "authors": ["Saeed Hedayatian", "Stefanos Nikolaidis"], "title": "AutoQD: Automatic Discovery of Diverse Behaviors with Quality-Diversity Optimization", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "22 pages, 5 figures", "summary": "Quality-Diversity (QD) algorithms have shown remarkable success in\ndiscovering diverse, high-performing solutions, but rely heavily on\nhand-crafted behavioral descriptors that constrain exploration to predefined\nnotions of diversity. Leveraging the equivalence between policies and occupancy\nmeasures, we present a theoretically grounded approach to automatically\ngenerate behavioral descriptors by embedding the occupancy measures of policies\nin Markov Decision Processes. Our method, AutoQD, leverages random Fourier\nfeatures to approximate the Maximum Mean Discrepancy (MMD) between policy\noccupancy measures, creating embeddings whose distances reflect meaningful\nbehavioral differences. A low-dimensional projection of these embeddings that\ncaptures the most behaviorally significant dimensions is then used as\nbehavioral descriptors for off-the-shelf QD methods. We prove that our\nembeddings converge to true MMD distances between occupancy measures as the\nnumber of sampled trajectories and embedding dimensions increase. Through\nexperiments in multiple continuous control tasks we demonstrate AutoQD's\nability in discovering diverse policies without predefined behavioral\ndescriptors, presenting a well-motivated alternative to prior methods in\nunsupervised Reinforcement Learning and QD optimization. Our approach opens new\npossibilities for open-ended learning and automated behavior discovery in\nsequential decision making settings without requiring domain-specific\nknowledge.", "AI": {"tldr": "AutoQD\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u884c\u4e3a\u63cf\u8ff0\u7b26\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5165\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u7b56\u7565\u5360\u7528\u5ea6\u91cf\uff0c\u907f\u514d\u4e86\u4f9d\u8d56\u9884\u5b9a\u4e49\u591a\u6837\u6027\u63cf\u8ff0\u7b26\u7684\u9650\u5236\u3002", "motivation": "\u4f20\u7edfQD\u7b97\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u884c\u4e3a\u63cf\u8ff0\u7b26\uff0c\u9650\u5236\u4e86\u63a2\u7d22\u7684\u591a\u6837\u6027\u3002AutoQD\u65e8\u5728\u81ea\u52a8\u751f\u6210\u63cf\u8ff0\u7b26\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u8fd1\u4f3c\u7b56\u7565\u5360\u7528\u5ea6\u91cf\u7684\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\uff0c\u751f\u6210\u53cd\u6620\u884c\u4e3a\u5dee\u5f02\u7684\u5d4c\u5165\uff0c\u5e76\u5c06\u5176\u4f4e\u7ef4\u6295\u5f71\u4f5c\u4e3a\u884c\u4e3a\u63cf\u8ff0\u7b26\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eAutoQD\u80fd\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u53d1\u73b0\u591a\u6837\u7b56\u7565\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u63cf\u8ff0\u7b26\uff0c\u4e14\u5d4c\u5165\u8ddd\u79bb\u968f\u6837\u672c\u548c\u7ef4\u5ea6\u589e\u52a0\u6536\u655b\u4e8e\u771f\u5b9eMMD\u8ddd\u79bb\u3002", "conclusion": "AutoQD\u4e3a\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u548cQD\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u652f\u6301\u5f00\u653e\u5b66\u4e60\u548c\u81ea\u52a8\u5316\u884c\u4e3a\u53d1\u73b0\uff0c\u65e0\u9700\u9886\u57df\u77e5\u8bc6\u3002"}}
{"id": "2506.05636", "pdf": "https://arxiv.org/pdf/2506.05636", "abs": "https://arxiv.org/abs/2506.05636", "authors": ["Markelle Kelly", "Alex Boyd", "Sam Showalter", "Mark Steyvers", "Padhraic Smyth"], "title": "Bayesian Inference for Correlated Human Experts and Classifiers", "categories": ["cs.LG", "cs.AI"], "comment": "accepted to ICML 2025", "summary": "Applications of machine learning often involve making predictions based on\nboth model outputs and the opinions of human experts. In this context, we\ninvestigate the problem of querying experts for class label predictions, using\nas few human queries as possible, and leveraging the class probability\nestimates of pre-trained classifiers. We develop a general Bayesian framework\nfor this problem, modeling expert correlation via a joint latent\nrepresentation, enabling simulation-based inference about the utility of\nadditional expert queries, as well as inference of posterior distributions over\nunobserved expert labels. We apply our approach to two real-world medical\nclassification problems, as well as to CIFAR-10H and ImageNet-16H,\ndemonstrating substantial reductions relative to baselines in the cost of\nquerying human experts while maintaining high prediction accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u5206\u7c7b\u5668\u548c\u4e13\u5bb6\u610f\u89c1\uff0c\u4ee5\u51cf\u5c11\u4eba\u5de5\u67e5\u8be2\u6210\u672c\u5e76\u4fdd\u6301\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\uff0c\u7ed3\u5408\u6a21\u578b\u8f93\u51fa\u548c\u4e13\u5bb6\u610f\u89c1\u8fdb\u884c\u9884\u6d4b\u65f6\uff0c\u5982\u4f55\u51cf\u5c11\u4eba\u5de5\u67e5\u8be2\u6210\u672c\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u6f5c\u5728\u8868\u793a\u5efa\u6a21\u4e13\u5bb6\u76f8\u5173\u6027\uff0c\u652f\u6301\u57fa\u4e8e\u6a21\u62df\u7684\u63a8\u7406\u548c\u672a\u89c2\u5bdf\u4e13\u5bb6\u6807\u7b7e\u7684\u540e\u9a8c\u5206\u5e03\u63a8\u65ad\u3002", "result": "\u5728\u771f\u5b9e\u533b\u7597\u5206\u7c7b\u95ee\u9898\u548cCIFAR-10H\u3001ImageNet-16H\u6570\u636e\u96c6\u4e0a\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4e13\u5bb6\u67e5\u8be2\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u4eba\u5de5\u67e5\u8be2\u9700\u6c42\uff0c\u540c\u65f6\u7ef4\u6301\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9700\u8981\u7ed3\u5408\u6a21\u578b\u548c\u4e13\u5bb6\u610f\u89c1\u7684\u573a\u666f\u3002"}}
{"id": "2506.05660", "pdf": "https://arxiv.org/pdf/2506.05660", "abs": "https://arxiv.org/abs/2506.05660", "authors": ["Markian Mandzak", "Elvira Yang", "Anna Zapaishchykova", "Yu-Hui Chen", "Lucas Heilbroner", "John Zielke", "Divyanshu Tak", "Reza Mojahed-Yazdi", "Francesca Romana Mussa", "Zezhong Ye", "Sridhar Vajapeyam", "Viviana Benitez", "Ralph Salloum", "Susan N. Chi", "Houman Sotoudeh", "Jakob Seidlitz", "Sabine Mueller", "Hugo J. W. L. Aerts", "Tina Y. Poussaint", "Benjamin H. Kann"], "title": "TissUnet: Improved Extracranial Tissue and Cranium Segmentation for Children through Adulthood", "categories": ["cs.CV", "cs.AI"], "comment": "44 pages, 4 tables, 6 figures, supplementary material", "summary": "Extracranial tissues visible on brain magnetic resonance imaging (MRI) may\nhold significant value for characterizing health conditions and clinical\ndecision-making, yet they are rarely quantified. Current tools have not been\nwidely validated, particularly in settings of developing brains or underlying\npathology. We present TissUnet, a deep learning model that segments skull bone,\nsubcutaneous fat, and muscle from routine three-dimensional T1-weighted MRI,\nwith or without contrast enhancement. The model was trained on 155 paired\nMRI-computed tomography (CT) scans and validated across nine datasets covering\na wide age range and including individuals with brain tumors. In comparison to\nAI-CT-derived labels from 37 MRI-CT pairs, TissUnet achieved a median Dice\ncoefficient of 0.79 [IQR: 0.77-0.81] in a healthy adult cohort. In a second\nvalidation using expert manual annotations, median Dice was 0.83 [IQR:\n0.83-0.84] in healthy individuals and 0.81 [IQR: 0.78-0.83] in tumor cases,\noutperforming previous state-of-the-art method. Acceptability testing resulted\nin an 89% acceptance rate after adjudication by a tie-breaker(N=108 MRIs), and\nTissUnet demonstrated excellent performance in the blinded comparative review\n(N=45 MRIs), including both healthy and tumor cases in pediatric populations.\nTissUnet enables fast, accurate, and reproducible segmentation of extracranial\ntissues, supporting large-scale studies on craniofacial morphology, treatment\neffects, and cardiometabolic risk using standard brain T1w MRI.", "AI": {"tldr": "TissUnet\u662f\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u5e38\u89c4MRI\u4e2d\u5206\u5272\u9885\u5916\u7ec4\u7ec7\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9885\u5916\u7ec4\u7ec7\u5728MRI\u4e2d\u53ef\u80fd\u5bf9\u5065\u5eb7\u8bc4\u4f30\u548c\u4e34\u5e8a\u51b3\u7b56\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u672a\u5e7f\u6cdb\u9a8c\u8bc1\u3002", "method": "\u4f7f\u7528155\u5bf9MRI-CT\u626b\u63cf\u8bad\u7ec3TissUnet\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5305\u62ec\u5065\u5eb7\u6210\u4eba\u548c\u8111\u80bf\u7624\u60a3\u8005\u3002", "result": "TissUnet\u5728\u5065\u5eb7\u6210\u4eba\u4e2d\u7684\u4e2d\u4f4dDice\u7cfb\u6570\u4e3a0.79\uff0c\u5728\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u4e2d\u4e3a0.83\uff08\u5065\u5eb7\u4eba\u7fa4\uff09\u548c0.81\uff08\u80bf\u7624\u60a3\u8005\uff09\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TissUnet\u80fd\u5feb\u901f\u3001\u51c6\u786e\u3001\u53ef\u91cd\u590d\u5730\u5206\u5272\u9885\u5916\u7ec4\u7ec7\uff0c\u652f\u6301\u5927\u89c4\u6a21\u7814\u7a76\u3002"}}
{"id": "2506.05667", "pdf": "https://arxiv.org/pdf/2506.05667", "abs": "https://arxiv.org/abs/2506.05667", "authors": ["Yuhan Hao", "Zhengning Li", "Lei Sun", "Weilong Wang", "Naixin Yi", "Sheng Song", "Caihong Qin", "Mofan Zhou", "Yifei Zhan", "Peng Jia", "Xianpeng Lang"], "title": "DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models", "categories": ["cs.CV", "cs.AI"], "comment": "Benchmark:\n  https://huggingface.co/datasets/LiAuto-DriveAction/drive-action", "summary": "Vision-Language-Action (VLA) models have advanced autonomous driving, but\nexisting benchmarks still lack scenario diversity, reliable action-level\nannotation, and evaluation protocols aligned with human preferences. To address\nthese limitations, we introduce DriveAction, the first action-driven benchmark\nspecifically designed for VLA models, comprising 16,185 QA pairs generated from\n2,610 driving scenarios. DriveAction leverages real-world driving data\nproactively collected by users of production-level autonomous vehicles to\nensure broad and representative scenario coverage, offers high-level discrete\naction labels collected directly from users' actual driving operations, and\nimplements an action-rooted tree-structured evaluation framework that\nexplicitly links vision, language, and action tasks, supporting both\ncomprehensive and task-specific assessment. Our experiments demonstrate that\nstate-of-the-art vision-language models (VLMs) require both vision and language\nguidance for accurate action prediction: on average, accuracy drops by 3.3%\nwithout vision input, by 4.1% without language input, and by 8.0% without\neither. Our evaluation supports precise identification of model bottlenecks\nwith robust and consistent results, thus providing new insights and a rigorous\nfoundation for advancing human-like decisions in autonomous driving.", "AI": {"tldr": "DriveAction\u662f\u9996\u4e2a\u4e13\u4e3aVLA\u6a21\u578b\u8bbe\u8ba1\u7684\u52a8\u4f5c\u9a71\u52a8\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b16,185\u4e2aQA\u5bf9\uff0c\u57fa\u4e8e2,610\u4e2a\u9a7e\u9a76\u573a\u666f\u3002\u5b83\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u573a\u666f\u591a\u6837\u6027\u3001\u52a8\u4f5c\u6807\u6ce8\u548c\u8bc4\u4f30\u534f\u8bae\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u573a\u666f\u591a\u6837\u6027\u3001\u53ef\u9760\u7684\u52a8\u4f5c\u6807\u6ce8\u548c\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u963b\u788d\u4e86VLA\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u3002", "method": "DriveAction\u5229\u7528\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u751f\u6210QA\u5bf9\uff0c\u63d0\u4f9b\u79bb\u6563\u52a8\u4f5c\u6807\u7b7e\uff0c\u5e76\u91c7\u7528\u6811\u72b6\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u52a8\u4f5c\u4efb\u52a1\u660e\u786e\u5173\u8054\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u9700\u8981\u89c6\u89c9\u548c\u8bed\u8a00\u8f93\u5165\u624d\u80fd\u51c6\u786e\u9884\u6d4b\u52a8\u4f5c\uff0c\u7f3a\u5931\u4efb\u4e00\u8f93\u5165\u4f1a\u5bfc\u81f4\u51c6\u786e\u7387\u4e0b\u964d3.3%-8.0%\u3002", "conclusion": "DriveAction\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4eba\u7c7b\u5316\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u548c\u4e25\u683c\u57fa\u7840\uff0c\u652f\u6301\u7cbe\u786e\u8bc6\u522b\u6a21\u578b\u74f6\u9888\u3002"}}
{"id": "2506.05673", "pdf": "https://arxiv.org/pdf/2506.05673", "abs": "https://arxiv.org/abs/2506.05673", "authors": ["Sajjad Abdoli", "Freeman Lewin", "Gediminas Vasiliauskas", "Fabian Schonholz"], "title": "Peer-Ranked Precision: Creating a Foundational Dataset for Fine-Tuning Vision Models from DataSeeds' Annotated Imagery", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "28 pages, 12 figures", "summary": "The development of modern Artificial Intelligence (AI) models, particularly\ndiffusion-based models employed in computer vision and image generation tasks,\nis undergoing a paradigmatic shift in development methodologies. Traditionally\ndominated by a \"Model Centric\" approach, in which performance gains were\nprimarily pursued through increasingly complex model architectures and\nhyperparameter optimization, the field is now recognizing a more nuanced\n\"Data-Centric\" approach. This emergent framework foregrounds the quality,\nstructure, and relevance of training data as the principal driver of model\nperformance. To operationalize this paradigm shift, we introduce the\nDataSeeds.AI sample dataset (the \"DSD\"), initially comprised of approximately\n10,610 high-quality human peer-ranked photography images accompanied by\nextensive multi-tier annotations. The DSD is a foundational computer vision\ndataset designed to usher in a new standard for commercial image datasets.\nRepresenting a small fraction of DataSeed.AI's 100 million-plus image catalog,\nthe DSD provides a scalable foundation necessary for robust commercial and\nmultimodal AI development. Through this in-depth exploratory analysis, we\ndocument the quantitative improvements generated by the DSD on specific models\nagainst known benchmarks and make the code and the trained models used in our\nevaluation publicly available.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u6a21\u578b\u5f00\u53d1\u4ece\u2018\u6a21\u578b\u4e2d\u5fc3\u2019\u5411\u2018\u6570\u636e\u4e2d\u5fc3\u2019\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5e76\u4ecb\u7ecd\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6DSD\u53ca\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5b9a\u91cf\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfAI\u5f00\u53d1\u8fc7\u4e8e\u4f9d\u8d56\u590d\u6742\u6a21\u578b\u67b6\u6784\u548c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u800c\u5ffd\u89c6\u4e86\u6570\u636e\u8d28\u91cf\u7684\u91cd\u8981\u6027\u3002\u8bba\u6587\u65e8\u5728\u63a8\u52a8\u2018\u6570\u636e\u4e2d\u5fc3\u2019\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u5f15\u5165DataSeeds.AI\u6837\u672c\u6570\u636e\u96c6\uff08DSD\uff09\uff0c\u5305\u542b10,610\u5f20\u9ad8\u8d28\u91cf\u56fe\u50cf\u53ca\u591a\u5c42\u7ea7\u6807\u6ce8\uff0c\u7528\u4e8e\u8bc4\u4f30\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "DSD\u663e\u8457\u63d0\u5347\u4e86\u7279\u5b9a\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u516c\u5f00\u4ee3\u7801\u548c\u8bad\u7ec3\u6a21\u578b\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "conclusion": "DSD\u4e3a\u5546\u4e1a\u548c\u591a\u6a21\u6001AI\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u2018\u6570\u636e\u4e2d\u5fc3\u2019\u65b9\u6cd5\u7684\u5e94\u7528\u3002"}}
{"id": "2506.05680", "pdf": "https://arxiv.org/pdf/2506.05680", "abs": "https://arxiv.org/abs/2506.05680", "authors": ["Tailin Zhou", "Zhilin Chen", "Wenlong Lyu", "Zhitang Chen", "Danny H. K. Tsang", "Jun Zhang"], "title": "Learning Design-Score Manifold to Guide Diffusion Models for Offline Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "This manuscript is submitted and under review", "summary": "Optimizing complex systems, from discovering therapeutic drugs to designing\nhigh-performance materials, remains a fundamental challenge across science and\nengineering, as the underlying rules are often unknown and costly to evaluate.\nOffline optimization aims to optimize designs for target scores using\npre-collected datasets without system interaction. However, conventional\napproaches may fail beyond training data, predicting inaccurate scores and\ngenerating inferior designs. This paper introduces ManGO, a diffusion-based\nframework that learns the design-score manifold, capturing the design-score\ninterdependencies holistically. Unlike existing methods that treat design and\nscore spaces in isolation, ManGO unifies forward prediction and backward\ngeneration, attaining generalization beyond training data. Key to this is its\nderivative-free guidance for conditional generation, coupled with adaptive\ninference-time scaling that dynamically optimizes denoising paths. Extensive\nevaluations demonstrate that ManGO outperforms 24 single- and 10\nmulti-objective optimization methods across diverse domains, including\nsynthetic tasks, robot control, material design, DNA sequence, and real-world\nengineering optimization.", "AI": {"tldr": "ManGO\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u8bbe\u8ba1-\u5206\u6570\u6d41\u5f62\uff0c\u7edf\u4e00\u524d\u5411\u9884\u6d4b\u548c\u540e\u5411\u751f\u6210\uff0c\u5b9e\u73b0\u8bad\u7ec3\u6570\u636e\u5916\u7684\u6cdb\u5316\uff0c\u5e76\u5728\u591a\u9886\u57df\u4f18\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u590d\u6742\u7cfb\u7edf\u4f18\u5316\uff08\u5982\u836f\u7269\u53d1\u73b0\u548c\u6750\u6599\u8bbe\u8ba1\uff09\u901a\u5e38\u56e0\u89c4\u5219\u672a\u77e5\u4e14\u8bc4\u4f30\u6210\u672c\u9ad8\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u4f20\u7edf\u79bb\u7ebf\u4f18\u5316\u65b9\u6cd5\u5728\u8bad\u7ec3\u6570\u636e\u5916\u8868\u73b0\u4e0d\u4f73\u3002", "method": "ManGO\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b66\u4e60\u8bbe\u8ba1-\u5206\u6570\u6d41\u5f62\uff0c\u7ed3\u5408\u65e0\u5bfc\u6570\u5f15\u5bfc\u7684\u6761\u4ef6\u751f\u6210\u548c\u81ea\u9002\u5e94\u63a8\u7406\u65f6\u95f4\u7f29\u653e\uff0c\u52a8\u6001\u4f18\u5316\u53bb\u566a\u8def\u5f84\u3002", "result": "ManGO\u5728\u5408\u6210\u4efb\u52a1\u3001\u673a\u5668\u4eba\u63a7\u5236\u3001\u6750\u6599\u8bbe\u8ba1\u3001DNA\u5e8f\u5217\u548c\u5de5\u7a0b\u4f18\u5316\u7b4924\u4e2a\u5355\u76ee\u6807\u548c10\u4e2a\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "ManGO\u901a\u8fc7\u7edf\u4e00\u8bbe\u8ba1-\u5206\u6570\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u4f18\u5316\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.05683", "pdf": "https://arxiv.org/pdf/2506.05683", "abs": "https://arxiv.org/abs/2506.05683", "authors": ["Fardis Nadimi", "Payam Abdisarabshali", "Kasra Borazjani", "Jacob Chakareski", "Seyyedali Hosseinalipour"], "title": "Multi-Modal Multi-Task Federated Foundation Models for Next-Generation Extended Reality Systems: Towards Privacy-Preserving Distributed Intelligence in AR/VR/MR", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.MM"], "comment": "16 pages, 4 Figures, 8 Tables", "summary": "Extended reality (XR) systems, which consist of virtual reality (VR),\naugmented reality (AR), and mixed reality (XR), offer a transformative\ninterface for immersive, multi-modal, and embodied human-computer interaction.\nIn this paper, we envision that multi-modal multi-task (M3T) federated\nfoundation models (FedFMs) can offer transformative capabilities for XR systems\nthrough integrating the representational strength of M3T foundation models\n(FMs) with the privacy-preserving model training principles of federated\nlearning (FL). We present a modular architecture for FedFMs, which entails\ndifferent coordination paradigms for model training and aggregations. Central\nto our vision is the codification of XR challenges that affect the\nimplementation of FedFMs under the SHIFT dimensions: (1) Sensor and modality\ndiversity, (2) Hardware heterogeneity and system-level constraints, (3)\nInteractivity and embodied personalization, (4) Functional/task variability,\nand (5) Temporality and environmental variability. We illustrate the\nmanifestation of these dimensions across a set of emerging and anticipated\napplications of XR systems. Finally, we propose evaluation metrics, dataset\nrequirements, and design tradeoffs necessary for the development of\nresource-aware FedFMs in XR. This perspective aims to chart the technical and\nconceptual foundations for context-aware privacy-preserving intelligence in the\nnext generation of XR systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u591a\u4efb\u52a1\u8054\u90a6\u57fa\u7840\u6a21\u578b\uff08FedFMs\uff09\u7684\u6269\u5c55\u73b0\u5b9e\uff08XR\uff09\u7cfb\u7edf\u67b6\u6784\uff0c\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u548c\u8054\u90a6\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u7279\u6027\uff0c\u89e3\u51b3XR\u7cfb\u7edf\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u6269\u5c55\u73b0\u5b9e\uff08XR\uff09\u7cfb\u7edf\u9700\u8981\u5f3a\u5927\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002\u8054\u90a6\u57fa\u7840\u6a21\u578b\uff08FedFMs\uff09\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u591a\u4efb\u52a1\u57fa\u7840\u6a21\u578b\u548c\u8054\u90a6\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u4e3aXR\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684FedFMs\u67b6\u6784\uff0c\u5305\u62ec\u6a21\u578b\u8bad\u7ec3\u548c\u805a\u5408\u7684\u534f\u8c03\u8303\u5f0f\uff0c\u5e76\u5b9a\u4e49\u4e86\u5f71\u54cdFedFMs\u5b9e\u65bd\u7684SHIFT\u7ef4\u5ea6\uff08\u4f20\u611f\u5668\u591a\u6837\u6027\u3001\u786c\u4ef6\u5f02\u6784\u6027\u3001\u4ea4\u4e92\u6027\u3001\u4efb\u52a1\u53ef\u53d8\u6027\u548c\u65f6\u95f4\u6027\uff09\u3002", "result": "\u901a\u8fc7\u5206\u6790XR\u7cfb\u7edf\u7684\u5e94\u7528\u573a\u666f\uff0c\u5c55\u793a\u4e86SHIFT\u7ef4\u5ea6\u7684\u5177\u4f53\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u6307\u6807\u3001\u6570\u636e\u96c6\u9700\u6c42\u548c\u8bbe\u8ba1\u6743\u8861\u3002", "conclusion": "\u8bba\u6587\u4e3a\u4e0b\u4e00\u4ee3XR\u7cfb\u7edf\u4e2d\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u9690\u79c1\u4fdd\u62a4\u667a\u80fd\u5960\u5b9a\u4e86\u6280\u672f\u548c\u6982\u5ff5\u57fa\u7840\u3002"}}
{"id": "2506.05692", "pdf": "https://arxiv.org/pdf/2506.05692", "abs": "https://arxiv.org/abs/2506.05692", "authors": ["Xinghang Li", "Jingzhe Ding", "Chao Peng", "Bing Zhao", "Xiang Gao", "Hongwan Gao", "Xinchen Gu"], "title": "SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The code generation capabilities of large language models(LLMs) have emerged\nas a critical dimension in evaluating their overall performance. However, prior\nresearch has largely overlooked the security risks inherent in the generated\ncode. In this work, we introduce \\benchmark, a benchmark specifically designed\nto assess the security of LLM-generated code. The dataset encompasses a wide\nrange of common software development scenarios and vulnerability types.\nBuilding upon this benchmark, we develop an automatic evaluation framework that\nleverages both static application security testing(SAST) and LLM-based judging\nto assess the presence of security vulnerabilities in model-generated code.\nThrough the empirical evaluation of state-of-the-art LLMs on \\benchmark, we\nreveal notable deficiencies in their ability to produce vulnerability-free\ncode. Our findings highlight pressing challenges and offer actionable insights\nfor future advancements in the secure code generation performance of LLMs. The\ndata and code will be released soon.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\\benchmark\uff0c\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u5b89\u5168\u6027\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u751f\u6210\u65e0\u6f0f\u6d1e\u4ee3\u7801\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86LLM\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4e9f\u9700\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u6784\u5efa\u5305\u542b\u591a\u79cd\u5f00\u53d1\u573a\u666f\u548c\u6f0f\u6d1e\u7c7b\u578b\u7684\u57fa\u51c6\uff0c\u7ed3\u5408SAST\u548cLLM\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\u73b0\u6709LLM\u5728\u751f\u6210\u5b89\u5168\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u63d0\u5347LLM\u5b89\u5168\u4ee3\u7801\u751f\u6210\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2506.05699", "pdf": "https://arxiv.org/pdf/2506.05699", "abs": "https://arxiv.org/abs/2506.05699", "authors": ["Ramteja Sajja", "Yusuf Sermet", "Brian Fodale", "Ibrahim Demir"], "title": "Evaluating AI-Powered Learning Assistants in Engineering Higher Education: Student Engagement, Ethical Challenges, and Policy Implications", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "26 pages, 10 Figures, 6 Tables", "summary": "As generative AI tools become increasingly integrated into higher education,\nunderstanding how students interact with and perceive these technologies is\nessential for responsible and effective adoption. This study evaluates the use\nof the Educational AI Hub, an AI-powered learning framework, in undergraduate\ncivil and environmental engineering courses at a large R1 public university.\nUsing a mixed-methods approach that combines pre- and post-surveys, system\nusage logs, and qualitative analysis of the open-ended prompts and questions\nstudents posed to the AI chatbot, the research explores students' perceptions\nof trust, ethical concerns, usability, and learning outcomes. Findings reveal\nthat students appreciated the AI assistant for its convenience and comfort,\nwith nearly half reporting greater ease in using the AI tool compared to\nseeking help from instructors or teaching assistants. The tool was seen as most\nhelpful for completing homework and understanding course concepts, though\nperceptions of its instructional quality were mixed. Ethical concerns emerged\nas a key barrier to full engagement: while most students viewed AI use as\nethically acceptable, many expressed uncertainties about institutional policies\nand apprehension about potential academic misconduct. This study contributes to\nthe growing body of research on AI in education by highlighting the importance\nof usability, policy clarity, and faculty guidance in fostering meaningful AI\nengagement. The findings suggest that while students are ready to embrace AI as\na supplement to human instruction, thoughtful integration and transparent\ninstitutional frameworks are critical for ensuring student confidence, trust,\nand learning effectiveness.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u672c\u79d1\u751f\u5bf9AI\u5b66\u4e60\u6846\u67b6\u7684\u4f7f\u7528\u548c\u611f\u77e5\uff0c\u53d1\u73b0\u5b66\u751f\u5bf9AI\u5de5\u5177\u7684\u4fbf\u5229\u6027\u6301\u79ef\u6781\u6001\u5ea6\uff0c\u4f46\u5bf9\u4f26\u7406\u95ee\u9898\u5b58\u5728\u62c5\u5fe7\u3002", "motivation": "\u4e86\u89e3\u5b66\u751f\u5bf9AI\u6280\u672f\u7684\u4e92\u52a8\u4e0e\u611f\u77e5\uff0c\u4ee5\u4fc3\u8fdb\u5176\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684\u8d1f\u8d23\u4efb\u548c\u6709\u6548\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u524d\u540e\u8c03\u67e5\u3001\u7cfb\u7edf\u4f7f\u7528\u65e5\u5fd7\u548c\u5b66\u751f\u5bf9AI\u804a\u5929\u673a\u5668\u4eba\u63d0\u95ee\u7684\u5b9a\u6027\u5206\u6790\u3002", "result": "\u5b66\u751f\u8ba4\u4e3aAI\u5de5\u5177\u65b9\u4fbf\u4e14\u8212\u9002\uff0c\u4f46\u5bf9\u4f26\u7406\u95ee\u9898\u548c\u653f\u7b56\u4e0d\u660e\u786e\u6027\u8868\u793a\u62c5\u5fe7\u3002", "conclusion": "AI\u53ef\u4f5c\u4e3a\u6559\u5b66\u7684\u8865\u5145\uff0c\u4f46\u9700\u900f\u660e\u6846\u67b6\u548c\u653f\u7b56\u652f\u6301\u4ee5\u786e\u4fdd\u5b66\u751f\u4fe1\u4efb\u548c\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2506.05700", "pdf": "https://arxiv.org/pdf/2506.05700", "abs": "https://arxiv.org/abs/2506.05700", "authors": ["Yan Wang", "Yueru He", "Ruoyu Xiang", "Jeff Zhao"], "title": "RKEFino1: A Regulation Knowledge-Enhanced Large Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) hold great promise for\nfinancial applications but introduce critical accuracy and compliance\nchallenges in Digital Regulatory Reporting (DRR). To address these issues, we\npropose RKEFino1, a regulation knowledge-enhanced financial reasoning model\nbuilt upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We\nformulate two QA tasks-knowledge-based and mathematical reasoning-and introduce\na novel Numerical NER task covering financial entities in both sentences and\ntables. Experimental results demonstrate the effectiveness and generalization\ncapacity of RKEFino1 in compliance-critical financial tasks. We have released\nour model on Hugging Face.", "AI": {"tldr": "RKEFino1\u662f\u4e00\u4e2a\u57fa\u4e8eFino1\u7684\u91d1\u878d\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u9886\u57df\u77e5\u8bc6\u589e\u5f3a\uff0c\u89e3\u51b3\u4e86\u6570\u5b57\u76d1\u7ba1\u62a5\u544a\u4e2d\u7684\u51c6\u786e\u6027\u548c\u5408\u89c4\u6027\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u51c6\u786e\u6027\u548c\u5408\u89c4\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6570\u5b57\u76d1\u7ba1\u62a5\u544a\u4e2d\u3002", "method": "\u57fa\u4e8eFino1\u6a21\u578b\uff0c\u7ed3\u5408XBRL\u3001CDM\u548cMOF\u7684\u9886\u57df\u77e5\u8bc6\u8fdb\u884c\u5fae\u8c03\uff0c\u8bbe\u8ba1\u4e86\u77e5\u8bc6\u95ee\u7b54\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e86\u6570\u503cNER\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRKEFino1\u5728\u5408\u89c4\u6027\u5173\u952e\u91d1\u878d\u4efb\u52a1\u4e2d\u5177\u6709\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RKEFino1\u6210\u529f\u89e3\u51b3\u4e86\u91d1\u878d\u9886\u57df\u7684\u5408\u89c4\u6027\u95ee\u9898\uff0c\u5e76\u5df2\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2506.05702", "pdf": "https://arxiv.org/pdf/2506.05702", "abs": "https://arxiv.org/abs/2506.05702", "authors": ["Chaofan Pan", "Jiafen Liu", "Yanhua Li", "Linbo Xiong", "Fan Min", "Wei Wei", "Xin Yang"], "title": "Action-Adaptive Continual Learning: Enabling Policy Generalization under Dynamic Action Spaces", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Continual Learning (CL) is a powerful tool that enables agents to learn a\nsequence of tasks, accumulating knowledge learned in the past and using it for\nproblem-solving or future task learning. However, existing CL methods often\nassume that the agent's capabilities remain static within dynamic environments,\nwhich doesn't reflect real-world scenarios where capabilities dynamically\nchange. This paper introduces a new and realistic problem: Continual Learning\nwith Dynamic Capabilities (CL-DC), posing a significant challenge for CL\nagents: How can policy generalization across different action spaces be\nachieved? Inspired by the cortical functions, we propose an Action-Adaptive\nContinual Learning framework (AACL) to address this challenge. Our framework\ndecouples the agent's policy from the specific action space by building an\naction representation space. For a new action space, the encoder-decoder of\naction representations is adaptively fine-tuned to maintain a balance between\nstability and plasticity. Furthermore, we release a benchmark based on three\nenvironments to validate the effectiveness of methods for CL-DC. Experimental\nresults demonstrate that our framework outperforms popular methods by\ngeneralizing the policy across action spaces.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff08CL-DC\uff09\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u80fd\u529b\u53d8\u5316\u4e0b\u7684\u7b56\u7565\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86AACL\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u4f5c\u8868\u793a\u7a7a\u95f4\u5b9e\u73b0\u7b56\u7565\u4e0e\u52a8\u4f5c\u7a7a\u95f4\u7684\u89e3\u8026\u3002", "motivation": "\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u4ee3\u7406\u80fd\u529b\u9759\u6001\uff0c\u800c\u73b0\u5b9e\u573a\u666f\u4e2d\u80fd\u529b\u52a8\u6001\u53d8\u5316\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u52a8\u6001\u80fd\u529b\u4e0b\u7684\u7b56\u7565\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faAACL\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u4f5c\u8868\u793a\u7a7a\u95f4\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u81ea\u9002\u5e94\u5fae\u8c03\uff0c\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eAACL\u6846\u67b6\u5728\u4e09\u79cd\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8de8\u52a8\u4f5c\u7a7a\u95f4\u7684\u7b56\u7565\u6cdb\u5316\u3002", "conclusion": "AACL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86CL-DC\u95ee\u9898\uff0c\u4e3a\u52a8\u6001\u80fd\u529b\u4e0b\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.05716", "pdf": "https://arxiv.org/pdf/2506.05716", "abs": "https://arxiv.org/abs/2506.05716", "authors": ["Adrian Ly", "Richard Dazeley", "Peter Vamplew", "Francisco Cruz", "Sunil Aryal"], "title": "Ensemble Elastic DQN: A novel multi-step ensemble approach to address overestimation in deep value-based reinforcement learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While many algorithmic extensions to Deep Q-Networks (DQN) have been\nproposed, there remains limited understanding of how different improvements\ninteract. In particular, multi-step and ensemble style extensions have shown\npromise in reducing overestimation bias, thereby improving sample efficiency\nand algorithmic stability. In this paper, we introduce a novel algorithm called\nEnsemble Elastic Step DQN (EEDQN), which unifies ensembles with elastic step\nupdates to stabilise algorithmic performance. EEDQN is designed to address two\nmajor challenges in deep reinforcement learning: overestimation bias and sample\nefficiency. We evaluated EEDQN against standard and ensemble DQN variants\nacross the MinAtar benchmark, a set of environments that emphasise behavioral\nlearning while reducing representational complexity. Our results show that\nEEDQN achieves consistently robust performance across all tested environments,\noutperforming baseline DQN methods and matching or exceeding state-of-the-art\nensemble DQNs in final returns on most of the MinAtar environments. These\nfindings highlight the potential of systematically combining algorithmic\nimprovements and provide evidence that ensemble and multi-step methods, when\ncarefully integrated, can yield substantial gains.", "AI": {"tldr": "EEDQN\u7ed3\u5408\u96c6\u6210\u548c\u5f39\u6027\u6b65\u957f\u66f4\u65b0\uff0c\u89e3\u51b3\u4e86DQN\u4e2d\u7684\u9ad8\u4f30\u504f\u5dee\u548c\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u5728MinAtar\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u6539\u8fdb\u65b9\u6cd5\uff08\u5982\u591a\u6b65\u548c\u96c6\u6210\u6269\u5c55\uff09\u5982\u4f55\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9ad8\u4f30\u504f\u5dee\u548c\u6837\u672c\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51faEnsemble Elastic Step DQN (EEDQN)\uff0c\u7ed3\u5408\u96c6\u6210\u548c\u5f39\u6027\u6b65\u957f\u66f4\u65b0\uff0c\u7a33\u5b9a\u7b97\u6cd5\u6027\u80fd\u3002", "result": "EEDQN\u5728MinAtar\u73af\u5883\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u4f18\u4e8e\u57fa\u7ebfDQN\u65b9\u6cd5\uff0c\u5e76\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u96c6\u6210DQN\u3002", "conclusion": "\u7cfb\u7edf\u7ed3\u5408\u7b97\u6cd5\u6539\u8fdb\uff08\u5982\u96c6\u6210\u548c\u591a\u6b65\u65b9\u6cd5\uff09\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2506.05718", "pdf": "https://arxiv.org/pdf/2506.05718", "abs": "https://arxiv.org/abs/2506.05718", "authors": ["Pascal Jr Tikeng Notsawo", "Guillaume Dumas", "Guillaume Rabusseau"], "title": "Grokking Beyond the Euclidean Norm of Model Parameters", "categories": ["cs.LG", "cs.AI", "stat.ML", "I.2.6"], "comment": "67 pages, 35 figures. Forty-second International Conference on\n  Machine Learning (ICML), 2025", "summary": "Grokking refers to a delayed generalization following overfitting when\noptimizing artificial neural networks with gradient-based methods. In this\nwork, we demonstrate that grokking can be induced by regularization, either\nexplicit or implicit. More precisely, we show that when there exists a model\nwith a property $P$ (e.g., sparse or low-rank weights) that generalizes on the\nproblem of interest, gradient descent with a small but non-zero regularization\nof $P$ (e.g., $\\ell_1$ or nuclear norm regularization) results in grokking.\nThis extends previous work showing that small non-zero weight decay induces\ngrokking. Moreover, our analysis shows that over-parameterization by adding\ndepth makes it possible to grok or ungrok without explicitly using\nregularization, which is impossible in shallow cases. We further show that the\n$\\ell_2$ norm is not a reliable proxy for generalization when the model is\nregularized toward a different property $P$, as the $\\ell_2$ norm grows in many\ncases where no weight decay is used, but the model generalizes anyway. We also\nshow that grokking can be amplified solely through data selection, with any\nother hyperparameter fixed.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u6b63\u5219\u5316\uff08\u663e\u5f0f\u6216\u9690\u5f0f\uff09\u8bf1\u5bfc\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5ef6\u8fdf\u6cdb\u5316\u73b0\u8c61\uff08grokking\uff09\uff0c\u5e76\u6269\u5c55\u4e86\u6b64\u524d\u5173\u4e8e\u6743\u91cd\u8870\u51cf\u7684\u7814\u7a76\u3002", "motivation": "\u7814\u7a76grokking\u73b0\u8c61\u53ca\u5176\u4e0e\u6b63\u5219\u5316\u7684\u5173\u7cfb\uff0c\u63a2\u7d22\u4e0d\u540c\u6b63\u5219\u5316\u65b9\u6cd5\u5bf9\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u548c\u5c0f\u89c4\u6a21\u6b63\u5219\u5316\uff08\u5982\u2113\u2081\u6216\u6838\u8303\u6570\uff09\uff0c\u5206\u6790\u4e0d\u540c\u6a21\u578b\u7ed3\u6784\u548c\u6570\u636e\u9009\u62e9\u5bf9grokking\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6df1\u5ea6\u6a21\u578b\u53ef\u901a\u8fc7\u9690\u5f0f\u6b63\u5219\u5316\u5b9e\u73b0grokking\uff0c\u4e14\u2113\u2082\u8303\u6570\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u4e0d\u80fd\u53ef\u9760\u53cd\u6620\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6b63\u5219\u5316\u548c\u6570\u636e\u9009\u62e9\u5747\u53ef\u8bf1\u5bfc\u6216\u589e\u5f3agrokking\u73b0\u8c61\uff0c\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.05721", "pdf": "https://arxiv.org/pdf/2506.05721", "abs": "https://arxiv.org/abs/2506.05721", "authors": ["Dumindu Tissera", "Omar Awadallah", "Muhammad Umair Danish", "Ayan Sadhu", "Katarina Grolinger"], "title": "Any-Class Presence Likelihood for Robust Multi-Label Classification with Abundant Negative Data", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T05 (Primary) 62H30 (Secondary)", "I.2.6; I.5.4"], "comment": null, "summary": "Multi-label Classification (MLC) assigns an instance to one or more\nnon-exclusive classes. A challenge arises when the dataset contains a large\nproportion of instances with no assigned class, referred to as negative data,\nwhich can overwhelm the learning process and hinder the accurate identification\nand classification of positive instances. Nevertheless, it is common in MLC\napplications such as industrial defect detection, agricultural disease\nidentification, and healthcare diagnosis to encounter large amounts of negative\ndata. Assigning a separate negative class to these instances further\ncomplicates the learning objective and introduces unnecessary redundancies. To\naddress this challenge, we redesign standard MLC loss functions by deriving a\nlikelihood of any class being present, formulated by a normalized weighted\ngeometric mean of the predicted class probabilities. We introduce a\nregularization parameter that controls the relative contribution of the absent\nclass probabilities to the any-class presence likelihood in positive instances.\nThe any-class presence likelihood complements the multi-label learning by\nencouraging the network to become more aware of implicit positive instances and\nimprove the label classification within those positive instances. Experiments\non large-scale datasets with negative data: SewerML, modified COCO, and\nChestX-ray14, across various networks and base loss functions show that our\nloss functions consistently improve MLC performance of their standard loss\ncounterparts, achieving gains of up to 6.01 percentage points in F1, 8.06 in\nF2, and 3.11 in mean average precision, all without additional parameters or\ncomputational complexity. Code available at:\nhttps://github.com/ML-for-Sensor-Data-Western/gmean-mlc", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u591a\u6807\u7b7e\u5206\u7c7b\uff08MLC\uff09\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u52a0\u6743\u51e0\u4f55\u5e73\u5747\u9884\u6d4b\u7c7b\u522b\u6982\u7387\uff0c\u89e3\u51b3\u4e86\u8d1f\u6570\u636e\u8fc7\u591a\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u7684\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u8d1f\u6570\u636e\uff08\u672a\u5206\u914d\u4efb\u4f55\u7c7b\u522b\u7684\u5b9e\u4f8b\uff09\u8fc7\u591a\u4f1a\u5e72\u6270\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5f71\u54cd\u6b63\u5b9e\u4f8b\u7684\u51c6\u786e\u5206\u7c7b\u3002", "method": "\u91cd\u65b0\u8bbe\u8ba1\u6807\u51c6MLC\u635f\u5931\u51fd\u6570\uff0c\u5f15\u5165\u5f52\u4e00\u5316\u52a0\u6743\u51e0\u4f55\u5e73\u5747\u9884\u6d4b\u7c7b\u522b\u6982\u7387\uff0c\u5e76\u6dfb\u52a0\u6b63\u5219\u5316\u53c2\u6570\u63a7\u5236\u8d1f\u7c7b\u522b\u7684\u8d21\u732e\u3002", "result": "\u5728\u591a\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86MLC\u6027\u80fd\uff0cF1\u3001F2\u548c\u5e73\u5747\u7cbe\u5ea6\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570\u5728\u4e0d\u589e\u52a0\u53c2\u6570\u6216\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u591a\u6807\u7b7e\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.05725", "pdf": "https://arxiv.org/pdf/2506.05725", "abs": "https://arxiv.org/abs/2506.05725", "authors": ["Fang Wu", "Vijay Prakash Dwivedi", "Jure Leskovec"], "title": "Large Language Models are Good Relational Learners", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, yet their application to relational deep learning (RDL)\nremains underexplored. Existing approaches adapt LLMs by traversing relational\nlinks between entities in a database and converting the structured data into\nflat text documents. Still, this text-based serialization disregards critical\nrelational structures, introduces redundancy, and often exceeds standard LLM\ncontext lengths. We introduce Rel-LLM, a novel architecture that utilizes a\ngraph neural network (GNN)- based encoder to generate structured relational\nprompts for LLMs within a retrieval-augmented generation (RAG) framework.\nUnlike traditional text-based serialization approaches, our method preserves\nthe inherent relational structure of databases while enabling LLMs to\neffectively process and reason over complex entity relationships. Specifically,\nthe GNN encoder extracts a local subgraph around an entity to build feature\nrepresentations that contain relevant entity relationships and temporal\ndependencies. These representations are transformed into structured prompts\nusing a denormalization process, effectively allowing the LLM to reason over\nrelational structures. Through extensive experiments, we demonstrate that\nRel-LLM outperforms existing methods on key RDL tasks, offering a scalable and\nefficient approach to integrating LLMs with structured data sources. Code is\navailable at https://github.com/smiles724/Rel-LLM.", "AI": {"tldr": "Rel-LLM\u662f\u4e00\u79cd\u65b0\u67b6\u6784\uff0c\u5229\u7528GNN\u7f16\u7801\u5668\u5728RAG\u6846\u67b6\u4e2d\u4e3aLLM\u751f\u6210\u7ed3\u6784\u5316\u5173\u7cfb\u63d0\u793a\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u7ed3\u6784\u5316\u6570\u636e\u8f6c\u6362\u4e3a\u6587\u672c\u65f6\u5ffd\u7565\u4e86\u5173\u7cfb\u7ed3\u6784\uff0c\u5bfc\u81f4\u5197\u4f59\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u95ee\u9898\u3002", "method": "\u4f7f\u7528GNN\u7f16\u7801\u5668\u63d0\u53d6\u5c40\u90e8\u5b50\u56fe\uff0c\u751f\u6210\u5305\u542b\u5b9e\u4f53\u5173\u7cfb\u548c\u65f6\u5e8f\u4f9d\u8d56\u7684\u7279\u5f81\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u53cd\u89c4\u8303\u5316\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRel-LLM\u5728\u5173\u952eRDL\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Rel-LLM\u4e3aLLM\u4e0e\u7ed3\u6784\u5316\u6570\u636e\u6e90\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.05736", "pdf": "https://arxiv.org/pdf/2506.05736", "abs": "https://arxiv.org/abs/2506.05736", "authors": ["En Yu", "Jie Lu", "Guangquan Zhang"], "title": "Generalized Incremental Learning under Concept Drift across Evolving Data Streams", "categories": ["cs.LG", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Real-world data streams exhibit inherent non-stationarity characterized by\nconcept drift, posing significant challenges for adaptive learning systems.\nWhile existing methods address isolated distribution shifts, they overlook the\ncritical co-evolution of label spaces and distributions under limited\nsupervision and persistent uncertainty. To address this, we formalize\nGeneralized Incremental Learning under Concept Drift (GILCD), characterizing\nthe joint evolution of distributions and label spaces in open-environment\nstreaming contexts, and propose a novel framework called Calibrated Source-Free\nAdaptation (CSFA). First, CSFA introduces a training-free prototype calibration\nmechanism that dynamically fuses emerging prototypes with base representations,\nenabling stable new-class identification without optimization overhead. Second,\nwe design a novel source-free adaptation algorithm, i.e., Reliable Surrogate\nGap Sharpness-aware (RSGS) minimization. It integrates sharpness-aware\nperturbation loss optimization with surrogate gap minimization, while employing\nentropy-based uncertainty filtering to discard unreliable samples. This\nmechanism ensures robust distribution alignment and mitigates generalization\ndegradation caused by uncertainties. Therefore, CSFA establishes a unified\nframework for stable adaptation to evolving semantics and distributions in\nopen-world streaming scenarios. Extensive experiments validate the superior\nperformance and effectiveness of CSFA compared to state-of-the-art approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCSFA\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5f00\u653e\u73af\u5883\u6d41\u6570\u636e\u4e2d\u7684\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u539f\u578b\u6821\u51c6\u548c\u65e0\u6e90\u9002\u5e94\u7b97\u6cd5\u5b9e\u73b0\u7a33\u5b9a\u9002\u5e94\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u6d41\u5177\u6709\u975e\u5e73\u7a33\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u6807\u7b7e\u7a7a\u95f4\u548c\u5206\u5e03\u7684\u8054\u5408\u6f14\u5316\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "CSFA\u7ed3\u5408\u4e86\u8bad\u7ec3\u65e0\u5173\u7684\u539f\u578b\u6821\u51c6\u673a\u5236\u548c\u57fa\u4e8e\u53ef\u9760\u66ff\u4ee3\u95f4\u9699\u7684\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\u7b97\u6cd5\uff0c\u52a8\u6001\u9002\u5e94\u65b0\u7c7b\u5e76\u8fc7\u6ee4\u4e0d\u53ef\u9760\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCSFA\u5728\u5f00\u653e\u4e16\u754c\u6d41\u6570\u636e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CSFA\u4e3a\u5f00\u653e\u73af\u5883\u6d41\u6570\u636e\u4e2d\u7684\u8bed\u4e49\u548c\u5206\u5e03\u6f14\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7a33\u5b9a\u9002\u5e94\u6846\u67b6\u3002"}}
{"id": "2506.05739", "pdf": "https://arxiv.org/pdf/2506.05739", "abs": "https://arxiv.org/abs/2506.05739", "authors": ["Zhilong Wang", "Neha Nagaraja", "Lan Zhang", "Hayretdin Bahsi", "Pawan Patil", "Peng Liu"], "title": "To Protect the LLM Agent Against the Prompt Injection Attack with Polymorphic Prompt", "categories": ["cs.CR", "cs.AI"], "comment": "To appear in the Industry Track of the 55th Annual IEEE/IFIP\n  International Conference on Dependable Systems and Networks (DSN 2025)", "summary": "LLM agents are widely used as agents for customer support, content\ngeneration, and code assistance. However, they are vulnerable to prompt\ninjection attacks, where adversarial inputs manipulate the model's behavior.\nTraditional defenses like input sanitization, guard models, and guardrails are\neither cumbersome or ineffective. In this paper, we propose a novel,\nlightweight defense mechanism called Polymorphic Prompt Assembling (PPA), which\nprotects against prompt injection with near-zero overhead. The approach is\nbased on the insight that prompt injection requires guessing and breaking the\nstructure of the system prompt. By dynamically varying the structure of system\nprompts, PPA prevents attackers from predicting the prompt structure, thereby\nenhancing security without compromising performance. We conducted experiments\nto evaluate the effectiveness of PPA against existing attacks and compared it\nwith other defense methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPPA\u7684\u8f7b\u91cf\u7ea7\u9632\u5fa1\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u53d8\u5316\u7cfb\u7edf\u63d0\u793a\u7684\u7ed3\u6784\u6765\u9632\u6b62\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "LLM\u4ee3\u7406\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5ba2\u6237\u652f\u6301\u3001\u5185\u5bb9\u751f\u6210\u548c\u4ee3\u7801\u8f85\u52a9\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u4f20\u7edf\u9632\u5fa1\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u6216\u7e41\u7410\u3002", "method": "\u63d0\u51faPPA\u673a\u5236\uff0c\u52a8\u6001\u53d8\u5316\u7cfb\u7edf\u63d0\u793a\u7ed3\u6784\uff0c\u4f7f\u653b\u51fb\u8005\u96be\u4ee5\u9884\u6d4b\u63d0\u793a\u7ed3\u6784\u3002", "result": "PPA\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u9632\u5fa1\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u4e14\u6027\u80fd\u5f00\u9500\u6781\u4f4e\u3002", "conclusion": "PPA\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u9632\u5fa1\u673a\u5236\uff0c\u80fd\u663e\u8457\u63d0\u5347LLM\u4ee3\u7406\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2506.05743", "pdf": "https://arxiv.org/pdf/2506.05743", "abs": "https://arxiv.org/abs/2506.05743", "authors": ["Ruining Sun", "Hongsheng Hu", "Wei Luo", "Zhaoxi Zhang", "Yanjun Zhang", "Haizhuan Yuan", "Leo Yu Zhang"], "title": "When Better Features Mean Greater Risks: The Performance-Privacy Trade-Off in Contrastive Learning", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted In ACM ASIA Conference on Computer and Communications\n  Security (ASIA CCS '25), August 25-29, 2025, Ha Noi, Vietnam. For Code, see\n  https://github.com/SeroneySun/LpLA_code", "summary": "With the rapid advancement of deep learning technology, pre-trained encoder\nmodels have demonstrated exceptional feature extraction capabilities, playing a\npivotal role in the research and application of deep learning. However, their\nwidespread use has raised significant concerns about the risk of training data\nprivacy leakage. This paper systematically investigates the privacy threats\nposed by membership inference attacks (MIAs) targeting encoder models, focusing\non contrastive learning frameworks. Through experimental analysis, we reveal\nthe significant impact of model architecture complexity on membership privacy\nleakage: As more advanced encoder frameworks improve feature-extraction\nperformance, they simultaneously exacerbate privacy-leakage risks. Furthermore,\nthis paper proposes a novel membership inference attack method based on the\np-norm of feature vectors, termed the Embedding Lp-Norm Likelihood Attack\n(LpLA). This method infers membership status, by leveraging the statistical\ndistribution characteristics of the p-norm of feature vectors. Experimental\nresults across multiple datasets and model architectures demonstrate that LpLA\noutperforms existing methods in attack performance and robustness, particularly\nunder limited attack knowledge and query volumes. This study not only uncovers\nthe potential risks of privacy leakage in contrastive learning frameworks, but\nalso provides a practical basis for privacy protection research in encoder\nmodels. We hope that this work will draw greater attention to the privacy risks\nassociated with self-supervised learning models and shed light on the\nimportance of a balance between model utility and training data privacy. Our\ncode is publicly available at: https://github.com/SeroneySun/LpLA_code.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9488\u5bf9\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4e2d\u7f16\u7801\u5668\u6a21\u578b\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\uff08MIA\uff09\u9690\u79c1\u5a01\u80c1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u5411\u91cfp-\u8303\u6570\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5LpLA\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u8bad\u7ec3\u6570\u636e\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u5f15\u8d77\u5173\u6ce8\uff0c\u672c\u6587\u65e8\u5728\u63ed\u793a\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u9690\u79c1\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u5411\u91cfp-\u8303\u6570\u7edf\u8ba1\u5206\u5e03\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5LpLA\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u6a21\u578b\u67b6\u6784\u590d\u6742\u5ea6\u5bf9\u9690\u79c1\u6cc4\u9732\u7684\u5f71\u54cd\u3002", "result": "LpLA\u5728\u653b\u51fb\u6027\u80fd\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6709\u9650\u653b\u51fb\u77e5\u8bc6\u548c\u67e5\u8be2\u91cf\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u4e3a\u7f16\u7801\u5668\u6a21\u578b\u9690\u79c1\u4fdd\u62a4\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8df5\u57fa\u7840\uff0c\u547c\u5401\u5173\u6ce8\u6a21\u578b\u6548\u7528\u4e0e\u9690\u79c1\u7684\u5e73\u8861\u3002"}}
{"id": "2506.05748", "pdf": "https://arxiv.org/pdf/2506.05748", "abs": "https://arxiv.org/abs/2506.05748", "authors": ["Rudransh Agnihotri", "Ananya Pandey"], "title": "Efficient Online RFT with Plug-and-Play LLM Judges: Unlocking State-of-the-Art Performance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reward-model training is the cost bottleneck in modern Reinforcement Learning\nHuman Feedback (RLHF) pipelines, often requiring tens of billions of parameters\nand an offline preference-tuning phase. In the proposed method, a frozen,\ninstruction-tuned 7B LLM is augmented with only a one line JSON rubric and a\nrank-16 LoRA adapter (affecting just 0.8% of the model's parameters), enabling\nit to serve as a complete substitute for the previously used heavyweight\nevaluation models. The plug-and-play judge achieves 96.2% accuracy on\nRewardBench, outperforming specialized reward networks ranging from 27B to 70B\nparameters. Additionally, it allows a 7B actor to outperform the top 70B DPO\nbaseline, which scores 61.8%, by achieving 92% exact match accuracy on GSM-8K\nutilizing online PPO. Thorough ablations indicate that (i) six in context\ndemonstrations deliver the majority of the zero-to-few-shot improvements\n(+2pp), and (ii) the LoRA effectively addresses the remaining disparity,\nparticularly in the safety and adversarial Chat-Hard segments. The proposed\nmodel introduces HH-Rationales, a subset of 10,000 pairs from Anthropic\nHH-RLHF, to examine interpretability, accompanied by human generated\njustifications. GPT-4 scoring indicates that our LoRA judge attains\napproximately = 9/10 in similarity to human explanations, while zero-shot\njudges score around =5/10. These results indicate that the combination of\nprompt engineering and tiny LoRA produces a cost effective, transparent, and\neasily adjustable reward function, removing the offline phase while achieving\nnew state-of-the-art outcomes for both static evaluation and online RLHF.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u5355\u884cJSON\u89c4\u5219\u548c\u8f7b\u91cf\u7ea7LoRA\u9002\u914d\u5668\uff0c7B LLM\u6210\u529f\u66ff\u4ee3\u4e86\u4f20\u7edf\u91cd\u578b\u5956\u52b1\u6a21\u578b\uff0c\u5728RewardBench\u4e0a\u8fbe\u523096.2%\u51c6\u786e\u7387\uff0c\u5e76\u5728\u5728\u7ebfRLHF\u4e2d\u5b9e\u73b0\u65b0SOTA\u3002", "motivation": "\u89e3\u51b3RLHF\u4e2d\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u7684\u9ad8\u6210\u672c\u548c\u590d\u6742\u6027\uff0c\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u76847B LLM\uff0c\u4ec5\u9700\u5355\u884cJSON\u89c4\u5219\u548crank-16 LoRA\u9002\u914d\u5668\uff08\u5f71\u54cd0.8%\u53c2\u6570\uff09\uff0c\u65e0\u9700\u79bb\u7ebf\u8c03\u4f18\u3002", "result": "\u5728RewardBench\u4e0a96.2%\u51c6\u786e\u7387\uff0c\u8d85\u8d8a27B-70B\u5956\u52b1\u7f51\u7edc\uff1b7B\u6a21\u578b\u5728GSM-8K\u4e0a92%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e70B DPO\u57fa\u7ebf\u3002", "conclusion": "\u8f7b\u91cf\u7ea7LoRA\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u900f\u660e\u4e14\u53ef\u8c03\u7684\u5956\u52b1\u51fd\u6570\uff0c\u6d88\u9664\u4e86\u79bb\u7ebf\u9636\u6bb5\u5e76\u8fbe\u5230\u65b0SOTA\u3002"}}
{"id": "2506.05751", "pdf": "https://arxiv.org/pdf/2506.05751", "abs": "https://arxiv.org/abs/2506.05751", "authors": ["Antrea Christou", "Chris Davis Jaldi", "Joseph Zalewski", "Hande K\u00fc\u00e7\u00fck McGinty", "Pascal Hitzler", "Cogan Shimizu"], "title": "An Ontology for Representing Curriculum and Learning Material", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Educational, learning, and training materials have become extremely\ncommonplace across the Internet. Yet, they frequently remain disconnected from\neach other, fall into platform silos, and so on. One way to overcome this is to\nprovide a mechanism to integrate the material and provide cross-links across\ntopics.\n  In this paper, we present the Curriculum KG Ontology, which we use as a\nframework for the dense interlinking of educational materials, by first\nstarting with organizational and broad pedagogical principles. We provide a\nmaterialized graph for the Prototype Open Knowledge Network use-case, and\nvalidate it using competency questions sourced from domain experts and\neducators.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCurriculum KG Ontology\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5bc6\u96c6\u94fe\u63a5\u6559\u80b2\u6750\u6599\uff0c\u89e3\u51b3\u4e92\u8054\u7f51\u4e0a\u6559\u80b2\u8d44\u6e90\u7684\u5206\u6563\u95ee\u9898\u3002", "motivation": "\u4e92\u8054\u7f51\u4e0a\u7684\u6559\u80b2\u8d44\u6e90\u5206\u6563\u4e14\u5b64\u7acb\uff0c\u7f3a\u4e4f\u6574\u5408\u4e0e\u8de8\u4e3b\u9898\u94fe\u63a5\u3002", "method": "\u4f7f\u7528Curriculum KG Ontology\u6846\u67b6\uff0c\u4ece\u7ec4\u7ec7\u548c\u6559\u5b66\u539f\u5219\u51fa\u53d1\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u539f\u578b\u5f00\u653e\u77e5\u8bc6\u7f51\u7edc\u3002", "result": "\u901a\u8fc7\u9886\u57df\u4e13\u5bb6\u548c\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u7684\u80dc\u4efb\u529b\u95ee\u9898\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6559\u80b2\u6750\u6599\u7684\u6574\u5408\u4e0e\u94fe\u63a5\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.05752", "pdf": "https://arxiv.org/pdf/2506.05752", "abs": "https://arxiv.org/abs/2506.05752", "authors": ["Zhongying Wang", "Thoai D. Ngo", "Hamidreza Zoraghein", "Benjamin Lucas", "Morteza Karimzadeh"], "title": "Integrating Spatiotemporal Features in LSTM for Spatially Informed COVID-19 Hospitalization Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "36 pages, 12 figures. This is the accepted version of the article\n  published in International Journal of Geographical Information Science. DOI\n  will be added upon publication", "summary": "The COVID-19 pandemic's severe impact highlighted the need for accurate,\ntimely hospitalization forecasting to support effective healthcare planning.\nHowever, most forecasting models struggled, especially during variant surges,\nwhen they were needed most. This study introduces a novel Long Short-Term\nMemory (LSTM) framework for forecasting daily state-level incident\nhospitalizations in the United States. We present a spatiotemporal feature,\nSocial Proximity to Hospitalizations (SPH), derived from Facebook's Social\nConnectedness Index to improve forecasts. SPH serves as a proxy for interstate\npopulation interaction, capturing transmission dynamics across space and time.\nOur parallel LSTM architecture captures both short- and long-term temporal\ndependencies, and our multi-horizon ensembling strategy balances consistency\nand forecasting error. Evaluation against COVID-19 Forecast Hub ensemble models\nduring the Delta and Omicron surges reveals superiority of our model. On\naverage, our model surpasses the ensemble by 27, 42, 54, and 69\nhospitalizations per state on the $7^{th}$, $14^{th}$, $21^{st}$, and $28^{th}$\nforecast days, respectively, during the Omicron surge. Data-ablation\nexperiments confirm SPH's predictive power, highlighting its effectiveness in\nenhancing forecasting models. This research not only advances hospitalization\nforecasting but also underscores the significance of spatiotemporal features,\nsuch as SPH, in refining predictive performance in modeling the complex\ndynamics of infectious disease spread.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLSTM\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u7a7a\u7279\u5f81SPH\uff0c\u663e\u8457\u63d0\u5347\u4e86COVID-19\u4f4f\u9662\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u53d8\u5f02\u682a\u6fc0\u589e\u671f\u95f4\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "COVID-19\u5927\u6d41\u884c\u51f8\u663e\u4e86\u51c6\u786e\u3001\u53ca\u65f6\u7684\u4f4f\u9662\u9884\u6d4b\u5bf9\u533b\u7597\u89c4\u5212\u7684\u91cd\u8981\u6027\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u53d8\u5f02\u682a\u6fc0\u589e\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f15\u5165LSTM\u6846\u67b6\u548c\u65f6\u7a7a\u7279\u5f81SPH\uff08\u57fa\u4e8eFacebook\u793e\u4ea4\u8fde\u63a5\u6307\u6570\uff09\uff0c\u91c7\u7528\u5e76\u884c\u67b6\u6784\u548c\u591a\u65f6\u95f4\u8303\u56f4\u96c6\u6210\u7b56\u7565\u3002", "result": "\u5728Delta\u548cOmicron\u6fc0\u589e\u671f\u95f4\uff0c\u6a21\u578b\u6bd4\u73b0\u6709\u96c6\u6210\u6a21\u578b\u5e73\u5747\u6bcf\u5dde\u5206\u522b\u51cf\u5c1127\u300142\u300154\u548c69\u4f8b\u4f4f\u9662\u9884\u6d4b\u8bef\u5dee\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u63d0\u5347\u4e86\u4f4f\u9662\u9884\u6d4b\u6280\u672f\uff0c\u8fd8\u8bc1\u660e\u4e86\u65f6\u7a7a\u7279\u5f81\uff08\u5982SPH\uff09\u5728\u4f20\u67d3\u75c5\u4f20\u64ad\u5efa\u6a21\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.05755", "pdf": "https://arxiv.org/pdf/2506.05755", "abs": "https://arxiv.org/abs/2506.05755", "authors": ["Yang Li", "Zhi Chen"], "title": "FlowOE: Imitation Learning with Flow Policy from Ensemble RL Experts for Optimal Execution under Heston Volatility and Concave Market Impacts", "categories": ["cs.LG", "cs.AI", "cs.CE", "q-fin.CP", "q-fin.TR"], "comment": "3 figures, 3 algorithms, 7 tables", "summary": "Optimal execution in financial markets refers to the process of strategically\ntransacting a large volume of assets over a period to achieve the best possible\noutcome by balancing the trade-off between market impact costs and timing or\nvolatility risks. Traditional optimal execution strategies, such as static\nAlmgren-Chriss models, often prove suboptimal in dynamic financial markets.\nThis paper propose flowOE, a novel imitation learning framework based on flow\nmatching models, to address these limitations. FlowOE learns from a diverse set\nof expert traditional strategies and adaptively selects the most suitable\nexpert behavior for prevailing market conditions. A key innovation is the\nincorporation of a refining loss function during the imitation process,\nenabling flowOE not only to mimic but also to improve upon the learned expert\nactions. To the best of our knowledge, this work is the first to apply flow\nmatching models in a stochastic optimal execution problem. Empirical\nevaluations across various market conditions demonstrate that flowOE\nsignificantly outperforms both the specifically calibrated expert models and\nother traditional benchmarks, achieving higher profits with reduced risk. These\nresults underscore the practical applicability and potential of flowOE to\nenhance adaptive optimal execution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u6a21\u578b\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6flowOE\uff0c\u7528\u4e8e\u4f18\u5316\u91d1\u878d\u5e02\u573a\u7684\u6267\u884c\u7b56\u7565\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6700\u4f18\u6267\u884c\u7b56\u7565\uff08\u5982\u9759\u6001Almgren-Chriss\u6a21\u578b\uff09\u5728\u52a8\u6001\u5e02\u573a\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0cflowOE\u65e8\u5728\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "method": "flowOE\u901a\u8fc7\u6d41\u5339\u914d\u6a21\u578b\u6a21\u4eff\u591a\u6837\u5316\u7684\u4e13\u5bb6\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u7cbe\u70bc\u635f\u5931\u51fd\u6570\u4f18\u5316\u5b66\u4e60\u8fc7\u7a0b\uff0c\u52a8\u6001\u9002\u5e94\u5e02\u573a\u6761\u4ef6\u3002", "result": "\u5b9e\u8bc1\u8868\u660eflowOE\u5728\u591a\u79cd\u5e02\u573a\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u4e13\u5bb6\u6a21\u578b\u548c\u4f20\u7edf\u57fa\u51c6\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6536\u76ca\u548c\u66f4\u4f4e\u98ce\u9669\u3002", "conclusion": "flowOE\u5c55\u793a\u4e86\u5728\u968f\u673a\u6700\u4f18\u6267\u884c\u95ee\u9898\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6f5c\u529b\uff0c\u4e3a\u81ea\u9002\u5e94\u6267\u884c\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.05759", "pdf": "https://arxiv.org/pdf/2506.05759", "abs": "https://arxiv.org/abs/2506.05759", "authors": ["Guang-Xing Li"], "title": "Revealing hidden correlations from complex spatial distributions: Adjacent Correlation Analysis", "categories": ["physics.comp-ph", "astro-ph.IM", "cs.AI", "math.DS"], "comment": "Code avaliable at\n  https://github.com/gxli/Adjacent-Correlation-Analysis", "summary": "Physics has been transforming our view of nature for centuries. While\ncombining physical knowledge with computational approaches has enabled detailed\nmodeling of physical systems' evolution, understanding the emergence of\npatterns and structures remains limited. Correlations between quantities are\nthe most reliable approach to describe relationships between different\nvariables. However, for complex patterns, directly searching for correlations\nis often impractical, as complexity and spatial inhomogeneity can obscure\ncorrelations. We discovered that the key is to search for correlations in local\nregions and developed a new method, adjacent correlation analysis, to extract\nsuch correlations and represent them in phase space. When multiple observations\nare available, a useful way to study a system is to analyze distributions in\nphase space using the Probability Density Function (PDF). Adjacent correlation\nanalysis evaluates vectors representing local correlations, which can be\noverlaid on the PDF plot to form the adjacent correlation plot. These\ncorrelation vectors often exhibit remarkably regular patterns and may lead to\nthe discovery of new laws. The vectors we derive are equivalent to the vector\nfield in dynamical systems on the attracting manifold. By efficiently\nrepresenting spatial patterns as correlation vectors in phase space, our\napproach opens avenues for classification, prediction, parameter fitting, and\nforecasting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u2014\u2014\u76f8\u90bb\u76f8\u5173\u6027\u5206\u6790\uff0c\u7528\u4e8e\u63d0\u53d6\u5c40\u90e8\u76f8\u5173\u6027\u5e76\u5728\u76f8\u7a7a\u95f4\u4e2d\u8868\u793a\uff0c\u4ece\u800c\u63ed\u793a\u590d\u6742\u6a21\u5f0f\u4e2d\u7684\u89c4\u5f8b\u3002", "motivation": "\u7406\u89e3\u590d\u6742\u7cfb\u7edf\u4e2d\u6a21\u5f0f\u548c\u7ed3\u6784\u7684\u6d8c\u73b0\u662f\u7269\u7406\u5b66\u4e2d\u7684\u6311\u6218\uff0c\u76f4\u63a5\u641c\u7d22\u76f8\u5173\u6027\u56e0\u590d\u6742\u6027\u548c\u7a7a\u95f4\u4e0d\u5747\u5300\u6027\u800c\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u901a\u8fc7\u76f8\u90bb\u76f8\u5173\u6027\u5206\u6790\u63d0\u53d6\u5c40\u90e8\u76f8\u5173\u6027\uff0c\u5e76\u7528\u76f8\u7a7a\u95f4\u4e2d\u7684\u5411\u91cf\u8868\u793a\uff0c\u7ed3\u5408\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\uff08PDF\uff09\u5206\u6790\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u8868\u793a\u7a7a\u95f4\u6a21\u5f0f\uff0c\u63ed\u793a\u89c4\u5f8b\u6027\uff0c\u5e76\u53ef\u7528\u4e8e\u5206\u7c7b\u3001\u9884\u6d4b\u3001\u53c2\u6570\u62df\u5408\u548c\u9884\u62a5\u3002", "conclusion": "\u76f8\u90bb\u76f8\u5173\u6027\u5206\u6790\u4e3a\u590d\u6742\u7cfb\u7edf\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u6709\u671b\u53d1\u73b0\u65b0\u89c4\u5f8b\u3002"}}
{"id": "2506.05767", "pdf": "https://arxiv.org/pdf/2506.05767", "abs": "https://arxiv.org/abs/2506.05767", "authors": ["Bi Huo", "Bin Tu", "Cheng Qin", "Da Zheng", "Debing Zhang", "Dongjie Zhang", "En Li", "Fu Guo", "Jian Yao", "Jie Lou", "Junfeng Tian", "Li Hu", "Ran Zhu", "Shengdong Chen", "Shuo Liu", "Su Guang", "Te Wo", "Weijun Zhang", "Xiaoming Shi", "Xinxin Peng", "Xing Wu", "Yawen Liu", "Yuqiu Ji", "Ze Wen", "Zhenhai Liu", "Zichao Li", "Zilong Liao"], "title": "dots.llm1 Technical Report", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mixture of Experts (MoE) models have emerged as a promising paradigm for\nscaling language models efficiently by activating only a subset of parameters\nfor each input token. In this report, we present dots.llm1, a large-scale MoE\nmodel that activates 14B parameters out of a total of 142B parameters,\ndelivering performance on par with state-of-the-art models while reducing\ntraining and inference costs. Leveraging our meticulously crafted and efficient\ndata processing pipeline, dots.llm1 achieves performance comparable to\nQwen2.5-72B after pretraining on 11.2T high-quality tokens and post-training to\nfully unlock its capabilities. Notably, no synthetic data is used during\npretraining. To foster further research, we open-source intermediate training\ncheckpoints at every one trillion tokens, providing valuable insights into the\nlearning dynamics of large language models.", "AI": {"tldr": "dots.llm1\u662f\u4e00\u79cd\u5927\u89c4\u6a21\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u4ec5\u6fc0\u6d3b\u90e8\u5206\u53c2\u6570\uff0814B/142B\uff09\u964d\u4f4e\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\uff0c\u6027\u80fd\u5ab2\u7f8eQwen2.5-72B\u3002", "motivation": "\u63a2\u7d22\u9ad8\u6548\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u4f7f\u7528MoE\u67b6\u6784\uff0c\u7ed3\u5408\u9ad8\u6548\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u9884\u8bad\u7ec311.2T\u9ad8\u8d28\u91cftoken\uff0c\u65e0\u5408\u6210\u6570\u636e\u3002", "result": "\u6027\u80fd\u4e0eQwen2.5-72B\u76f8\u5f53\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "dots.llm1\u5c55\u793a\u4e86MoE\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u5f00\u6e90\u4e2d\u95f4\u68c0\u67e5\u70b9\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2506.05780", "pdf": "https://arxiv.org/pdf/2506.05780", "abs": "https://arxiv.org/abs/2506.05780", "authors": ["Meng Fan", "Yifan Zuo", "Patrick Blaes", "Harley Montgomery", "Subhasis Das"], "title": "Robust sensor fusion against on-vehicle sensor staleness", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "This paper has been accepted by CVPR 2025 Precognition Workshop", "summary": "Sensor fusion is crucial for a performant and robust Perception system in\nautonomous vehicles, but sensor staleness, where data from different sensors\narrives with varying delays, poses significant challenges. Temporal\nmisalignment between sensor modalities leads to inconsistent object state\nestimates, severely degrading the quality of trajectory predictions that are\ncritical for safety. We present a novel and model-agnostic approach to address\nthis problem via (1) a per-point timestamp offset feature (for LiDAR and radar\nboth relative to camera) that enables fine-grained temporal awareness in sensor\nfusion, and (2) a data augmentation strategy that simulates realistic sensor\nstaleness patterns observed in deployed vehicles. Our method is integrated into\na perspective-view detection model that consumes sensor data from multiple\nLiDARs, radars and cameras. We demonstrate that while a conventional model\nshows significant regressions when one sensor modality is stale, our approach\nreaches consistently good performance across both synchronized and stale\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u4f20\u611f\u5668\u6570\u636e\u65f6\u95f4\u4e0d\u540c\u6b65\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u70b9\u7ea7\u65f6\u95f4\u6233\u504f\u79fb\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u611f\u5668\u6570\u636e\u7684\u65f6\u95f4\u4e0d\u540c\u6b65\uff08staleness\uff09\u4f1a\u5bfc\u81f4\u7269\u4f53\u72b6\u6001\u4f30\u8ba1\u4e0d\u4e00\u81f4\uff0c\u4e25\u91cd\u5f71\u54cd\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u5a01\u80c1\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u70b9\u7ea7\u65f6\u95f4\u6233\u504f\u79fb\u7279\u5f81\uff08LiDAR\u548c\u96f7\u8fbe\u76f8\u5bf9\u4e8e\u76f8\u673a\uff09\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u6a21\u62df\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u4f20\u611f\u5668staleness\u6a21\u5f0f\u3002", "result": "\u5728\u4f20\u7edf\u6a21\u578b\u56e0\u4f20\u611f\u5668staleness\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u60c5\u51b5\u4e0b\uff0c\u65b0\u65b9\u6cd5\u5728\u540c\u6b65\u548c\u4e0d\u540c\u6b65\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u4f20\u611f\u5668\u65f6\u95f4\u4e0d\u540c\u6b65\u95ee\u9898\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.05814", "pdf": "https://arxiv.org/pdf/2506.05814", "abs": "https://arxiv.org/abs/2506.05814", "authors": ["Yogesh Verma", "Amauri H. Souza", "Vikas Garg"], "title": "Positional Encoding meets Persistent Homology on Graphs", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.SI"], "comment": "Accepted at ICML 2025", "summary": "The local inductive bias of message-passing graph neural networks (GNNs)\nhampers their ability to exploit key structural information (e.g., connectivity\nand cycles). Positional encoding (PE) and Persistent Homology (PH) have emerged\nas two promising approaches to mitigate this issue. PE schemes endow GNNs with\nlocation-aware features, while PH methods enhance GNNs with multiresolution\ntopological features. However, a rigorous theoretical characterization of the\nrelative merits and shortcomings of PE and PH has remained elusive. We bridge\nthis gap by establishing that neither paradigm is more expressive than the\nother, providing novel constructions where one approach fails but the other\nsucceeds. Our insights inform the design of a novel learnable method, PiPE\n(Persistence-informed Positional Encoding), which is provably more expressive\nthan both PH and PE. PiPE demonstrates strong performance across a variety of\ntasks (e.g., molecule property prediction, graph classification, and\nout-of-distribution generalization), thereby advancing the frontiers of graph\nrepresentation learning. Code is available at\nhttps://github.com/Aalto-QuML/PIPE.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u6d88\u606f\u4f20\u9012\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e24\u79cd\u6539\u8fdb\u65b9\u6cd5\uff08PE\u548cPH\uff09\uff0c\u5e76\u8bc1\u660e\u4e24\u8005\u8868\u8fbe\u80fd\u529b\u76f8\u5f53\u3002\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5PiPE\uff0c\u6027\u80fd\u4f18\u4e8e\u4e24\u8005\u3002", "motivation": "\u89e3\u51b3GNNs\u56e0\u5c40\u90e8\u5f52\u7eb3\u504f\u5dee\u800c\u65e0\u6cd5\u5229\u7528\u5173\u952e\u7ed3\u6784\u4fe1\u606f\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u6bd4\u8f83PE\u548cPH\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u65b0\u65b9\u6cd5PiPE\u3002", "result": "PiPE\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8ePE\u548cPH\u3002", "conclusion": "PiPE\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2506.05821", "pdf": "https://arxiv.org/pdf/2506.05821", "abs": "https://arxiv.org/abs/2506.05821", "authors": ["Quansong He", "Xiangde Min", "Kaishen Wang", "Tao He"], "title": "FuseUNet: A Multi-Scale Feature Fusion Method for U-like Networks", "categories": ["cs.CV", "cs.AI"], "comment": "ICML2025", "summary": "Medical image segmentation is a critical task in computer vision, with UNet\nserving as a milestone architecture. The typical component of UNet family is\nthe skip connection, however, their skip connections face two significant\nlimitations: (1) they lack effective interaction between features at different\nscales, and (2) they rely on simple concatenation or addition operations, which\nconstrain efficient information integration. While recent improvements to UNet\nhave focused on enhancing encoder and decoder capabilities, these limitations\nremain overlooked. To overcome these challenges, we propose a novel multi-scale\nfeature fusion method that reimagines the UNet decoding process as solving an\ninitial value problem (IVP), treating skip connections as discrete nodes. By\nleveraging principles from the linear multistep method, we propose an adaptive\nordinary differential equation method to enable effective multi-scale feature\nfusion. Our approach is independent of the encoder and decoder architectures,\nmaking it adaptable to various U-Net-like networks. Experiments on ACDC,\nKiTS2023, MSD brain tumor, and ISIC2017/2018 skin lesion segmentation datasets\ndemonstrate improved feature utilization, reduced network parameters, and\nmaintained high performance. The code is available at\nhttps://github.com/nayutayuki/FuseUNet.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06UNet\u7684\u89e3\u7801\u8fc7\u7a0b\u89c6\u4e3a\u521d\u59cb\u503c\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfUNet\u4e2d\u8df3\u8dc3\u8fde\u63a5\u7684\u4e24\u5927\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfUNet\u7684\u8df3\u8dc3\u8fde\u63a5\u7f3a\u4e4f\u4e0d\u540c\u5c3a\u5ea6\u7279\u5f81\u95f4\u7684\u6709\u6548\u4ea4\u4e92\uff0c\u4e14\u4f9d\u8d56\u7b80\u5355\u7684\u62fc\u63a5\u6216\u52a0\u6cd5\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u4fe1\u606f\u6574\u5408\u6548\u7387\u3002", "method": "\u5c06\u89e3\u7801\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u521d\u59cb\u503c\u95ee\u9898\uff0c\u5229\u7528\u7ebf\u6027\u591a\u6b65\u6cd5\u63d0\u51fa\u81ea\u9002\u5e94\u5e38\u5fae\u5206\u65b9\u7a0b\u65b9\u6cd5\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u9ad8\u4e86\u7279\u5f81\u5229\u7528\u7387\uff0c\u51cf\u5c11\u4e86\u7f51\u7edc\u53c2\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u72ec\u7acb\u4e8e\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u5404\u79cdUNet\u7c7b\u7f51\u7edc\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.05831", "pdf": "https://arxiv.org/pdf/2506.05831", "abs": "https://arxiv.org/abs/2506.05831", "authors": ["Yihan Xie", "Sijing Li", "Tianwei Lin", "Zhuonan Wang", "Chenglin Yang", "Yu Zhong", "Wenqiao Zhang", "Haoyuan Li", "Hao Jiang", "Fengda Zhang", "Qishan Chen", "Jun Xiao", "Yueting Zhuang", "Beng Chin Ooi"], "title": "Heartcare Suite: Multi-dimensional Understanding of ECG with Raw Multi-lead Signal Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We present Heartcare Suite, a multimodal comprehensive framework for\nfinegrained electrocardiogram (ECG) understanding. It comprises three key\ncomponents: (i) Heartcare-220K, a high-quality, structured, and comprehensive\nmultimodal ECG dataset covering essential tasks such as disease diagnosis,\nwaveform morphology analysis, and rhythm interpretation. (ii) Heartcare-Bench,\na systematic and multi-dimensional benchmark designed to evaluate diagnostic\nintelligence and guide the optimization of Medical Multimodal Large Language\nModels (Med-MLLMs) in ECG scenarios. and (iii) HeartcareGPT with a tailored\ntokenizer Bidirectional ECG Abstract Tokenization (Beat), which compresses raw\nmulti-lead signals into semantically rich discrete tokens via duallevel vector\nquantization and query-guided bidirectional diffusion mechanism. Built upon\nHeartcare-220K, HeartcareGPT achieves strong generalization and SoTA\nperformance across multiple clinically meaningful tasks. Extensive experiments\ndemonstrate that Heartcare Suite is highly effective in advancing ECGspecific\nmultimodal understanding and evaluation. Our project is available at\nhttps://github.com/Wznnnnn/Heartcare-Suite .", "AI": {"tldr": "Heartcare Suite\u662f\u4e00\u4e2a\u591a\u6a21\u6001ECG\u7406\u89e3\u6846\u67b6\uff0c\u5305\u542b\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578bHeartcareGPT\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728ECG\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u63d0\u5347ECG\u7684\u591a\u6a21\u6001\u7406\u89e3\u548c\u8bc4\u4f30\u80fd\u529b\uff0c\u652f\u6301\u75be\u75c5\u8bca\u65ad\u3001\u6ce2\u5f62\u5206\u6790\u548c\u5fc3\u5f8b\u89e3\u91ca\u7b49\u4efb\u52a1\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u90e8\u5206\uff1a\u9ad8\u8d28\u91cf\u6570\u636e\u96c6Heartcare-220K\u3001\u57fa\u51c6\u6d4b\u8bd5Heartcare-Bench\u548c\u6a21\u578bHeartcareGPT\uff08\u91c7\u7528BEAT\u5206\u8bcd\u5668\uff09\u3002", "result": "HeartcareGPT\u5728\u591a\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u6cdb\u5316\u6027\u548c\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "Heartcare Suite\u6709\u6548\u63a8\u52a8\u4e86ECG\u591a\u6a21\u6001\u7406\u89e3\u548c\u8bc4\u4f30\u7684\u53d1\u5c55\uff0c\u9879\u76ee\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.05833", "pdf": "https://arxiv.org/pdf/2506.05833", "abs": "https://arxiv.org/abs/2506.05833", "authors": ["Yiwen Ding", "Krishna Manoorkar"], "title": "Fuzzy Lattice-based Description Logic", "categories": ["cs.LO", "cs.AI"], "comment": "In Proceedings LSFA 2024, arXiv:2506.05219", "summary": "Recently, description logic LE-ALC was introduced for reasoning in the\nsemantic environment of enriched formal contexts, and a polynomial-time\ntableaux algorithm was developed to check the consistency of knowledge bases\nwith acyclic TBoxes. In this work, we introduce a fuzzy generalization of\nLE-ALC called LE-FALC which provides a description logic counterpart of\nmany-valued normal non-distributive logic a.k.a. many-valued LE-logic. This\ndescription logic can be used to represent and reason about knowledge in the\nformal framework of fuzzy formal contexts and fuzzy formal concepts. We provide\na tableaux algorithm that provides a complete and sound polynomial-time\ndecision procedure to check the consistency of LE-FALC ABoxes. As a result, we\nalso obtain an exponential-time decision procedure for checking the consistency\nof LE-FALC with acyclic TBoxes by unraveling.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86LE-ALC\u7684\u6a21\u7cca\u6cdb\u5316\u7248\u672cLE-FALC\uff0c\u7528\u4e8e\u5728\u6a21\u7cca\u5f62\u5f0f\u4e0a\u4e0b\u6587\u4e2d\u8868\u793a\u548c\u63a8\u7406\u77e5\u8bc6\uff0c\u5e76\u63d0\u4f9b\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u51b3\u7b56\u7b97\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u5728\u6a21\u7cca\u5f62\u5f0f\u4e0a\u4e0b\u6587\u4e2d\u6269\u5c55\u63cf\u8ff0\u903b\u8f91\u7684\u5e94\u7528\uff0c\u63d0\u51faLE-FALC\u4ee5\u652f\u6301\u591a\u503c\u975e\u5206\u914d\u903b\u8f91\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u9879\u5f0f\u65f6\u95f4\u8868\u7b97\u6cd5\uff0c\u7528\u4e8e\u68c0\u67e5LE-FALC ABox\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u5c55\u5f00\u65b9\u6cd5\u5904\u7406\u5faa\u73afTBox\u3002", "result": "\u7b97\u6cd5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b8c\u6210ABox\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5904\u7406\u5faa\u73afTBox\u7684\u6307\u6570\u65f6\u95f4\u51b3\u7b56\u65b9\u6cd5\u3002", "conclusion": "LE-FALC\u4e3a\u6a21\u7cca\u5f62\u5f0f\u4e0a\u4e0b\u6587\u4e2d\u7684\u77e5\u8bc6\u8868\u793a\u548c\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\uff0c\u7b97\u6cd5\u5177\u6709\u5b8c\u5907\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2506.05834", "pdf": "https://arxiv.org/pdf/2506.05834", "abs": "https://arxiv.org/abs/2506.05834", "authors": ["Sandro Preto", "Marcelo Finger"], "title": "Regional, Lattice and Logical Representations of Neural Networks", "categories": ["cs.LO", "cs.AI", "cs.LG"], "comment": "In Proceedings LSFA 2024, arXiv:2506.05219", "summary": "A possible path to the interpretability of neural networks is to\n(approximately) represent them in the regional format of piecewise linear\nfunctions, where regions of inputs are associated to linear functions computing\nthe network outputs. We present an algorithm for the translation of feedforward\nneural networks with ReLU activation functions in hidden layers and truncated\nidentity activation functions in the output layer. We also empirically\ninvestigate the complexity of regional representations outputted by our method\nfor neural networks with varying sizes. Lattice and logical representations of\nneural networks are straightforward from regional representations as long as\nthey satisfy a specific property. So we empirically investigate to what extent\nthe translations by our algorithm satisfy such property.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06ReLU\u6fc0\u6d3b\u7684\u795e\u7ecf\u7f51\u7edc\u8f6c\u5316\u4e3a\u5206\u6bb5\u7ebf\u6027\u51fd\u6570\u533a\u57df\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u590d\u6742\u6027\u548c\u9002\u7528\u6027\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027\u7684\u4e00\u79cd\u53ef\u80fd\u8def\u5f84\uff0c\u901a\u8fc7\u5c06\u5176\u8868\u793a\u4e3a\u5206\u6bb5\u7ebf\u6027\u51fd\u6570\u7684\u533a\u57df\u5f62\u5f0f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\uff0c\u5c06\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff08\u9690\u85cf\u5c42\u4e3aReLU\u6fc0\u6d3b\uff0c\u8f93\u51fa\u5c42\u4e3a\u622a\u65ad\u6052\u7b49\u6fc0\u6d3b\uff09\u8f6c\u5316\u4e3a\u533a\u57df\u8868\u793a\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u4e86\u4e0d\u540c\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u7684\u533a\u57df\u8868\u793a\u590d\u6742\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6ee1\u8db3\u7279\u5b9a\u6027\u8d28\u7684\u7a0b\u5ea6\u3002", "conclusion": "\u533a\u57df\u8868\u793a\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u4e14\u53ef\u8fdb\u4e00\u6b65\u8f6c\u5316\u4e3a\u683c\u6216\u903b\u8f91\u8868\u793a\u3002"}}
{"id": "2506.05850", "pdf": "https://arxiv.org/pdf/2506.05850", "abs": "https://arxiv.org/abs/2506.05850", "authors": ["Cheonbok Park", "Jeonghoon Kim", "Joosung Lee", "Sanghwan Bae", "Jaegul Choo", "Kangmin Yoo"], "title": "Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "We identify \\textbf{Cross-lingual Collapse}, a systematic drift in which the\nchain-of-thought (CoT) of a multilingual language model reverts to its dominant\npre-training language even when the prompt is expressed in a different\nlanguage. Recent large language models (LLMs) with reinforcement learning with\nverifiable reward (RLVR) have achieved strong logical reasoning performances by\nexposing their intermediate reasoning traces, giving rise to large reasoning\nmodels (LRMs). However, the mechanism behind multilingual reasoning in LRMs is\nnot yet fully explored. To investigate the issue, we fine-tune multilingual\nLRMs with Group-Relative Policy Optimization (GRPO) on translated versions of\nthe GSM$8$K and SimpleRL-Zoo datasets in three different languages: Chinese,\nKorean, and Ukrainian. During training, we monitor both task accuracy and\nlanguage consistency of the reasoning chains. Our experiments reveal three key\nfindings: (i) GRPO rapidly amplifies pre-training language imbalances, leading\nto the erosion of low-resource languages within just a few hundred updates;\n(ii) language consistency reward mitigates this drift but does so at the\nexpense of an almost 5 - 10 pp drop in accuracy. and (iii) the resulting\nlanguage collapse is severely damaging and largely irreversible, as subsequent\nfine-tuning struggles to steer the model back toward its original\ntarget-language reasoning capabilities. Together, these findings point to a\nremarkable conclusion: \\textit{not all languages are trained equally for\nreasoning}. Furthermore, our paper sheds light on the roles of reward shaping,\ndata difficulty, and pre-training priors in eliciting multilingual reasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u591a\u8bed\u8a00\u5927\u6a21\u578b\u5728\u63a8\u7406\u65f6\u5b58\u5728\"\u8de8\u8bed\u8a00\u5d29\u6e83\"\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u4f1a\u503e\u5411\u4e8e\u56de\u5f52\u5230\u5176\u9884\u8bad\u7ec3\u7684\u4e3b\u5bfc\u8bed\u8a00\uff0c\u5373\u4f7f\u63d0\u793a\u4f7f\u7528\u5176\u4ed6\u8bed\u8a00\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u8bad\u7ec3\u4e0d\u5e73\u8861\u3001\u8bed\u8a00\u4e00\u81f4\u6027\u5956\u52b1\u7684\u4ee3\u4ef7\u4ee5\u53ca\u5d29\u6e83\u7684\u4e0d\u53ef\u9006\u6027\u3002", "motivation": "\u63a2\u7d22\u591a\u8bed\u8a00\u5927\u6a21\u578b\uff08LRMs\uff09\u5728\u591a\u8bed\u8a00\u63a8\u7406\u4e2d\u7684\u673a\u5236\uff0c\u5c24\u5176\u662f\u5176\u601d\u7ef4\u94fe\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528Group-Relative Policy Optimization\uff08GRPO\uff09\u5728\u4e2d\u6587\u3001\u97e9\u8bed\u548c\u4e4c\u514b\u5170\u8bed\u7684\u7ffb\u8bd1\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u591a\u8bed\u8a00LRMs\uff0c\u5e76\u76d1\u63a7\u4efb\u52a1\u51c6\u786e\u6027\u548c\u8bed\u8a00\u4e00\u81f4\u6027\u3002", "result": "\u53d1\u73b0GRPO\u4f1a\u8fc5\u901f\u653e\u5927\u9884\u8bad\u7ec3\u8bed\u8a00\u4e0d\u5e73\u8861\uff0c\u8bed\u8a00\u4e00\u81f4\u6027\u5956\u52b1\u867d\u80fd\u7f13\u89e3\u5d29\u6e83\u4f46\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u4e14\u5d29\u6e83\u73b0\u8c61\u51e0\u4e4e\u4e0d\u53ef\u9006\u3002", "conclusion": "\u591a\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u5e76\u975e\u6240\u6709\u8bed\u8a00\u8868\u73b0\u5e73\u7b49\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u5956\u52b1\u5851\u9020\u3001\u6570\u636e\u96be\u5ea6\u548c\u9884\u8bad\u7ec3\u5148\u9a8c\u5728\u591a\u8bed\u8a00\u63a8\u7406\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2506.05851", "pdf": "https://arxiv.org/pdf/2506.05851", "abs": "https://arxiv.org/abs/2506.05851", "authors": ["Marcel Klemt", "Carlotta Segna", "Anna Rohrbach"], "title": "DeepFake Doctor: Diagnosing and Treating Audio-Video Fake Detection", "categories": ["cs.MM", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Generative AI advances rapidly, allowing the creation of very realistic\nmanipulated video and audio. This progress presents a significant security and\nethical threat, as malicious users can exploit DeepFake techniques to spread\nmisinformation. Recent DeepFake detection approaches explore the multimodal\n(audio-video) threat scenario. In particular, there is a lack of\nreproducibility and critical issues with existing datasets - such as the\nrecently uncovered silence shortcut in the widely used FakeAVCeleb dataset.\nConsidering the importance of this topic, we aim to gain a deeper understanding\nof the key issues affecting benchmarking in audio-video DeepFake detection. We\nexamine these challenges through the lens of the three core benchmarking\npillars: datasets, detection methods, and evaluation protocols. To address\nthese issues, we spotlight the recent DeepSpeak v1 dataset and are the first to\npropose an evaluation protocol and benchmark it using SOTA models. We introduce\nSImple Multimodal BAseline (SIMBA), a competitive yet minimalistic approach\nthat enables the exploration of diverse design choices. We also deepen insights\ninto the issue of audio shortcuts and present a promising mitigation strategy.\nFinally, we analyze and enhance the evaluation scheme on the widely used\nFakeAVCeleb dataset. Our findings offer a way forward in the complex area of\naudio-video DeepFake detection.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u6280\u672f\u5e26\u6765\u7684DeepFake\u5b89\u5168\u5a01\u80c1\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6848\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4DeepFake\u6280\u672f\u88ab\u6076\u610f\u5229\u7528\uff0c\u4e9f\u9700\u89e3\u51b3\u97f3\u9891-\u89c6\u9891\u591a\u6a21\u6001\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6570\u636e\u96c6\u3001\u68c0\u6d4b\u65b9\u6cd5\u548c\u8bc4\u4f30\u534f\u8bae\u4e09\u5927\u6838\u5fc3\u95ee\u9898\uff0c\u63d0\u51faDeepSpeak v1\u6570\u636e\u96c6\u3001SIMBA\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdbFakeAVCeleb\u7684\u8bc4\u4f30\u65b9\u6848\u3002", "result": "\u63d0\u51fa\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u9a8c\u8bc1\u4e86SIMBA\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u89e3\u51b3\u4e86\u97f3\u9891\u6377\u5f84\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u4e3a\u97f3\u9891-\u89c6\u9891DeepFake\u68c0\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u548c\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2506.05856", "pdf": "https://arxiv.org/pdf/2506.05856", "abs": "https://arxiv.org/abs/2506.05856", "authors": ["Yuqian Fu", "Runze Wang", "Yanwei Fu", "Danda Pani Paudel", "Luc Van Gool"], "title": "Cross-View Multi-Modal Segmentation @ Ego-Exo4D Challenges 2025", "categories": ["cs.CV", "cs.AI"], "comment": "The 2nd Price Award of EgoExo4D Relations, Second Joint EgoVis\n  Workshop with CVPR2025, technical report paper is accepted by CVPRW 25", "summary": "In this report, we present a cross-view multi-modal object segmentation\napproach for the object correspondence task in the Ego-Exo4D Correspondence\nChallenges 2025. Given object queries from one perspective (e.g., ego view),\nthe goal is to predict the corresponding object masks in another perspective\n(e.g., exo view). To tackle this task, we propose a multimodal condition fusion\nmodule that enhances object localization by leveraging both visual masks and\ntextual descriptions as segmentation conditions. Furthermore, to address the\nvisual domain gap between ego and exo views, we introduce a cross-view object\nalignment module that enforces object-level consistency across perspectives,\nthereby improving the model's robustness to viewpoint changes. Our proposed\nmethod ranked second on the leaderboard of the large-scale Ego-Exo4D object\ncorrespondence benchmark. Code will be made available at\nhttps://github.com/lovelyqian/ObjectRelator.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u89c6\u89d2\u591a\u6a21\u6001\u5bf9\u8c61\u5206\u5272\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3Ego-Exo4D Correspondence Challenges 2025\u4e2d\u7684\u5bf9\u8c61\u5bf9\u5e94\u4efb\u52a1\u3002\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u63a9\u7801\u548c\u6587\u672c\u63cf\u8ff0\u4f5c\u4e3a\u5206\u5272\u6761\u4ef6\uff0c\u5e76\u5f15\u5165\u8de8\u89c6\u89d2\u5bf9\u8c61\u5bf9\u9f50\u6a21\u5757\uff0c\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u7b2c\u4e8c\u3002", "motivation": "\u89e3\u51b3\u8de8\u89c6\u89d2\uff08\u5982ego\u548cexo\u89c6\u56fe\uff09\u4e2d\u5bf9\u8c61\u5bf9\u5e94\u4efb\u52a1\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u89c6\u89c9\u57df\u5dee\u5f02\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u6761\u4ef6\u878d\u5408\u6a21\u5757\u548c\u8de8\u89c6\u89d2\u5bf9\u8c61\u5bf9\u9f50\u6a21\u5757\uff0c\u7ed3\u5408\u89c6\u89c9\u63a9\u7801\u548c\u6587\u672c\u63cf\u8ff0\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "result": "\u5728Ego-Exo4D\u5bf9\u8c61\u5bf9\u5e94\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u7b2c\u4e8c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u8de8\u89c6\u89d2\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u8c61\u5206\u5272\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.05869", "pdf": "https://arxiv.org/pdf/2506.05869", "abs": "https://arxiv.org/abs/2506.05869", "authors": ["Han Ji", "Yuqi Feng", "Jiahao Fan", "Yanan Sun"], "title": "Loss Functions for Predictor-based Neural Architecture Search", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Evaluation is a critical but costly procedure in neural architecture search\n(NAS). Performance predictors have been widely adopted to reduce evaluation\ncosts by directly estimating architecture performance. The effectiveness of\npredictors is heavily influenced by the choice of loss functions. While\ntraditional predictors employ regression loss functions to evaluate the\nabsolute accuracy of architectures, recent approaches have explored various\nranking-based loss functions, such as pairwise and listwise ranking losses, to\nfocus on the ranking of architecture performance. Despite their success in NAS,\nthe effectiveness and characteristics of these loss functions have not been\nthoroughly investigated. In this paper, we conduct the first comprehensive\nstudy on loss functions in performance predictors, categorizing them into three\nmain types: regression, ranking, and weighted loss functions. Specifically, we\nassess eight loss functions using a range of NAS-relevant metrics on 13 tasks\nacross five search spaces. Our results reveal that specific categories of loss\nfunctions can be effectively combined to enhance predictor-based NAS.\nFurthermore, our findings could provide practical guidance for selecting\nappropriate loss functions for various tasks. We hope this work provides\nmeaningful insights to guide the development of loss functions for\npredictor-based methods in the NAS community.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u7814\u7a76\u4e86\u6027\u80fd\u9884\u6d4b\u5668\u4e2d\u635f\u5931\u51fd\u6570\u7684\u6709\u6548\u6027\uff0c\u5c06\u5176\u5206\u4e3a\u56de\u5f52\u3001\u6392\u5e8f\u548c\u52a0\u6743\u4e09\u7c7b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u7279\u5b9a\u7c7b\u522b\u7684\u635f\u5931\u51fd\u6570\u7ec4\u5408\u80fd\u63d0\u5347\u9884\u6d4b\u5668\u6027\u80fd\u3002", "motivation": "\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u4e2d\u8bc4\u4f30\u6210\u672c\u9ad8\uff0c\u6027\u80fd\u9884\u6d4b\u5668\u901a\u8fc7\u635f\u5931\u51fd\u6570\u9009\u62e9\u76f4\u63a5\u5f71\u54cd\u6548\u679c\uff0c\u4f46\u73b0\u6709\u635f\u5931\u51fd\u6570\u7684\u7279\u6027\u548c\u6548\u679c\u5c1a\u672a\u6df1\u5165\u63a2\u7a76\u3002", "method": "\u5c06\u635f\u5931\u51fd\u6570\u5206\u4e3a\u4e09\u7c7b\uff08\u56de\u5f52\u3001\u6392\u5e8f\u3001\u52a0\u6743\uff09\uff0c\u572813\u4e2a\u4efb\u52a1\u548c5\u4e2a\u641c\u7d22\u7a7a\u95f4\u4e0a\u8bc4\u4f308\u79cd\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7279\u5b9a\u7c7b\u522b\u7684\u635f\u5931\u51fd\u6570\u7ec4\u5408\u80fd\u6709\u6548\u63d0\u5347\u9884\u6d4b\u5668\u6027\u80fd\uff0c\u5e76\u4e3a\u4e0d\u540c\u4efb\u52a1\u63d0\u4f9b\u635f\u5931\u51fd\u6570\u9009\u62e9\u6307\u5bfc\u3002", "conclusion": "\u672c\u6587\u4e3aNAS\u793e\u533a\u63d0\u4f9b\u4e86\u635f\u5931\u51fd\u6570\u5f00\u53d1\u7684\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u63ed\u793a\u4e86\u635f\u5931\u51fd\u6570\u7ec4\u5408\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.05873", "pdf": "https://arxiv.org/pdf/2506.05873", "abs": "https://arxiv.org/abs/2506.05873", "authors": ["Yushang Zhao", "Yike Peng", "Dannier Li", "Yuxin Yang", "Chengrui Zhou", "Jing Dong"], "title": "Research on Personalized Financial Product Recommendation by Integrating Large Language Models and Graph Neural Networks", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "With the rapid growth of fintech, personalized financial product\nrecommendations have become increasingly important. Traditional methods like\ncollaborative filtering or content-based models often fail to capture users'\nlatent preferences and complex relationships. We propose a hybrid framework\nintegrating large language models (LLMs) and graph neural networks (GNNs). A\npre-trained LLM encodes text data (e.g., user reviews) into rich feature\nvectors, while a heterogeneous user-product graph models interactions and\nsocial ties. Through a tailored message-passing mechanism, text and graph\ninformation are fused within the GNN to jointly optimize embeddings.\nExperiments on public and real-world financial datasets show our model\noutperforms standalone LLM or GNN in accuracy, recall, and NDCG, with strong\ninterpretability. This work offers new insights for personalized financial\nrecommendations and cross-modal fusion in broader recommendation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u91d1\u878d\u4ea7\u54c1\u63a8\u8350\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u63a8\u8350\u65b9\u6cd5\uff08\u5982\u534f\u540c\u8fc7\u6ee4\u6216\u5185\u5bb9\u6a21\u578b\uff09\u96be\u4ee5\u6355\u6349\u7528\u6237\u6f5c\u5728\u504f\u597d\u548c\u590d\u6742\u5173\u7cfb\uff0c\u91d1\u878d\u79d1\u6280\u5feb\u901f\u53d1\u5c55\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a8\u8350\u65b9\u6848\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3LLM\u7f16\u7801\u6587\u672c\u6570\u636e\uff08\u5982\u7528\u6237\u8bc4\u8bba\uff09\uff0c\u6784\u5efa\u5f02\u8d28\u7528\u6237-\u4ea7\u54c1\u56fe\uff0c\u901a\u8fc7\u6d88\u606f\u4f20\u9012\u673a\u5236\u878d\u5408\u6587\u672c\u548c\u56fe\u4fe1\u606f\uff0c\u4f18\u5316\u5d4c\u5165\u8868\u793a\u3002", "result": "\u5728\u516c\u5f00\u548c\u771f\u5b9e\u91d1\u878d\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u53ec\u56de\u7387\u548cNDCG\u4e0a\u4f18\u4e8e\u5355\u72ecLLM\u6216GNN\uff0c\u4e14\u5177\u6709\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e2a\u6027\u5316\u91d1\u878d\u63a8\u8350\u53ca\u8de8\u6a21\u6001\u878d\u5408\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.05876", "pdf": "https://arxiv.org/pdf/2506.05876", "abs": "https://arxiv.org/abs/2506.05876", "authors": ["Yue Lin", "Shuhui Zhu", "William A Cunningham", "Wenhao Li", "Pascal Poupart", "Hongyuan Zha", "Baoxiang Wang"], "title": "Bayesian Persuasion as a Bargaining Game", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "Bayesian persuasion, an extension of cheap-talk communication, involves an\ninformed sender committing to a signaling scheme to influence a receiver's\nactions. Compared to cheap talk, this sender's commitment enables the receiver\nto verify the incentive compatibility of signals beforehand, facilitating\ncooperation. While effective in one-shot scenarios, Bayesian persuasion faces\ncomputational complexity (NP-hardness) when extended to long-term interactions,\nwhere the receiver may adopt dynamic strategies conditional on past outcomes\nand future expectations. To address this complexity, we introduce the\nbargaining perspective, which allows: (1) a unified framework and\nwell-structured solution concept for long-term persuasion, with desirable\nproperties such as fairness and Pareto efficiency; (2) a clear distinction\nbetween two previously conflated advantages: the sender's informational\nadvantage and first-proposer advantage. With only modest modifications to the\nstandard setting, this perspective makes explicit the common knowledge of the\ngame structure and grants the receiver comparable commitment capabilities,\nthereby reinterpreting classic one-sided persuasion as a balanced information\nbargaining framework. The framework is validated through a two-stage\nvalidation-and-inference paradigm: We first demonstrate that GPT-o3 and\nDeepSeek-R1, out of publicly available LLMs, reliably handle standard tasks; We\nthen apply them to persuasion scenarios to test that the outcomes align with\nwhat our information-bargaining framework suggests. All code, results, and\nterminal logs are publicly available at\ngithub.com/YueLin301/InformationBargaining.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba8\u4ef7\u8fd8\u4ef7\u89c6\u89d2\u7684\u957f\u671f\u8d1d\u53f6\u65af\u8bf4\u670d\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u8bf4\u670d\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u6027\u95ee\u9898\uff08NP-hard\uff09\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u7b80\u5316\u5e76\u63d0\u5347\u5408\u4f5c\u6548\u7387\u3002", "method": "\u5f15\u5165\u8ba8\u4ef7\u8fd8\u4ef7\u89c6\u89d2\uff0c\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\u548c\u7ed3\u6784\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u533a\u5206\u4fe1\u606f\u4f18\u52bf\u548c\u5148\u53d1\u4f18\u52bf\uff0c\u5e76\u901a\u8fc7LLMs\uff08GPT-o3\u548cDeepSeek-R1\uff09\u9a8c\u8bc1\u3002", "result": "\u6846\u67b6\u5177\u6709\u516c\u5e73\u6027\u548c\u5e15\u7d2f\u6258\u6548\u7387\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4e0e\u4fe1\u606f\u8ba8\u4ef7\u8fd8\u4ef7\u6846\u67b6\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u8ba8\u4ef7\u8fd8\u4ef7\u89c6\u89d2\u4e3a\u957f\u671f\u8d1d\u53f6\u65af\u8bf4\u670d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u660e\u786e\u4e86\u4fe1\u606f\u4f18\u52bf\u4e0e\u5148\u53d1\u4f18\u52bf\u7684\u533a\u522b\u3002"}}
{"id": "2506.05883", "pdf": "https://arxiv.org/pdf/2506.05883", "abs": "https://arxiv.org/abs/2506.05883", "authors": ["Daming Wang", "Yuhao Song", "Zijian He", "Kangliang Chen", "Xing Pan", "Lu Deng", "Weihao Gu"], "title": "HMVLM: Multistage Reasoning-Enhanced Vision-Language Model for Long-Tailed Driving Scenarios", "categories": ["cs.CV", "cs.AI"], "comment": "WOD Vision-based End-to-End Driving Challenge", "summary": "We present HaoMo Vision-Language Model (HMVLM), an end-to-end driving\nframework that implements the slow branch of a cognitively inspired fast-slow\narchitecture. A fast controller outputs low-level steering, throttle, and brake\ncommands, while a slow planner-a large vision-language model-generates\nhigh-level intents such as \"yield to pedestrian\" or \"merge after the truck\"\nwithout compromising latency. HMVLM introduces three upgrades: (1) selective\nfive-view prompting with an embedded 4s history of ego kinematics, (2)\nmulti-stage chain-of-thought (CoT) prompting that enforces a Scene\nUnderstanding -> Driving Decision -> Trajectory Inference reasoning flow, and\n(3) spline-based trajectory post-processing that removes late-stage jitter and\nsharp turns. Trained on the Waymo Open Dataset, these upgrades enable HMVLM to\nachieve a Rater Feedback Score (RFS) of 7.7367, securing 2nd place in the 2025\nWaymo Vision-based End-to-End (E2E) Driving Challenge and surpassing the public\nbaseline by 2.77%.", "AI": {"tldr": "HaoMo Vision-Language Model (HMVLM) \u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u9a7e\u9a76\u6846\u67b6\uff0c\u7ed3\u5408\u5feb\u901f\u63a7\u5236\u5668\u548c\u6162\u901f\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u63d0\u793a\u548c\u591a\u9636\u6bb5\u63a8\u7406\u63d0\u5347\u9a7e\u9a76\u51b3\u7b56\u80fd\u529b\uff0c\u5728Waymo\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5904\u7406\u4f4e\u7ea7\u522b\u63a7\u5236\u547d\u4ee4\u548c\u9ad8\u7ea7\u522b\u9a7e\u9a76\u610f\u56fe\u7684\u9a7e\u9a76\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "method": "HMVLM \u91c7\u7528\u9009\u62e9\u6027\u4e94\u89c6\u56fe\u63d0\u793a\u3001\u591a\u9636\u6bb5\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u548c\u57fa\u4e8e\u6837\u6761\u7684\u8f68\u8ff9\u540e\u5904\u7406\u6280\u672f\uff0c\u7ed3\u5408Waymo\u6570\u636e\u96c6\u8bad\u7ec3\u3002", "result": "\u57282025 Waymo\u6311\u6218\u8d5b\u4e2d\uff0cHMVLM \u83b7\u5f97Rater Feedback Score (RFS) 7.7367\uff0c\u6392\u540d\u7b2c\u4e8c\uff0c\u8d85\u8d8a\u57fa\u7ebf2.77%\u3002", "conclusion": "HMVLM \u901a\u8fc7\u8ba4\u77e5\u542f\u53d1\u7684\u67b6\u6784\u548c\u591a\u9879\u6280\u672f\u5347\u7ea7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.05896", "pdf": "https://arxiv.org/pdf/2506.05896", "abs": "https://arxiv.org/abs/2506.05896", "authors": ["Chongshang Yan", "Jiaxuan He", "Delun Li", "Yi Yang", "Wenjie Song"], "title": "Object Navigation with Structure-Semantic Reasoning-Based Multi-level Map and Multimodal Decision-Making LLM", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "16 pages, 11 figures", "summary": "The zero-shot object navigation (ZSON) in unknown open-ended environments\ncoupled with semantically novel target often suffers from the significant\ndecline in performance due to the neglect of high-dimensional implicit scene\ninformation and the long-range target searching task. To address this, we\nproposed an active object navigation framework with Environmental Attributes\nMap (EAM) and MLLM Hierarchical Reasoning module (MHR) to improve its success\nrate and efficiency. EAM is constructed by reasoning observed environments with\nSBERT and predicting unobserved ones with Diffusion, utilizing human space\nregularities that underlie object-room correlations and area adjacencies. MHR\nis inspired by EAM to perform frontier exploration decision-making, avoiding\nthe circuitous trajectories in long-range scenarios to improve path efficiency.\nExperimental results demonstrate that the EAM module achieves 64.5\\% scene\nmapping accuracy on MP3D dataset, while the navigation task attains SPLs of\n28.4\\% and 26.3\\% on HM3D and MP3D benchmarks respectively - representing\nabsolute improvements of 21.4\\% and 46.0\\% over baseline methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u73af\u5883\u5c5e\u6027\u5730\u56fe\uff08EAM\uff09\u548cMLLM\u5206\u5c42\u63a8\u7406\u6a21\u5757\uff08MHR\uff09\u7684\u4e3b\u52a8\u76ee\u6807\u5bfc\u822a\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a\u5728\u672a\u77e5\u5f00\u653e\u73af\u5883\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5ffd\u89c6\u4e86\u9ad8\u7ef4\u9690\u5f0f\u573a\u666f\u4fe1\u606f\u548c\u957f\u8ddd\u79bb\u76ee\u6807\u641c\u7d22\u4efb\u52a1\u3002", "method": "\u901a\u8fc7EAM\u6a21\u5757\u5229\u7528SBERT\u548cDiffusion\u63a8\u7406\u73af\u5883\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408MHR\u6a21\u5757\u4f18\u5316\u8def\u5f84\u51b3\u7b56\u3002", "result": "EAM\u5728MP3D\u6570\u636e\u96c6\u4e0a\u8fbe\u523064.5%\u7684\u573a\u666f\u6620\u5c04\u51c6\u786e\u7387\uff0c\u5bfc\u822a\u4efb\u52a1\u5728HM3D\u548cMP3D\u57fa\u51c6\u4e0a\u7684SPL\u5206\u522b\u4e3a28.4%\u548c26.3%\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408EAM\u548cMHR\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002"}}
{"id": "2506.05899", "pdf": "https://arxiv.org/pdf/2506.05899", "abs": "https://arxiv.org/abs/2506.05899", "authors": ["Jakaria Islam Emon", "Kazi Tamanna Alam", "Md. Abu Salek"], "title": "WhisQ: Cross-Modal Representation Learning for Text-to-Music MOS Prediction", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "3 pages", "summary": "Mean Opinion Score (MOS) prediction for text to music systems requires\nevaluating both overall musical quality and text prompt alignment. This paper\nintroduces WhisQ, a multimodal architecture that addresses this dual-assessment\nchallenge through sequence level co-attention and optimal transport\nregularization. WhisQ employs the Whisper Base pretrained model for temporal\naudio encoding and Qwen 3, a 0.6B Small Language Model (SLM), for text\nencoding, with both maintaining sequence structure for fine grained cross-modal\nmodeling. The architecture features specialized prediction pathways: OMQ is\npredicted from pooled audio embeddings, while TA leverages bidirectional\nsequence co-attention between audio and text. Sinkhorn optimal transport loss\nfurther enforce semantic alignment in the shared embedding space. On the\nMusicEval Track-1 dataset, WhisQ achieves substantial improvements over the\nbaseline: 7% improvement in Spearman correlation for OMQ and 14% for TA.\nAblation studies reveal that optimal transport regularization provides the\nlargest performance gain (10% SRCC improvement), demonstrating the importance\nof explicit cross-modal alignment for text-to-music evaluation.", "AI": {"tldr": "WhisQ\u662f\u4e00\u79cd\u591a\u6a21\u6001\u67b6\u6784\uff0c\u901a\u8fc7\u5e8f\u5217\u7ea7\u5171\u540c\u6ce8\u610f\u529b\u548c\u6700\u4f18\u4f20\u8f93\u6b63\u5219\u5316\uff0c\u63d0\u5347\u6587\u672c\u5230\u97f3\u4e50\u7cfb\u7edf\u7684MOS\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u97f3\u4e50\u7cfb\u7edf\u4e2d\u6574\u4f53\u97f3\u4e50\u8d28\u91cf\u548c\u6587\u672c\u63d0\u793a\u5bf9\u9f50\u7684\u53cc\u91cd\u8bc4\u4f30\u6311\u6218\u3002", "method": "\u7ed3\u5408Whisper Base\u97f3\u9891\u7f16\u7801\u548cQwen 3\u6587\u672c\u7f16\u7801\uff0c\u91c7\u7528\u5e8f\u5217\u7ea7\u5171\u540c\u6ce8\u610f\u529b\u548c\u6700\u4f18\u4f20\u8f93\u635f\u5931\u3002", "result": "\u5728MusicEval Track-1\u6570\u636e\u96c6\u4e0a\uff0cOMQ\u548cTA\u7684Spearman\u76f8\u5173\u6027\u5206\u522b\u63d0\u53477%\u548c14%\u3002", "conclusion": "\u6700\u4f18\u4f20\u8f93\u6b63\u5219\u5316\u5bf9\u8de8\u6a21\u6001\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2506.05901", "pdf": "https://arxiv.org/pdf/2506.05901", "abs": "https://arxiv.org/abs/2506.05901", "authors": ["Chenyang Shao", "Xinyang Liu", "Yutang Lin", "Fengli Xu", "Yong Li"], "title": "Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced Model Router", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-step reasoning has proven essential for enhancing the problem-solving\ncapabilities of Large Language Models (LLMs) by decomposing complex tasks into\nintermediate steps, either explicitly or implicitly. Extending the reasoning\nchain at test time through deeper thought processes or broader exploration, can\nfurthur improve performance, but often incurs substantial costs due to the\nexplosion in token usage. Yet, many reasoning steps are relatively simple and\ncan be handled by more efficient smaller-scale language models (SLMs). This\nmotivates hybrid approaches that allocate subtasks across models of varying\ncapacities. However, realizing such collaboration requires accurate task\ndecomposition and difficulty-aware subtask allocation, which is challenging. To\naddress this, we propose R2-Reasoner, a novel framework that enables\ncollaborative reasoning across heterogeneous LLMs by dynamically routing\nsub-tasks based on estimated complexity. At the core of our framework is a\nReinforced Model Router, composed of a task decomposer and a subtask allocator.\nThe task decomposer segments complex input queries into logically ordered\nsubtasks, while the subtask allocator assigns each subtask to the most\nappropriate model, ranging from lightweight SLMs to powerful LLMs, balancing\naccuracy and efficiency. To train this router, we introduce a staged pipeline\nthat combines supervised fine-tuning on task-specific datasets with Group\nRelative Policy Optimization algorithm, enabling self-supervised refinement\nthrough iterative reinforcement learning. Extensive experiments across four\nchallenging benchmarks demonstrate that R2-Reasoner reduces API costs by 86.85%\nwhile maintaining or surpassing baseline accuracy. Our framework paves the way\nfor more cost-effective and adaptive LLM reasoning. The code is open-source at\nhttps://anonymous.4open.science/r/R2_Reasoner .", "AI": {"tldr": "R2-Reasoner\u662f\u4e00\u4e2a\u52a8\u6001\u5206\u914d\u5b50\u4efb\u52a1\u7ed9\u4e0d\u540c\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8def\u7531\uff0c\u663e\u8457\u964d\u4f4eAPI\u6210\u672c\u5e76\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u6b65\u63a8\u7406\u867d\u63d0\u5347LLM\u6027\u80fd\uff0c\u4f46\u6210\u672c\u9ad8\u6602\uff1b\u8bb8\u591a\u5b50\u4efb\u52a1\u53ef\u7531\u5c0f\u578b\u6a21\u578b\u5904\u7406\uff0c\u9700\u9ad8\u6548\u5206\u914d\u673a\u5236\u3002", "method": "\u63d0\u51faR2-Reasoner\u6846\u67b6\uff0c\u5305\u542b\u4efb\u52a1\u5206\u89e3\u5668\u548c\u5b50\u4efb\u52a1\u5206\u914d\u5668\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8def\u7531\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793aAPI\u6210\u672c\u964d\u4f4e86.85%\uff0c\u6027\u80fd\u4fdd\u6301\u6216\u8d85\u8d8a\u57fa\u7ebf\u3002", "conclusion": "R2-Reasoner\u4e3a\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.05917", "pdf": "https://arxiv.org/pdf/2506.05917", "abs": "https://arxiv.org/abs/2506.05917", "authors": ["Steven Landgraf", "Markus Hillemann", "Markus Ulrich"], "title": "Rethinking Semi-supervised Segmentation Beyond Accuracy: Reliability and Robustness", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Semantic segmentation is critical for scene understanding but demands costly\npixel-wise annotations, attracting increasing attention to semi-supervised\napproaches to leverage abundant unlabeled data. While semi-supervised\nsegmentation is often promoted as a path toward scalable, real-world\ndeployment, it is astonishing that current evaluation protocols exclusively\nfocus on segmentation accuracy, entirely overlooking reliability and\nrobustness. These qualities, which ensure consistent performance under diverse\nconditions (robustness) and well-calibrated model confidences as well as\nmeaningful uncertainties (reliability), are essential for safety-critical\napplications like autonomous driving, where models must handle unpredictable\nenvironments and avoid sudden failures at all costs. To address this gap, we\nintroduce the Reliable Segmentation Score (RSS), a novel metric that combines\npredictive accuracy, calibration, and uncertainty quality measures via a\nharmonic mean. RSS penalizes deficiencies in any of its components, providing\nan easy and intuitive way of holistically judging segmentation models.\nComprehensive evaluations of UniMatchV2 against its predecessor and a\nsupervised baseline show that semi-supervised methods often trade reliability\nfor accuracy. While out-of-domain evaluations demonstrate UniMatchV2's\nrobustness, they further expose persistent reliability shortcomings. We\nadvocate for a shift in evaluation protocols toward more holistic metrics like\nRSS to better align semi-supervised learning research with real-world\ndeployment needs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807RSS\uff0c\u7528\u4e8e\u7efc\u5408\u8861\u91cf\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u6a21\u578b\u7684\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u534f\u8bae\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u534a\u76d1\u7763\u5206\u5272\u8bc4\u4f30\u4ec5\u5173\u6ce8\u51c6\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u800c\u8fd9\u4e9b\u5bf9\u5b89\u5168\u5173\u952e\u5e94\u7528\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165Reliable Segmentation Score (RSS)\uff0c\u901a\u8fc7\u8c03\u548c\u5e73\u5747\u503c\u7ed3\u5408\u9884\u6d4b\u51c6\u786e\u6027\u3001\u6821\u51c6\u548c\u4e0d\u786e\u5b9a\u6027\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u534a\u76d1\u7763\u65b9\u6cd5\u5e38\u4ee5\u727a\u7272\u53ef\u9760\u6027\u6362\u53d6\u51c6\u786e\u6027\uff0cUniMatchV2\u5728\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u53ef\u9760\u6027\u4ecd\u6709\u4e0d\u8db3\u3002", "conclusion": "\u547c\u5401\u91c7\u7528\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982RSS\uff09\uff0c\u4ee5\u66f4\u597d\u5730\u6ee1\u8db3\u5b9e\u9645\u90e8\u7f72\u9700\u6c42\u3002"}}
{"id": "2506.05925", "pdf": "https://arxiv.org/pdf/2506.05925", "abs": "https://arxiv.org/abs/2506.05925", "authors": ["Zarreen Reza", "Alexander Mazur", "Michael T. Dugdale", "Robin Ray-Chaudhuri"], "title": "Small Models, Big Support: A Local LLM Framework for Teacher-Centric Content Creation and Assessment using RAG and CAG", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) are increasingly utilized as\nstudent-facing educational aids, their potential to directly support educators,\nparticularly through locally deployable and customizable open-source solutions,\nremains significantly underexplored. Many existing educational solutions rely\non cloud-based infrastructure or proprietary tools, which are costly and may\nraise privacy concerns. Regulated industries with limited budgets require\naffordable, self-hosted solutions. We introduce an end-to-end, open-source\nframework leveraging small (3B-7B parameters), locally deployed LLMs for\ncustomized teaching material generation and assessment. Our system uniquely\nincorporates an interactive loop crucial for effective small-model refinement,\nand an auxiliary LLM verifier to mitigate jailbreaking risks, enhancing output\nreliability and safety. Utilizing Retrieval and Context Augmented Generation\n(RAG/CAG), it produces factually accurate, customized pedagogically-styled\ncontent. Deployed on-premises for data privacy and validated through an\nevaluation pipeline and a college physics pilot, our findings show that\ncarefully engineered small LLM systems can offer robust, affordable, practical,\nand safe educator support, achieving utility comparable to larger models for\ntargeted tasks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5c0f\u578b\u672c\u5730\u90e8\u7f72\u7684LLM\uff083B-7B\u53c2\u6570\uff09\u4e3a\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u5b9a\u5236\u5316\u6559\u5b66\u6750\u6599\u751f\u6210\u548c\u8bc4\u4f30\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4e91\u670d\u52a1\u548c\u4e13\u6709\u5de5\u5177\u7684\u6210\u672c\u4e0e\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6559\u80b2\u89e3\u51b3\u65b9\u6848\u591a\u4f9d\u8d56\u4e91\u57fa\u7840\u8bbe\u65bd\u6216\u4e13\u6709\u5de5\u5177\uff0c\u6210\u672c\u9ad8\u4e14\u9690\u79c1\u98ce\u9669\u5927\uff0c\u800c\u9884\u7b97\u6709\u9650\u7684\u884c\u4e1a\u9700\u8981\u7ecf\u6d4e\u5b9e\u60e0\u3001\u53ef\u81ea\u6258\u7ba1\u7684\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u5f00\u6e90\u6846\u67b6\uff0c\u7ed3\u5408\u5c0f\u578b\u672c\u5730LLM\u3001\u4ea4\u4e92\u5f0f\u5faa\u73af\u4f18\u5316\u548c\u8f85\u52a9LLM\u9a8c\u8bc1\u5668\uff0c\u91c7\u7528RAG/CAG\u6280\u672f\u751f\u6210\u51c6\u786e\u3001\u5b9a\u5236\u5316\u7684\u6559\u5b66\u5185\u5bb9\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u6d41\u7a0b\u548c\u5927\u5b66\u7269\u7406\u8bd5\u70b9\u9a8c\u8bc1\uff0c\u8868\u660e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5c0f\u578bLLM\u7cfb\u7edf\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u53ef\u5ab2\u7f8e\u5927\u578b\u6a21\u578b\uff0c\u63d0\u4f9b\u7ecf\u6d4e\u3001\u5b9e\u7528\u4e14\u5b89\u5168\u7684\u652f\u6301\u3002", "conclusion": "\u5c0f\u578b\u672c\u5730LLM\u7cfb\u7edf\u7ecf\u8fc7\u4f18\u5316\u540e\uff0c\u80fd\u4e3a\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u9ad8\u6548\u3001\u7ecf\u6d4e\u4e14\u5b89\u5168\u7684\u652f\u6301\uff0c\u9002\u7528\u4e8e\u9884\u7b97\u6709\u9650\u7684\u573a\u666f\u3002"}}
{"id": "2506.05927", "pdf": "https://arxiv.org/pdf/2506.05927", "abs": "https://arxiv.org/abs/2506.05927", "authors": ["Bel\u00e9n Ag\u00fcera-Marco", "Itziar Gonzalez-Dios"], "title": "LengClaro2023: A Dataset of Administrative Texts in Spanish with Plain Language adaptations", "categories": ["cs.CL", "cs.AI"], "comment": "In this report, we present a part of the master thesis written by\n  Bel\\'en Ag\\\"uera Marco in order to obtain the B.S. Language Analysis and\n  Processing at the University of the Basque Country (UPV/EHU), supervised by\n  Itziar Gonzalez-Dios", "summary": "In this work, we present LengClaro2023, a dataset of legal-administrative\ntexts in Spanish. Based on the most frequently used procedures from the Spanish\nSocial Security website, we have created for each text two simplified\nequivalents. The first version follows the recommendations provided by arText\nclaro. The second version incorporates additional recommendations from plain\nlanguage guidelines to explore further potential improvements in the system.\nThe linguistic resource created in this work can be used for evaluating\nautomatic text simplification (ATS) systems in Spanish.", "AI": {"tldr": "LengClaro2023\u662f\u4e00\u4e2a\u897f\u73ed\u7259\u8bed\u6cd5\u5f8b\u884c\u653f\u6587\u672c\u6570\u636e\u96c6\uff0c\u5305\u542b\u539f\u59cb\u6587\u672c\u53ca\u5176\u4e24\u79cd\u7b80\u5316\u7248\u672c\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u6587\u672c\u7b80\u5316\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u897f\u73ed\u7259\u8bed\u81ea\u52a8\u6587\u672c\u7b80\u5316\uff08ATS\uff09\u7cfb\u7edf\u63d0\u4f9b\u8bc4\u4f30\u8d44\u6e90\u3002", "method": "\u57fa\u4e8e\u897f\u73ed\u7259\u793e\u4f1a\u4fdd\u969c\u7f51\u7ad9\u5e38\u7528\u7a0b\u5e8f\uff0c\u4e3a\u6bcf\u4e2a\u6587\u672c\u521b\u5efa\u4e24\u79cd\u7b80\u5316\u7248\u672c\uff1a\u4e00\u79cd\u9075\u5faaarText claro\u5efa\u8bae\uff0c\u53e6\u4e00\u79cd\u7ed3\u5408\u7b80\u660e\u8bed\u8a00\u6307\u5357\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u53ef\u7528\u4e8e\u8bc4\u4f30\u897f\u73ed\u7259\u8bedATS\u7cfb\u7edf\u7684\u8bed\u8a00\u8d44\u6e90\u3002", "conclusion": "LengClaro2023\u6570\u636e\u96c6\u4e3a\u897f\u73ed\u7259\u8bed\u6587\u672c\u7b80\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2506.05928", "pdf": "https://arxiv.org/pdf/2506.05928", "abs": "https://arxiv.org/abs/2506.05928", "authors": ["Jie Cao", "Tianwei Lin", "Hongyang He", "Rolan Yan", "Wenqiao Zhang", "Juncheng Li", "Dongping Zhang", "Siliang Tang", "Yueting Zhuang"], "title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts\n(MoE) to further enhance the performance of parameter-efficient fine-tuning\n(PEFT) methods in Large Language Model (LLM) applications. Existing methods\nemploy \\emph{homogeneous} MoE-LoRA architectures composed of LoRA experts with\neither similar or identical structures and capacities. However, these\napproaches often suffer from representation collapse and expert load imbalance,\nwhich negatively impact the potential of LLMs. To address these challenges, we\npropose a \\emph{heterogeneous} \\textbf{Mixture-of-Adapters (MoA)} approach.\nThis method dynamically integrates PEFT adapter experts with diverse\nstructures, leveraging their complementary representational capabilities to\nfoster expert specialization, thereby enhancing the effective transfer of\npre-trained knowledge to downstream tasks. MoA supports two variants:\n\\textbf{(i)} \\textit{Soft MoA} achieves fine-grained integration by performing\na weighted fusion of all expert outputs; \\textbf{(ii)} \\textit{Sparse MoA}\nactivates adapter experts sparsely based on their contribution, achieving this\nwith negligible performance degradation. Experimental results demonstrate that\nheterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance\nand parameter efficiency. Our project is available at\nhttps://github.com/DCDmllm/MoA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMixture-of-Adapters (MoA)\u7684\u5f02\u6784\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6574\u5408\u591a\u6837\u7ed3\u6784\u7684PEFT\u9002\u914d\u5668\u4e13\u5bb6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u540c\u8d28MoE-LoRA\u65b9\u6cd5\u4e2d\u7684\u8868\u793a\u5d29\u6e83\u548c\u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u540c\u8d28MoE-LoRA\u65b9\u6cd5\u5b58\u5728\u8868\u793a\u5d29\u6e83\u548c\u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u5f02\u6784MoA\u65b9\u6cd5\uff0c\u652f\u6301\u4e24\u79cd\u53d8\u4f53\uff1aSoft MoA\uff08\u52a0\u6743\u878d\u5408\u6240\u6709\u4e13\u5bb6\u8f93\u51fa\uff09\u548cSparse MoA\uff08\u7a00\u758f\u6fc0\u6d3b\u8d21\u732e\u663e\u8457\u7684\u4e13\u5bb6\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f02\u6784MoA\u5728\u6027\u80fd\u548c\u53c2\u6570\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u540c\u8d28MoE-LoRA\u65b9\u6cd5\u3002", "conclusion": "MoA\u901a\u8fc7\u5f02\u6784\u4e13\u5bb6\u6574\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u77e5\u8bc6\u5411\u4e0b\u6e38\u4efb\u52a1\u7684\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2506.05934", "pdf": "https://arxiv.org/pdf/2506.05934", "abs": "https://arxiv.org/abs/2506.05934", "authors": ["Yixuan Zhu", "Haolin Wang", "Shilin Ma", "Wenliang Zhao", "Yansong Tang", "Lei Chen", "Jie Zhou"], "title": "FADE: Frequency-Aware Diffusion Model Factorization for Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025", "summary": "Recent advancements in diffusion frameworks have significantly enhanced video\nediting, achieving high fidelity and strong alignment with textual prompts.\nHowever, conventional approaches using image diffusion models fall short in\nhandling video dynamics, particularly for challenging temporal edits like\nmotion adjustments. While current video diffusion models produce high-quality\nresults, adapting them for efficient editing remains difficult due to the heavy\ncomputational demands that prevent the direct application of previous image\nediting techniques. To overcome these limitations, we introduce FADE, a\ntraining-free yet highly effective video editing approach that fully leverages\nthe inherent priors from pre-trained video diffusion models via frequency-aware\nfactorization. Rather than simply using these models, we first analyze the\nattention patterns within the video model to reveal how video priors are\ndistributed across different components. Building on these insights, we propose\na factorization strategy to optimize each component's specialized role.\nFurthermore, we devise spectrum-guided modulation to refine the sampling\ntrajectory with frequency domain cues, preventing information leakage and\nsupporting efficient, versatile edits while preserving the basic spatial and\ntemporal structure. Extensive experiments on real-world videos demonstrate that\nour method consistently delivers high-quality, realistic and temporally\ncoherent editing results both qualitatively and quantitatively. Code is\navailable at https://github.com/EternalEvan/FADE .", "AI": {"tldr": "FADE\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u611f\u77e5\u5206\u89e3\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u9891\u52a8\u6001\u7f16\u8f91\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u7f16\u8f91\u4e2d\u96be\u4ee5\u5904\u7406\u52a8\u6001\u5185\u5bb9\uff0c\u800c\u89c6\u9891\u6269\u6563\u6a21\u578b\u867d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u679c\uff0c\u4f46\u8ba1\u7b97\u91cf\u5927\u4e14\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u56fe\u50cf\u7f16\u8f91\u6280\u672f\u3002", "method": "FADE\u901a\u8fc7\u5206\u6790\u89c6\u9891\u6a21\u578b\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u63d0\u51fa\u9891\u7387\u611f\u77e5\u5206\u89e3\u7b56\u7565\uff0c\u5e76\u8bbe\u8ba1\u9891\u8c31\u5f15\u5bfc\u8c03\u5236\u4f18\u5316\u91c7\u6837\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFADE\u5728\u771f\u5b9e\u89c6\u9891\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u4e00\u81f4\u7684\u7f16\u8f91\u6548\u679c\u3002", "conclusion": "FADE\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89c6\u9891\u7f16\u8f91\u89e3\u51b3\u65b9\u6848\uff0c\u4fdd\u7559\u4e86\u89c6\u9891\u7684\u65f6\u7a7a\u7ed3\u6784\u3002"}}
{"id": "2506.05936", "pdf": "https://arxiv.org/pdf/2506.05936", "abs": "https://arxiv.org/abs/2506.05936", "authors": ["Wei Li", "Yanbin Wei", "Qiushi Huang", "Jiangyue Yan", "Yang Chen", "James T. Kwok", "Yu Zhang"], "title": "DynamicMind: A Tri-Mode Thinking System for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern large language models (LLMs) often struggle to dynamically adapt their\nreasoning depth to varying task complexities, leading to suboptimal performance\nor inefficient resource utilization. To address this, we introduce DynamicMind,\na novel tri-mode thinking system. DynamicMind empowers LLMs to autonomously\nselect between Fast, Normal, and Slow thinking modes for zero-shot question\nanswering (ZSQA) tasks through cognitive-inspired prompt engineering. Our\nframework's core innovations include: (1) expanding the established\ndual-process framework of fast and slow thinking into a tri-mode thinking\nsystem involving a normal thinking mode to preserve the intrinsic capabilities\nof LLM; (2) proposing the Thinking Density metric, which aligns computational\nresource allocation with problem complexity; and (3) developing the Thinking\nMode Capacity (TMC) dataset and a lightweight Mind Router to predict the\noptimal thinking mode. Extensive experiments across diverse mathematical,\ncommonsense, and scientific QA benchmarks demonstrate that DynamicMind achieves\nsuperior ZSQA capabilities while establishing an effective trade-off between\nperformance and computational efficiency.", "AI": {"tldr": "DynamicMind\u662f\u4e00\u79cd\u4e09\u6a21\u5f0f\u601d\u7ef4\u7cfb\u7edf\uff0c\u901a\u8fc7\u8ba4\u77e5\u542f\u53d1\u5f0f\u63d0\u793a\u5de5\u7a0b\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u591f\u81ea\u4e3b\u9009\u62e9\u5feb\u901f\u3001\u6b63\u5e38\u6216\u6162\u901f\u601d\u7ef4\u6a21\u5f0f\uff0c\u4ee5\u4f18\u5316\u96f6\u6837\u672c\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u3002", "motivation": "\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u96be\u4ee5\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u6216\u8d44\u6e90\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u3002DynamicMind\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e09\u6a21\u5f0f\u601d\u7ef4\u7cfb\u7edf\uff08\u5feb\u901f\u3001\u6b63\u5e38\u3001\u6162\u901f\uff09\uff0c\u5f15\u5165Thinking Density\u6307\u6807\u548cThinking Mode Capacity\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u8f7b\u91cf\u7ea7Mind Router\u9884\u6d4b\u6700\u4f18\u601d\u7ef4\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u79cd\u6570\u5b66\u3001\u5e38\u8bc6\u548c\u79d1\u5b66\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDynamicMind\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u96f6\u6837\u672c\u95ee\u7b54\u80fd\u529b\uff0c\u5e76\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "DynamicMind\u901a\u8fc7\u4e09\u6a21\u5f0f\u601d\u7ef4\u7cfb\u7edf\u548c\u8d44\u6e90\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u96f6\u6837\u672c\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u548c\u6548\u7387\u3002"}}
{"id": "2506.05937", "pdf": "https://arxiv.org/pdf/2506.05937", "abs": "https://arxiv.org/abs/2506.05937", "authors": ["Charmaine Barker", "Daniel Bethell", "Simos Gerasimou"], "title": "Quantifying Adversarial Uncertainty in Evidential Deep Learning using Conflict Resolution", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reliability of deep learning models is critical for deployment in high-stakes\napplications, where out-of-distribution or adversarial inputs may lead to\ndetrimental outcomes. Evidential Deep Learning, an efficient paradigm for\nuncertainty quantification, models predictions as Dirichlet distributions of a\nsingle forward pass. However, EDL is particularly vulnerable to adversarially\nperturbed inputs, making overconfident errors. Conflict-aware Evidential Deep\nLearning (C-EDL) is a lightweight post-hoc uncertainty quantification approach\nthat mitigates these issues, enhancing adversarial and OOD robustness without\nretraining. C-EDL generates diverse, task-preserving transformations per input\nand quantifies representational disagreement to calibrate uncertainty estimates\nwhen needed. C-EDL's conflict-aware prediction adjustment improves detection of\nOOD and adversarial inputs, maintaining high in-distribution accuracy and low\ncomputational overhead. Our experimental evaluation shows that C-EDL\nsignificantly outperforms state-of-the-art EDL variants and competitive\nbaselines, achieving substantial reductions in coverage for OOD data (up to\n55%) and adversarial data (up to 90%), across a range of datasets, attack\ntypes, and uncertainty metrics.", "AI": {"tldr": "Conflict-aware Evidential Deep Learning (C-EDL) \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u8868\u5f81\u5206\u6b67\u6765\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6027\u548c\u5206\u5e03\u5916\u8f93\u5165\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982Evidential Deep Learning\uff09\u5bf9\u5bf9\u6297\u6027\u548c\u5206\u5e03\u5916\u8f93\u5165\u5bb9\u6613\u4ea7\u751f\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9519\u8bef\u3002", "method": "C-EDL\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u4efb\u52a1\u4fdd\u7559\u53d8\u6362\uff0c\u91cf\u5316\u8868\u5f81\u5206\u6b67\uff0c\u52a8\u6001\u8c03\u6574\u9884\u6d4b\u4ee5\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cC-EDL\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u5206\u5e03\u5916\u6570\u636e\uff08\u51cf\u5c1155%\uff09\u548c\u5bf9\u6297\u6570\u636e\uff08\u51cf\u5c1190%\uff09\u7684\u8986\u76d6\u7387\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "C-EDL\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u529b\u3002"}}
{"id": "2506.05941", "pdf": "https://arxiv.org/pdf/2506.05941", "abs": "https://arxiv.org/abs/2506.05941", "authors": ["Luka Hobor", "Mario Brcic", "Lidija Polutnik", "Ante Kapetanovic"], "title": "Comparative Analysis of Modern Machine Learning Models for Retail Sales Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "20 total pages, 10 pages article, 10 pages appendix, 3 figures, 24\n  tables", "summary": "Accurate forecasting is key for all business planning. When estimated sales\nare too high, brick-and-mortar retailers may incur higher costs due to unsold\ninventories, higher labor and storage space costs, etc. On the other hand, when\nforecasts underestimate the level of sales, firms experience lost sales,\nshortages, and impact on the reputation of the retailer in their relevant\nmarket. Accurate forecasting presents a competitive advantage for companies. It\nfacilitates the achievement of revenue and profit goals and execution of\npricing strategy and tactics. In this study, we provide an exhaustive\nassessment of the forecasting models applied to a high-resolution\nbrick-and-mortar retail dataset. Our forecasting framework addresses the\nproblems found in retail environments, including intermittent demand, missing\nvalues, and frequent product turnover. We compare tree-based ensembles (such as\nXGBoost and LightGBM) and state-of-the-art neural network architectures\n(including N-BEATS, NHITS, and the Temporal Fusion Transformer) across various\nexperimental settings. Our results show that localized modeling strategies\nespecially those using tree-based models on individual groups with non-imputed\ndata, consistently deliver superior forecasting accuracy and computational\nefficiency. In contrast, neural models benefit from advanced imputation\nmethods, yet still fall short in handling the irregularities typical of\nphysical retail data. These results further practical understanding for model\nselection in retail environment and highlight the significance of data\npreprocessing to improve forecast performance.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u96f6\u552e\u4e1a\u4e2d\u51c6\u786e\u9884\u6d4b\u7684\u91cd\u8981\u6027\uff0c\u6bd4\u8f83\u4e86\u6811\u96c6\u6210\u6a21\u578b\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u96f6\u552e\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6811\u6a21\u578b\u5728\u5c40\u90e8\u5efa\u6a21\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u5bf9\u96f6\u552e\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u8fc7\u9ad8\u6216\u8fc7\u4f4e\u7684\u9884\u6d4b\u90fd\u4f1a\u5e26\u6765\u6210\u672c\u6216\u58f0\u8a89\u635f\u5931\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540c\u9884\u6d4b\u6a21\u578b\u5728\u96f6\u552e\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u6811\u96c6\u6210\u6a21\u578b\uff08\u5982XGBoost\u3001LightGBM\uff09\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08\u5982N-BEATS\u3001NHITS\uff09\u5bf9\u9ad8\u5206\u8fa8\u7387\u96f6\u552e\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u6bd4\u8f83\u5176\u8868\u73b0\u3002", "result": "\u6811\u96c6\u6210\u6a21\u578b\u5728\u5c40\u90e8\u5efa\u6a21\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u9700\u8981\u9ad8\u7ea7\u586b\u8865\u65b9\u6cd5\u4f46\u4ecd\u96be\u4ee5\u5904\u7406\u96f6\u552e\u6570\u636e\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u96f6\u552e\u4e1a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u9884\u5904\u7406\u5bf9\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.05947", "pdf": "https://arxiv.org/pdf/2506.05947", "abs": "https://arxiv.org/abs/2506.05947", "authors": ["Xinjie Zhang", "Wenxuan Wang", "Qin Jin"], "title": "IntentionESC: An Intention-Centered Framework for Enhancing Emotional Support in Dialogue Systems", "categories": ["cs.CL", "cs.AI"], "comment": "ACL2025 findings", "summary": "In emotional support conversations, unclear intentions can lead supporters to\nemploy inappropriate strategies, inadvertently imposing their expectations or\nsolutions on the seeker. Clearly defined intentions are essential for guiding\nboth the supporter's motivations and the overall emotional support process. In\nthis paper, we propose the Intention-centered Emotional Support Conversation\n(IntentionESC) framework, which defines the possible intentions of supporters\nin emotional support conversations, identifies key emotional state aspects for\ninferring these intentions, and maps them to appropriate support strategies.\nWhile Large Language Models (LLMs) excel in text generating, they fundamentally\noperate as probabilistic models trained on extensive datasets, lacking a true\nunderstanding of human thought processes and intentions. To address this\nlimitation, we introduce the Intention Centric Chain-of-Thought (ICECoT)\nmechanism. ICECoT enables LLMs to mimic human reasoning by analyzing emotional\nstates, inferring intentions, and selecting suitable support strategies,\nthereby generating more effective emotional support responses. To train the\nmodel with ICECoT and integrate expert knowledge, we design an automated\nannotation pipeline that produces high-quality training data. Furthermore, we\ndevelop a comprehensive evaluation scheme to assess emotional support efficacy\nand conduct extensive experiments to validate our framework. Our data and code\nare available at https://github.com/43zxj/IntentionESC_ICECoT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faIntentionESC\u6846\u67b6\u548cICECoT\u673a\u5236\uff0c\u901a\u8fc7\u660e\u786e\u652f\u6301\u8005\u610f\u56fe\u548c\u60c5\u611f\u72b6\u6001\uff0c\u63d0\u5347\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u7684\u6548\u679c\u3002", "motivation": "\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u4e2d\uff0c\u6a21\u7cca\u7684\u610f\u56fe\u53ef\u80fd\u5bfc\u81f4\u652f\u6301\u8005\u91c7\u7528\u4e0d\u6070\u5f53\u7684\u7b56\u7565\uff0c\u660e\u786e\u610f\u56fe\u5bf9\u6307\u5bfc\u652f\u6301\u8fc7\u7a0b\u548c\u52a8\u673a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faIntentionESC\u6846\u67b6\u5b9a\u4e49\u652f\u6301\u8005\u610f\u56fe\uff0c\u7ed3\u5408ICECoT\u673a\u5236\u4f7fLLM\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\uff0c\u751f\u6210\u66f4\u6709\u6548\u7684\u652f\u6301\u54cd\u5e94\u3002", "result": "\u5f00\u53d1\u4e86\u81ea\u52a8\u6807\u6ce8\u7ba1\u9053\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "IntentionESC\u548cICECoT\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u7684\u6548\u679c\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.05952", "pdf": "https://arxiv.org/pdf/2506.05952", "abs": "https://arxiv.org/abs/2506.05952", "authors": ["Dongjie Fu", "Tengjiao Sun", "Pengcheng Fang", "Xiaohao Cai", "Hansung Kim"], "title": "MOGO: Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 4 figures, conference", "summary": "Recent advances in transformer-based text-to-motion generation have led to\nimpressive progress in synthesizing high-quality human motion. Nevertheless,\njointly achieving high fidelity, streaming capability, real-time\nresponsiveness, and scalability remains a fundamental challenge. In this paper,\nwe propose MOGO (Motion Generation with One-pass), a novel autoregressive\nframework tailored for efficient and real-time 3D motion generation. MOGO\ncomprises two key components: (1) MoSA-VQ, a motion scale-adaptive residual\nvector quantization module that hierarchically discretizes motion sequences\nwith learnable scaling to produce compact yet expressive representations; and\n(2) RQHC-Transformer, a residual quantized hierarchical causal transformer that\ngenerates multi-layer motion tokens in a single forward pass, significantly\nreducing inference latency. To enhance semantic fidelity, we further introduce\na text condition alignment mechanism that improves motion decoding under\ntextual control. Extensive experiments on benchmark datasets including\nHumanML3D, KIT-ML, and CMP demonstrate that MOGO achieves competitive or\nsuperior generation quality compared to state-of-the-art transformer-based\nmethods, while offering substantial improvements in real-time performance,\nstreaming generation, and generalization under zero-shot settings.", "AI": {"tldr": "MOGO\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5b9e\u65f63D\u52a8\u4f5c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7MoSA-VQ\u548cRQHC-Transformer\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u52a8\u4f5c\u751f\u6210\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u5728\u540c\u65f6\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u5b9e\u65f6\u54cd\u5e94\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "MOGO\u5305\u542bMoSA-VQ\uff08\u81ea\u9002\u5e94\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u6a21\u5757\uff09\u548cRQHC-Transformer\uff08\u6b8b\u91cf\u5316\u5c42\u6b21\u56e0\u679cTransformer\uff09\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u9012\u751f\u6210\u52a8\u4f5c\u3002", "result": "\u5728HumanML3D\u7b49\u6570\u636e\u96c6\u4e0a\uff0cMOGO\u5728\u751f\u6210\u8d28\u91cf\u548c\u5b9e\u65f6\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u96f6\u6837\u672c\u6cdb\u5316\u3002", "conclusion": "MOGO\u5728\u9ad8\u6548\u5b9e\u65f6\u52a8\u4f5c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.05968", "pdf": "https://arxiv.org/pdf/2506.05968", "abs": "https://arxiv.org/abs/2506.05968", "authors": ["Motoki Omura", "Kazuki Ota", "Takayuki Osa", "Yusuke Mukuta", "Tatsuya Harada"], "title": "Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "Accepted at ICML 2025. Source code:\n  https://github.com/motokiomura/annealed-q-learning", "summary": "For continuous action spaces, actor-critic methods are widely used in online\nreinforcement learning (RL). However, unlike RL algorithms for discrete\nactions, which generally model the optimal value function using the Bellman\noptimality operator, RL algorithms for continuous actions typically model\nQ-values for the current policy using the Bellman operator. These algorithms\nfor continuous actions rely exclusively on policy updates for improvement,\nwhich often results in low sample efficiency. This study examines the\neffectiveness of incorporating the Bellman optimality operator into\nactor-critic frameworks. Experiments in a simple environment show that modeling\noptimal values accelerates learning but leads to overestimation bias. To\naddress this, we propose an annealing approach that gradually transitions from\nthe Bellman optimality operator to the Bellman operator, thereby accelerating\nlearning while mitigating bias. Our method, combined with TD3 and SAC,\nsignificantly outperforms existing approaches across various locomotion and\nmanipulation tasks, demonstrating improved performance and robustness to\nhyperparameters related to optimality.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f15\u5165Bellman\u6700\u4f18\u7b97\u5b50\u5bf9actor-critic\u65b9\u6cd5\u7684\u6539\u8fdb\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9000\u706b\u65b9\u6cd5\u4ee5\u5e73\u8861\u5b66\u4e60\u52a0\u901f\u4e0e\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7b56\u7565\u66f4\u65b0\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u7684\u65b9\u6cd5\u5219\u901a\u8fc7Bellman\u6700\u4f18\u7b97\u5b50\u5efa\u6a21\u6700\u4f18\u503c\u51fd\u6570\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5c06Bellman\u6700\u4f18\u7b97\u5b50\u5f15\u5165\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9000\u706b\u65b9\u6cd5\uff0c\u9010\u6b65\u4eceBellman\u6700\u4f18\u7b97\u5b50\u8fc7\u6e21\u5230Bellman\u7b97\u5b50\uff0c\u4ee5\u52a0\u901f\u5b66\u4e60\u5e76\u51cf\u5c11\u504f\u5dee\u3002\u8be5\u65b9\u6cd5\u4e0eTD3\u548cSAC\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5efa\u6a21\u6700\u4f18\u503c\u80fd\u52a0\u901f\u5b66\u4e60\u4f46\u4f1a\u5bfc\u81f4\u9ad8\u4f30\u504f\u5dee\u3002\u9000\u706b\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u8d85\u53c2\u6570\u9c81\u68d2\u6027\u3002", "conclusion": "\u5f15\u5165Bellman\u6700\u4f18\u7b97\u5b50\u5e76\u901a\u8fc7\u9000\u706b\u65b9\u6cd5\u5e73\u8861\u5b66\u4e60\u4e0e\u504f\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2506.05970", "pdf": "https://arxiv.org/pdf/2506.05970", "abs": "https://arxiv.org/abs/2506.05970", "authors": ["Kazutoshi Shinoda", "Nobukatsu Hojo", "Kyosuke Nishida", "Yoshihiro Yamazaki", "Keita Suzuki", "Hiroaki Sugiyama", "Kuniko Saito"], "title": "Let's Put Ourselves in Sally's Shoes: Shoes-of-Others Prefixing Improves Theory of Mind in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "14pages, 12 figures", "summary": "Recent studies have shown that Theory of Mind (ToM) in large language models\n(LLMs) has not reached human-level performance yet. Since fine-tuning LLMs on\nToM datasets often degrades their generalization, several inference-time\nmethods have been proposed to enhance ToM in LLMs. However, existing\ninference-time methods for ToM are specialized for inferring beliefs from\ncontexts involving changes in the world state. In this study, we present a new\ninference-time method for ToM, Shoes-of-Others (SoO) prefixing, which makes\nfewer assumptions about contexts and is applicable to broader scenarios. SoO\nprefixing simply specifies the beginning of LLM outputs with ``Let's put\nourselves in A's shoes.'', where A denotes the target character's name. We\nevaluate SoO prefixing on two benchmarks that assess ToM in conversational and\nnarrative contexts without changes in the world state and find that it\nconsistently improves ToM across five categories of mental states. Our analysis\nsuggests that SoO prefixing elicits faithful thoughts, thereby improving the\nToM performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\uff08SoO\u524d\u7f00\u6cd5\uff09\uff0c\u901a\u8fc7\u7b80\u5355\u5730\u5728LLM\u8f93\u51fa\u524d\u6dfb\u52a0\u7279\u5b9a\u524d\u7f00\uff0c\u63d0\u5347\u5176\u5728\u672a\u6d89\u53ca\u4e16\u754c\u72b6\u6001\u53d8\u5316\u7684\u573a\u666f\u4e2d\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u65f6\u65b9\u6cd5\u5bf9\u5fc3\u7406\u7406\u8bba\u7684\u63d0\u5347\u5c40\u9650\u4e8e\u7279\u5b9a\u573a\u666f\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faShoes-of-Others\uff08SoO\uff09\u524d\u7f00\u6cd5\uff0c\u5373\u5728LLM\u8f93\u51fa\u524d\u6dfb\u52a0\u7279\u5b9a\u524d\u7f00\uff08\u5982\u201cLet's put ourselves in A's shoes\u201d\uff09\u3002", "result": "\u5728\u4e24\u79cd\u672a\u6d89\u53ca\u4e16\u754c\u72b6\u6001\u53d8\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSoO\u524d\u7f00\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u5fc3\u7406\u7406\u8bba\u8868\u73b0\u3002", "conclusion": "SoO\u524d\u7f00\u6cd5\u662f\u4e00\u79cd\u7b80\u5355\u4e14\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u5347LLM\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u3002"}}
{"id": "2506.05971", "pdf": "https://arxiv.org/pdf/2506.05971", "abs": "https://arxiv.org/abs/2506.05971", "authors": ["Jacob Bamberger", "Benjamin Gutteridge", "Scott le Roux", "Michael M. Bronstein", "Xiaowen Dong"], "title": "On Measuring Long-Range Interactions in Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025", "summary": "Long-range graph tasks -- those dependent on interactions between distant\nnodes -- are an open problem in graph neural network research. Real-world\nbenchmark tasks, especially the Long Range Graph Benchmark, have become popular\nfor validating the long-range capability of proposed architectures. However,\nthis is an empirical approach that lacks both robustness and theoretical\nunderpinning; a more principled characterization of the long-range problem is\nrequired. To bridge this gap, we formalize long-range interactions in graph\ntasks, introduce a range measure for operators on graphs, and validate it with\nsynthetic experiments. We then leverage our measure to examine commonly used\ntasks and architectures, and discuss to what extent they are, in fact,\nlong-range. We believe our work advances efforts to define and address the\nlong-range problem on graphs, and that our range measure will aid evaluation of\nnew datasets and architectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u56fe\u4efb\u52a1\u4e2d\u957f\u7a0b\u4ea4\u4e92\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\u548c\u8303\u56f4\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7528\u4e8e\u8bc4\u4f30\u73b0\u6709\u4efb\u52a1\u548c\u67b6\u6784\u7684\u957f\u7a0b\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\u7814\u7a76\u4e2d\u957f\u7a0b\u4efb\u52a1\u7f3a\u4e4f\u7406\u8bba\u652f\u6491\u548c\u9c81\u68d2\u6027\u9a8c\u8bc1\u7684\u95ee\u9898\u3002", "method": "\u5f62\u5f0f\u5316\u5b9a\u4e49\u56fe\u4efb\u52a1\u4e2d\u7684\u957f\u7a0b\u4ea4\u4e92\uff0c\u5f15\u5165\u56fe\u64cd\u4f5c\u7b26\u7684\u8303\u56f4\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u9a8c\u8bc1\u4e86\u8303\u56f4\u5ea6\u91cf\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86\u5e38\u89c1\u4efb\u52a1\u548c\u67b6\u6784\u7684\u957f\u7a0b\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5b9a\u4e49\u548c\u89e3\u51b3\u56fe\u4e0a\u7684\u957f\u7a0b\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u8303\u56f4\u5ea6\u91cf\u6709\u52a9\u4e8e\u65b0\u6570\u636e\u96c6\u548c\u67b6\u6784\u7684\u8bc4\u4f30\u3002"}}
{"id": "2506.05980", "pdf": "https://arxiv.org/pdf/2506.05980", "abs": "https://arxiv.org/abs/2506.05980", "authors": ["Geonwoo Cho", "Jaemoon Lee", "Jaegyun Im", "Subi Lee", "Jihwan Lee", "Sundong Kim"], "title": "AMPED: Adaptive Multi-objective Projection for balancing Exploration and skill Diversification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Skill-based reinforcement learning (SBRL) enables rapid adaptation in\nenvironments with sparse rewards by pretraining a skill-conditioned policy.\nEffective skill learning requires jointly maximizing both exploration and skill\ndiversity. However, existing methods often face challenges in simultaneously\noptimizing for these two conflicting objectives. In this work, we propose a new\nmethod, Adaptive Multi-objective Projection for balancing Exploration and skill\nDiversification (AMPED), which explicitly addresses both exploration and skill\ndiversification. We begin by conducting extensive ablation studies to identify\nand define a set of objectives that effectively capture the aspects of\nexploration and skill diversity, respectively. During the skill pretraining\nphase, AMPED introduces a gradient surgery technique to balance the objectives\nof exploration and skill diversity, mitigating conflicts and reducing reliance\non heuristic tuning. In the subsequent fine-tuning phase, AMPED incorporates a\nskill selector module that dynamically selects suitable skills for downstream\ntasks, based on task-specific performance signals. Our approach achieves\nperformance that surpasses SBRL baselines across various benchmarks. These\nresults highlight the importance of explicitly harmonizing exploration and\ndiversity and demonstrate the effectiveness of AMPED in enabling robust and\ngeneralizable skill learning. Project Page: https://geonwoo.me/amped/", "AI": {"tldr": "AMPED\u662f\u4e00\u79cd\u65b0\u7684\u6280\u80fd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u624b\u672f\u6280\u672f\u5e73\u8861\u63a2\u7d22\u4e0e\u6280\u80fd\u591a\u6837\u6027\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u52a8\u6001\u9009\u62e9\u6280\u80fd\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4f18\u5316\u63a2\u7d22\u4e0e\u6280\u80fd\u591a\u6837\u6027\u8fd9\u4e24\u4e2a\u51b2\u7a81\u76ee\u6807\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "AMPED\u901a\u8fc7\u68af\u5ea6\u624b\u672f\u6280\u672f\u5e73\u8861\u63a2\u7d22\u4e0e\u591a\u6837\u6027\u76ee\u6807\uff0c\u5e76\u5f15\u5165\u6280\u80fd\u9009\u62e9\u6a21\u5757\u52a8\u6001\u9002\u914d\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "AMPED\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4f18\u4e8e\u73b0\u6709SBRL\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "AMPED\u901a\u8fc7\u663e\u5f0f\u534f\u8c03\u63a2\u7d22\u4e0e\u591a\u6837\u6027\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u901a\u7528\u7684\u6280\u80fd\u5b66\u4e60\u3002"}}
{"id": "2506.05984", "pdf": "https://arxiv.org/pdf/2506.05984", "abs": "https://arxiv.org/abs/2506.05984", "authors": ["Cheng-Han Chiang", "Xiaofei Wang", "Chung-Ching Lin", "Kevin Lin", "Linjie Li", "Radu Kopetz", "Yao Qian", "Zhendong Wang", "Zhengyuan Yang", "Hung-yi Lee", "Lijuan Wang"], "title": "Audio-Aware Large Language Models as Judges for Speaking Styles", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "Audio-aware large language models (ALLMs) can understand the textual and\nnon-textual information in the audio input. In this paper, we explore using\nALLMs as an automatic judge to assess the speaking styles of speeches. We use\nALLM judges to evaluate the speeches generated by SLMs on two tasks: voice\nstyle instruction following and role-playing. The speaking style we consider\nincludes emotion, volume, speaking pace, word emphasis, pitch control, and\nnon-verbal elements. We use four spoken language models (SLMs) to complete the\ntwo tasks and use humans and ALLMs to judge the SLMs' responses. We compare two\nALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and\nshow that the agreement between Gemini and human judges is comparable to the\nagreement between human evaluators. These promising results show that ALLMs can\nbe used as a judge to evaluate SLMs. Our results also reveal that current SLMs,\neven GPT-4o-audio, still have room for improvement in controlling the speaking\nstyle and generating natural dialogues.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u97f3\u9891\u611f\u77e5\u5927\u8bed\u8a00\u6a21\u578b\uff08ALLMs\uff09\u4f5c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\uff0c\u8bc4\u4f30\u8bed\u97f3\u751f\u6210\u6a21\u578b\u7684\u8bf4\u8bdd\u98ce\u683c\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1ALLMs\u80fd\u5426\u4f5c\u4e3a\u8bc4\u4f30\u8bed\u97f3\u98ce\u683c\u7684\u5de5\u5177\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u4f7f\u7528\u4e24\u79cdALLMs\uff08GPT-4o-audio\u548cGemini-2.5-pro\uff09\u8bc4\u4f30\u56db\u79cd\u8bed\u97f3\u6a21\u578b\u5728\u4e24\u9879\u4efb\u52a1\uff08\u8bed\u97f3\u98ce\u683c\u6307\u4ee4\u9075\u5faa\u548c\u89d2\u8272\u626e\u6f14\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u5bf9\u6bd4\u3002", "result": "Gemini-2.5-pro\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u8f83\u9ad8\uff0c\u8868\u660eALLMs\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\uff1b\u540c\u65f6\u53d1\u73b0\u73b0\u6709\u8bed\u97f3\u6a21\u578b\u5728\u98ce\u683c\u63a7\u5236\u548c\u81ea\u7136\u5bf9\u8bdd\u751f\u6210\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "ALLMs\u53ef\u4f5c\u4e3a\u8bed\u97f3\u98ce\u683c\u8bc4\u4f30\u7684\u6709\u6548\u5de5\u5177\uff0c\u4f46\u5f53\u524d\u8bed\u97f3\u6a21\u578b\u4ecd\u9700\u4f18\u5316\u3002"}}
{"id": "2506.06006", "pdf": "https://arxiv.org/pdf/2506.06006", "abs": "https://arxiv.org/abs/2506.06006", "authors": ["Yifu Qiu", "Yftah Ziser", "Anna Korhonen", "Shay B. Cohen", "Edoardo M. Ponti"], "title": "Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "To what extent do vision-and-language foundation models possess a realistic\nworld model (observation $\\times$ action $\\rightarrow$ observation) and a\ndynamics model (observation $\\times$ observation $\\rightarrow$ action), when\nactions are expressed through language? While open-source foundation models\nstruggle with both, we find that fine-tuning them to acquire a dynamics model\nthrough supervision is significantly easier than acquiring a world model. In\nturn, dynamics models can be used to bootstrap world models through two main\nstrategies: 1) weakly supervised learning from synthetic data and 2) inference\ntime verification. Firstly, the dynamics model can annotate actions for\nunlabelled pairs of video frame observations to expand the training data. We\nfurther propose a new objective, where image tokens in observation pairs are\nweighted by their importance, as predicted by a recognition model. Secondly,\nthe dynamics models can assign rewards to multiple samples of the world model\nto score them, effectively guiding search at inference time. We evaluate the\nworld models resulting from both strategies through the task of action-centric\nimage editing on Aurora-Bench. Our best model achieves a performance\ncompetitive with state-of-the-art image editing models, improving on them by a\nmargin of $15\\%$ on real-world subsets according to GPT4o-as-judge, and\nachieving the best average human evaluation across all subsets of Aurora-Bench.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5728\u52a8\u6001\u6a21\u578b\uff08\u89c2\u5bdf\u00d7\u89c2\u5bdf\u2192\u52a8\u4f5c\uff09\u4e0a\u7684\u5fae\u8c03\u6bd4\u4e16\u754c\u6a21\u578b\uff08\u89c2\u5bdf\u00d7\u52a8\u4f5c\u2192\u89c2\u5bdf\uff09\u66f4\u5bb9\u6613\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u7b56\u7565\u5229\u7528\u52a8\u6001\u6a21\u578b\u5f15\u5bfc\u4e16\u754c\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u662f\u5426\u5177\u5907\u771f\u5b9e\u7684\u4e16\u754c\u6a21\u578b\u548c\u52a8\u6001\u6a21\u578b\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u5f00\u6e90\u6a21\u578b\u5728\u8fd9\u4e24\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u52a8\u6001\u6a21\u578b\uff0c\u5e76\u5229\u7528\u5176\u901a\u8fc7\u5f31\u76d1\u7763\u5b66\u4e60\u548c\u63a8\u7406\u65f6\u9a8c\u8bc1\u4e24\u79cd\u7b56\u7565\u5f15\u5bfc\u4e16\u754c\u6a21\u578b\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728Aurora-Bench\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\uff0cGPT4o\u8bc4\u4f30\u63d0\u534715%\uff0c\u4eba\u7c7b\u8bc4\u4ef7\u6700\u4f73\u3002", "conclusion": "\u52a8\u6001\u6a21\u578b\u53ef\u6709\u6548\u5f15\u5bfc\u4e16\u754c\u6a21\u578b\uff0c\u63d0\u5347\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2506.06007", "pdf": "https://arxiv.org/pdf/2506.06007", "abs": "https://arxiv.org/abs/2506.06007", "authors": ["Alejandro Puente-Castro", "Enrique Fernandez-Blanco", "Daniel Rivero", "Andres Molares-Ulloa"], "title": "Enhancing Orthopox Image Classification Using Hybrid Machine Learning and Deep Learning Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Orthopoxvirus infections must be accurately classified from medical pictures\nfor an easy and early diagnosis and epidemic prevention. The necessity for\nautomated and scalable solutions is highlighted by the fact that traditional\ndiagnostic techniques can be time-consuming and require expert interpretation\nand there are few and biased data sets of the different types of Orthopox. In\norder to improve classification performance and lower computational costs, a\nhybrid strategy is put forth in this paper that uses Machine Learning models\ncombined with pretrained Deep Learning models to extract deep feature\nrepresentations without the need for augmented data. The findings show that\nthis feature extraction method, when paired with other methods in the\nstate-of-the-art, produces excellent classification outcomes while preserving\ntraining and inference efficiency. The proposed approach demonstrates strong\ngeneralization and robustness across multiple evaluation settings, offering a\nscalable and interpretable solution for real-world clinical deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u533b\u5b66\u56fe\u50cf\u4e2d\u51c6\u786e\u5206\u7c7bOrthopox\u75c5\u6bd2\u611f\u67d3\uff0c\u65e0\u9700\u6570\u636e\u589e\u5f3a\u5373\u53ef\u63d0\u53d6\u6df1\u5ea6\u7279\u5f81\u3002", "motivation": "\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u5bb6\uff0c\u4e14\u6570\u636e\u7a00\u7f3a\u4e14\u504f\u9887\uff0c\u9700\u8981\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u53d6\u6df1\u5ea6\u7279\u5f81\uff0c\u65e0\u9700\u6570\u636e\u589e\u5f3a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e34\u5e8a\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06008", "pdf": "https://arxiv.org/pdf/2506.06008", "abs": "https://arxiv.org/abs/2506.06008", "authors": ["Peijie Liu", "Fengli Xu", "Yong Li"], "title": "Token Signature: Predicting Chain-of-Thought Gains with Token Decoding Feature in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 6 figures, 13 tables(Accept by ICML2025)", "summary": "Chain-of-Thought (CoT) technique has proven effective in improving the\nperformance of large language models (LLMs) on complex reasoning tasks.\nHowever, the performance gains are inconsistent across different tasks, and the\nunderlying mechanism remains a long-standing research question. In this work,\nwe make a preliminary observation that the monotonicity of token probability\ndistributions may be correlated with the gains achieved through CoT reasoning.\nLeveraging this insight, we propose two indicators based on the token\nprobability distribution to assess CoT effectiveness across different tasks. By\ncombining instance-level indicators with logistic regression model, we\nintroduce Dynamic CoT, a method that dynamically select between CoT and direct\nanswer. Furthermore, we extend Dynamic CoT to closed-source models by\ntransferring decision strategies learned from open-source models. Our\nindicators for assessing CoT effectiveness achieve an accuracy of 89.2\\%, and\nDynamic CoT reduces token consumption by more than 35\\% while maintaining high\naccuracy. Overall, our work offers a novel perspective on the underlying\nmechanisms of CoT reasoning and provides a framework for its more efficient\ndeployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u6807\u8bb0\u6982\u7387\u5206\u5e03\u7684\u6307\u6807\u8bc4\u4f30CoT\u6709\u6548\u6027\uff0c\u5e76\u5f15\u5165Dynamic CoT\u52a8\u6001\u9009\u62e9\u56de\u7b54\u65b9\u5f0f\uff0c\u51cf\u5c11\u6807\u8bb0\u6d88\u8017\u3002", "motivation": "CoT\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u6548\u679c\u4e0d\u4e00\u81f4\uff0c\u5176\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22CoT\u6027\u80fd\u63d0\u5347\u7684\u6f5c\u5728\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u57fa\u4e8e\u6807\u8bb0\u6982\u7387\u5206\u5e03\u7684\u6307\u6807\uff0c\u7ed3\u5408\u903b\u8f91\u56de\u5f52\u6a21\u578b\u52a8\u6001\u9009\u62e9CoT\u6216\u76f4\u63a5\u56de\u7b54\uff0c\u5e76\u6269\u5c55\u5230\u95ed\u6e90\u6a21\u578b\u3002", "result": "\u6307\u6807\u8bc4\u4f30CoT\u6709\u6548\u6027\u7684\u51c6\u786e\u7387\u8fbe89.2%\uff0cDynamic CoT\u51cf\u5c1135%\u4ee5\u4e0a\u6807\u8bb0\u6d88\u8017\u4e14\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u4e3aCoT\u673a\u5236\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u90e8\u7f72\u6846\u67b6\u3002"}}
{"id": "2506.06009", "pdf": "https://arxiv.org/pdf/2506.06009", "abs": "https://arxiv.org/abs/2506.06009", "authors": ["Haoke Zhang", "Xiaobo Liang", "Cunxiang Wang", "Juntao Li", "Min Zhang"], "title": "Unlocking Recursive Thinking of LLMs: Alignment via Refinement", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the Findings of ACL 2025", "summary": "The OpenAI o1-series models have demonstrated that leveraging long-form Chain\nof Thought (CoT) can substantially enhance performance. However, the recursive\nthinking capabilities of Large Language Models (LLMs) remain limited,\nparticularly in the absence of expert-curated data for distillation. In this\npaper, we propose \\textbf{AvR}: \\textbf{Alignment via Refinement}, a novel\nmethod aimed at unlocking the potential of LLMs for recursive reasoning through\nlong-form CoT. AvR introduces a refinement process that integrates criticism\nand improvement actions, guided by differentiable learning techniques to\noptimize \\textbf{refinement-aware rewards}. As a result, the synthesized\nmulti-round data can be organized as a long refinement thought, further\nenabling test-time scaling. Experimental results show that AvR significantly\noutperforms conventional preference optimization methods. Notably, with only 3k\nsynthetic samples, our method boosts the performance of the LLaMA-3-8B-Instruct\nmodel by over 20\\% in win rate on AlpacaEval 2.0. Our code is available at\nGithub (https://github.com/Banner-Z/AvR.git).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAvR\uff08Alignment via Refinement\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u957f\u94fe\u601d\u7ef4\uff08CoT\uff09\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9012\u5f52\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dLLMs\u7684\u9012\u5f52\u601d\u7ef4\u80fd\u529b\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u91ca\u653e\u5176\u6f5c\u529b\u3002", "method": "AvR\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u7ed3\u5408\u6279\u8bc4\u548c\u6539\u8fdb\u52a8\u4f5c\u7684\u7ec6\u5316\u8fc7\u7a0b\uff0c\u5229\u7528\u53ef\u5fae\u5206\u5b66\u4e60\u6280\u672f\u4f18\u5316\u7ec6\u5316\u611f\u77e5\u5956\u52b1\uff0c\u751f\u6210\u591a\u8f6e\u6570\u636e\u5e76\u7ec4\u7ec7\u4e3a\u957f\u7ec6\u5316\u601d\u7ef4\u94fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAvR\u4ec5\u75283k\u5408\u6210\u6837\u672c\u5c31\u5c06LLaMA-3-8B-Instruct\u6a21\u578b\u5728AlpacaEval 2.0\u4e0a\u7684\u80dc\u7387\u63d0\u5347\u4e8620%\u4ee5\u4e0a\u3002", "conclusion": "AvR\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347LLMs\u7684\u9012\u5f52\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.06018", "pdf": "https://arxiv.org/pdf/2506.06018", "abs": "https://arxiv.org/abs/2506.06018", "authors": ["Chaoyi Zhu", "Zaitang Li", "Renyi Yang", "Robert Birke", "Pin-Yu Chen", "Tsung-Yi Ho", "Lydia Y. Chen"], "title": "Optimization-Free Universal Watermark Forgery with Regenerative Diffusion Models", "categories": ["cs.MM", "cs.AI", "cs.CR"], "comment": null, "summary": "Watermarking becomes one of the pivotal solutions to trace and verify the\norigin of synthetic images generated by artificial intelligence models, but it\nis not free of risks. Recent studies demonstrate the capability to forge\nwatermarks from a target image onto cover images via adversarial optimization\nwithout knowledge of the target generative model and watermark schemes. In this\npaper, we uncover a greater risk of an optimization-free and universal\nwatermark forgery that harnesses existing regenerative diffusion models. Our\nproposed forgery attack, PnP (Plug-and-Plant), seamlessly extracts and\nintegrates the target watermark via regenerating the image, without needing any\nadditional optimization routine. It allows for universal watermark forgery that\nworks independently of the target image's origin or the watermarking model\nused. We explore the watermarked latent extracted from the target image and\nvisual-textual context of cover images as priors to guide sampling of the\nregenerative process. Extensive evaluation on 24 scenarios of\nmodel-data-watermark combinations demonstrates that PnP can successfully forge\nthe watermark (up to 100% detectability and user attribution), and maintain the\nbest visual perception. By bypassing model retraining and enabling adaptability\nto any image, our approach significantly broadens the scope of forgery attacks,\npresenting a greater challenge to the security of current watermarking\ntechniques for diffusion models and the authority of watermarking schemes in\nsynthetic data generation and governance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4f18\u5316\u7684\u901a\u7528\u6c34\u5370\u4f2a\u9020\u65b9\u6cd5PnP\uff0c\u5229\u7528\u518d\u751f\u6269\u6563\u6a21\u578b\u63d0\u53d6\u5e76\u690d\u5165\u76ee\u6807\u6c34\u5370\uff0c\u65e0\u9700\u989d\u5916\u4f18\u5316\uff0c\u4e14\u72ec\u7acb\u4e8e\u56fe\u50cf\u6765\u6e90\u6216\u6c34\u5370\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6c34\u5370\u4f2a\u9020\u65b9\u6cd5\u4f9d\u8d56\u5bf9\u6297\u4f18\u5316\uff0c\u4e14\u9700\u76ee\u6807\u751f\u6210\u6a21\u578b\u548c\u6c34\u5370\u65b9\u6848\u7684\u77e5\u8bc6\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u4f2a\u9020\u98ce\u9669\uff0c\u5229\u7528\u73b0\u6709\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u901a\u7528\u4f2a\u9020\u3002", "method": "\u63d0\u51faPnP\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u518d\u751f\u56fe\u50cf\u63d0\u53d6\u5e76\u690d\u5165\u76ee\u6807\u6c34\u5370\uff0c\u5229\u7528\u6c34\u5370\u6f5c\u53d8\u91cf\u548c\u89c6\u89c9-\u6587\u672c\u4e0a\u4e0b\u6587\u4f5c\u4e3a\u5148\u9a8c\u6307\u5bfc\u91c7\u6837\u3002", "result": "\u572824\u79cd\u6a21\u578b-\u6570\u636e-\u6c34\u5370\u7ec4\u5408\u4e2d\uff0cPnP\u6210\u529f\u4f2a\u9020\u6c34\u5370\uff08\u68c0\u6d4b\u7387\u548c\u7528\u6237\u5f52\u56e0\u8fbe100%\uff09\uff0c\u5e76\u4fdd\u6301\u6700\u4f73\u89c6\u89c9\u611f\u77e5\u3002", "conclusion": "PnP\u7ed5\u8fc7\u6a21\u578b\u91cd\u8bad\u7ec3\uff0c\u9002\u5e94\u4efb\u4f55\u56fe\u50cf\uff0c\u6269\u5927\u4e86\u4f2a\u9020\u653b\u51fb\u8303\u56f4\uff0c\u5bf9\u5f53\u524d\u6c34\u5370\u6280\u672f\u5b89\u5168\u6027\u63d0\u51fa\u66f4\u5927\u6311\u6218\u3002"}}
{"id": "2506.06020", "pdf": "https://arxiv.org/pdf/2506.06020", "abs": "https://arxiv.org/abs/2506.06020", "authors": ["Zeqi Zhou", "Fang Wu", "Shayan Talaei", "Haokai Zhao", "Cheng Meixin", "Tinson Xu", "Amin Saberi", "Yejin Choi"], "title": "When to Trust Context: Self-Reflective Debates for Context Reliability", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models frequently encounter conflicts between their parametric\nknowledge and contextual input, often resulting in factual inconsistencies or\nhallucinations. We propose Self-Reflective Debate for Contextual Reliability\n(SR-DCR), a lightweight framework that integrates token-level self-confidence\nwith an asymmetric multi-agent debate to adjudicate such conflicts. A critic,\ndeprived of context, challenges a defender who argues from the given passage; a\njudge model evaluates the debate and determines the context's reliability. The\nfinal answer is selected by combining the verdict with model confidence.\nExperiments on the ClashEval benchmark demonstrate that SR-DCR consistently\nenhances robustness to misleading context while maintaining accuracy on\ntrustworthy inputs, outperforming both classical debate and confidence-only\nbaselines with minimal computational overhead. The code is available at\nhttps://github.com/smiles724/Self-Reflective-Debates.", "AI": {"tldr": "SR-DCR\u6846\u67b6\u901a\u8fc7\u81ea\u53cd\u601d\u8fa9\u8bba\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5bf9\u4e0a\u4e0b\u6587\u51b2\u7a81\u7684\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u8fa9\u8bba\u548c\u4ec5\u4f9d\u8d56\u7f6e\u4fe1\u5ea6\u7684\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u53c2\u6570\u77e5\u8bc6\u4e0e\u4e0a\u4e0b\u6587\u8f93\u5165\u51b2\u7a81\u65f6\u7684\u4e0d\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u4ee4\u724c\u7ea7\u81ea\u7f6e\u4fe1\u5ea6\u548c\u975e\u5bf9\u79f0\u591a\u4ee3\u7406\u8fa9\u8bba\uff0c\u901a\u8fc7\u6279\u8bc4\u8005\u3001\u8fa9\u62a4\u8005\u548c\u6cd5\u5b98\u6a21\u578b\u8bc4\u4f30\u4e0a\u4e0b\u6587\u53ef\u9760\u6027\u3002", "result": "\u5728ClashEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSR-DCR\u663e\u8457\u63d0\u5347\u5bf9\u8bef\u5bfc\u6027\u4e0a\u4e0b\u6587\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "SR-DCR\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u53ef\u9760\u6027\u3002"}}
{"id": "2506.06028", "pdf": "https://arxiv.org/pdf/2506.06028", "abs": "https://arxiv.org/abs/2506.06028", "authors": ["Nikunj Shah", "Utsav Dey", "Kenji Nishimiya"], "title": "End-to-End Framework for Robot Lawnmower Coverage Path Planning using Cellular Decomposition", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, ICRA 2025, Workshop on Field Robotics", "summary": "Efficient Coverage Path Planning (CPP) is necessary for autonomous robotic\nlawnmowers to effectively navigate and maintain lawns with diverse and\nirregular shapes. This paper introduces a comprehensive end-to-end pipeline for\nCPP, designed to convert user-defined boundaries on an aerial map into\noptimized coverage paths seamlessly. The pipeline includes user input\nextraction, coordinate transformation, area decomposition and path generation\nusing our novel AdaptiveDecompositionCPP algorithm, preview and customization\nthrough an interactive coverage path visualizer, and conversion to actionable\nGPS waypoints. The AdaptiveDecompositionCPP algorithm combines cellular\ndecomposition with an adaptive merging strategy to reduce non-mowing travel\nthereby enhancing operational efficiency. Experimental evaluations,\nencompassing both simulations and real-world lawnmower tests, demonstrate the\neffectiveness of the framework in coverage completeness and mowing efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u9ad8\u6548\u8986\u76d6\u8def\u5f84\u89c4\u5212\uff08CPP\uff09\u6d41\u7a0b\uff0c\u7528\u4e8e\u81ea\u4e3b\u5272\u8349\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u89e3\u7b97\u6cd5\u4f18\u5316\u8def\u5f84\uff0c\u51cf\u5c11\u975e\u5272\u8349\u884c\u7a0b\u3002", "motivation": "\u81ea\u4e3b\u5272\u8349\u673a\u5668\u4eba\u9700\u8981\u9ad8\u6548\u8986\u76d6\u8def\u5f84\u89c4\u5212\u4ee5\u5e94\u5bf9\u591a\u6837\u4e14\u4e0d\u89c4\u5219\u5f62\u72b6\u7684\u8349\u576a\u3002", "method": "\u63d0\u51faAdaptiveDecompositionCPP\u7b97\u6cd5\uff0c\u7ed3\u5408\u8702\u7a9d\u5206\u89e3\u548c\u81ea\u9002\u5e94\u5408\u5e76\u7b56\u7565\uff0c\u901a\u8fc7\u7528\u6237\u8f93\u5165\u63d0\u53d6\u3001\u5750\u6807\u8f6c\u6362\u3001\u533a\u57df\u5206\u89e3\u548c\u8def\u5f84\u751f\u6210\u7b49\u6b65\u9aa4\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u8986\u76d6\u5b8c\u6574\u6027\u548c\u5272\u8349\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7aef\u5230\u7aef\u6d41\u7a0b\u548c\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5272\u8349\u673a\u5668\u4eba\u7684\u64cd\u4f5c\u6548\u7387\u3002"}}
{"id": "2506.06035", "pdf": "https://arxiv.org/pdf/2506.06035", "abs": "https://arxiv.org/abs/2506.06035", "authors": ["Shiyi Zhang", "Dong Liang", "Hairong Zheng", "Yihang Zhou"], "title": "HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion", "categories": ["cs.CV", "cs.AI", "I.2"], "comment": "15 pages, 6 figures, 3 tabs", "summary": "Reconstructing visual information from brain activity bridges the gap between\nneuroscience and computer vision. Even though progress has been made in\ndecoding images from fMRI using generative models, a challenge remains in\naccurately recovering highly complex visual stimuli. This difficulty stems from\ntheir elemental density and diversity, sophisticated spatial structures, and\nmultifaceted semantic information.\n  To address these challenges, we propose HAVIR that contains two adapters: (1)\nThe AutoKL Adapter transforms fMRI voxels into a latent diffusion prior,\ncapturing topological structures; (2) The CLIP Adapter converts the voxels to\nCLIP text and image embeddings, containing semantic information. These\ncomplementary representations are fused by Versatile Diffusion to generate the\nfinal reconstructed image. To extract the most essential semantic information\nfrom complex scenarios, the CLIP Adapter is trained with text captions\ndescribing the visual stimuli and their corresponding semantic images\nsynthesized from these captions. The experimental results demonstrate that\nHAVIR effectively reconstructs both structural features and semantic\ninformation of visual stimuli even in complex scenarios, outperforming existing\nmodels.", "AI": {"tldr": "HAVIR\u901a\u8fc7\u7ed3\u5408AutoKL\u548cCLIP\u9002\u914d\u5668\uff0c\u5229\u7528\u6f5c\u5728\u6269\u6563\u548c\u8bed\u4e49\u5d4c\u5165\u6280\u672f\uff0c\u6210\u529f\u4ecefMRI\u6570\u636e\u4e2d\u91cd\u5efa\u590d\u6742\u89c6\u89c9\u523a\u6fc0\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u4ece\u5927\u8111\u6d3b\u52a8\u4e2d\u91cd\u5efa\u89c6\u89c9\u4fe1\u606f\u662f\u795e\u7ecf\u79d1\u5b66\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6865\u6881\uff0c\u4f46\u590d\u6742\u89c6\u89c9\u523a\u6fc0\u7684\u9ad8\u5bc6\u5ea6\u3001\u591a\u6837\u6027\u548c\u591a\u8bed\u4e49\u4fe1\u606f\u4f7f\u5176\u96be\u4ee5\u51c6\u786e\u6062\u590d\u3002", "method": "HAVIR\u5305\u542b\u4e24\u4e2a\u9002\u914d\u5668\uff1aAutoKL\u9002\u914d\u5668\u5c06fMRI\u4f53\u7d20\u8f6c\u6362\u4e3a\u6f5c\u5728\u6269\u6563\u5148\u9a8c\u4ee5\u6355\u6349\u62d3\u6251\u7ed3\u6784\uff1bCLIP\u9002\u914d\u5668\u5c06\u4f53\u7d20\u8f6c\u6362\u4e3aCLIP\u6587\u672c\u548c\u56fe\u50cf\u5d4c\u5165\u4ee5\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\uff0c\u6700\u7ec8\u901a\u8fc7Versatile Diffusion\u878d\u5408\u751f\u6210\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHAVIR\u5728\u590d\u6742\u573a\u666f\u4e2d\u80fd\u6709\u6548\u91cd\u5efa\u89c6\u89c9\u523a\u6fc0\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "HAVIR\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ecefMRI\u6570\u636e\u4e2d\u91cd\u5efa\u590d\u6742\u89c6\u89c9\u523a\u6fc0\u7684\u80fd\u529b\u3002"}}
{"id": "2506.06048", "pdf": "https://arxiv.org/pdf/2506.06048", "abs": "https://arxiv.org/abs/2506.06048", "authors": ["Haripriya Harikumar", "Santu Rana"], "title": "TRUST: Test-time Resource Utilization for Superior Trustworthiness", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Standard uncertainty estimation techniques, such as dropout, often struggle\nto clearly distinguish reliable predictions from unreliable ones. We attribute\nthis limitation to noisy classifier weights, which, while not impairing overall\nclass-level predictions, render finer-level statistics less informative. To\naddress this, we propose a novel test-time optimization method that accounts\nfor the impact of such noise to produce more reliable confidence estimates.\nThis score defines a monotonic subset-selection function, where population\naccuracy consistently increases as samples with lower scores are removed, and\nit demonstrates superior performance in standard risk-based metrics such as\nAUSE and AURC. Additionally, our method effectively identifies discrepancies\nbetween training and test distributions, reliably differentiates\nin-distribution from out-of-distribution samples, and elucidates key\ndifferences between CNN and ViT classifiers across various vision datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u5206\u7c7b\u5668\u6743\u91cd\u566a\u58f0\uff0c\u63d0\u5347\u9884\u6d4b\u53ef\u9760\u6027\u548c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u4f20\u7edf\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff08\u5982dropout\uff09\u96be\u4ee5\u6e05\u6670\u533a\u5206\u53ef\u9760\u4e0e\u4e0d\u53ef\u9760\u9884\u6d4b\uff0c\u4e3b\u8981\u56e0\u5206\u7c7b\u5668\u6743\u91cd\u566a\u58f0\u5f71\u54cd\u7ec6\u7c92\u5ea6\u7edf\u8ba1\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6d4b\u8bd5\u65f6\u4f18\u5316\u65b9\u6cd5\uff0c\u8003\u8651\u566a\u58f0\u5f71\u54cd\uff0c\u751f\u6210\u66f4\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u5e76\u5b9a\u4e49\u5355\u8c03\u5b50\u96c6\u9009\u62e9\u51fd\u6570\u3002", "result": "\u5728AUSE\u548cAURC\u7b49\u98ce\u9669\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u5206\u5e03\u5dee\u5f02\uff0c\u533a\u5206\u5206\u5e03\u5185\u5916\u6837\u672c\uff0c\u5e76\u63ed\u793aCNN\u4e0eViT\u5206\u7c7b\u5668\u7684\u5173\u952e\u5dee\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u89c9\u6570\u636e\u96c6\uff0c\u4e3a\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.06054", "pdf": "https://arxiv.org/pdf/2506.06054", "abs": "https://arxiv.org/abs/2506.06054", "authors": ["Minglang Chen", "Jie He", "Caixu Xu", "Bocheng Liang", "Shengli Li", "Guannan He", "Xiongjie Tao"], "title": "FPDANet: A Multi-Section Classification Model for Intelligent Screening of Fetal Ultrasound", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "ResNet has been widely used in image classification tasks due to its ability\nto model the residual dependence of constant mappings for linear computation.\nHowever, the ResNet method adopts a unidirectional transfer of features and\nlacks an effective method to correlate contextual information, which is not\neffective in classifying fetal ultrasound images in the classification task,\nand fetal ultrasound images have problems such as low contrast, high\nsimilarity, and high noise. Therefore, we propose a bilateral multi-scale\ninformation fusion network-based FPDANet to address the above challenges.\nSpecifically, we design the positional attention mechanism (DAN) module, which\nutilizes the similarity of features to establish the dependency of different\nspatial positional features and enhance the feature representation. In\naddition, we design a bilateral multi-scale (FPAN) information fusion module to\ncapture contextual and global feature dependencies at different feature scales,\nthereby further improving the model representation. FPDANet classification\nresults obtained 91.05\\% and 100\\% in Top-1 and Top-5 metrics, respectively,\nand the experimental results proved the effectiveness and robustness of\nFPDANet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u8fb9\u591a\u5c3a\u5ea6\u4fe1\u606f\u878d\u5408\u7684\u7f51\u7edcFPDANet\uff0c\u7528\u4e8e\u89e3\u51b3\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u4f4e\u5bf9\u6bd4\u5ea6\u3001\u9ad8\u76f8\u4f3c\u6027\u548c\u9ad8\u566a\u58f0\u95ee\u9898\u3002", "motivation": "ResNet\u5728\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u5176\u5355\u5411\u7279\u5f81\u4f20\u9012\u548c\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u4fe1\u606f\u5173\u8054\u3002", "method": "\u8bbe\u8ba1\u4e86\u4f4d\u7f6e\u6ce8\u610f\u529b\u673a\u5236\uff08DAN\uff09\u6a21\u5757\u548c\u53cc\u8fb9\u591a\u5c3a\u5ea6\uff08FPAN\uff09\u4fe1\u606f\u878d\u5408\u6a21\u5757\uff0c\u4ee5\u589e\u5f3a\u7279\u5f81\u8868\u793a\u548c\u6355\u83b7\u5168\u5c40\u4f9d\u8d56\u3002", "result": "FPDANet\u5728Top-1\u548cTop-5\u6307\u6807\u4e0a\u5206\u522b\u8fbe\u523091.05%\u548c100%\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660eFPDANet\u5728\u80ce\u513f\u8d85\u58f0\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.06057", "pdf": "https://arxiv.org/pdf/2506.06057", "abs": "https://arxiv.org/abs/2506.06057", "authors": ["Chen Xiong", "Zihao Wang", "Rui Zhu", "Tsung-Yi Ho", "Pin-Yu Chen", "Jingwei Xiong", "Haixu Tang", "Lucila Ohno-Machado"], "title": "Hey, That's My Data! Label-Only Dataset Inference in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing\nby excelling at interpreting, reasoning about, and generating human language.\nHowever, their reliance on large-scale, often proprietary datasets poses a\ncritical challenge: unauthorized usage of such data can lead to copyright\ninfringement and significant financial harm. Existing dataset-inference methods\ntypically depend on log probabilities to detect suspicious training material,\nyet many leading LLMs have begun withholding or obfuscating these signals. This\nreality underscores the pressing need for label-only approaches capable of\nidentifying dataset membership without relying on internal model logits.\n  We address this gap by introducing CatShift, a label-only dataset-inference\nframework that capitalizes on catastrophic forgetting: the tendency of an LLM\nto overwrite previously learned knowledge when exposed to new data. If a\nsuspicious dataset was previously seen by the model, fine-tuning on a portion\nof it triggers a pronounced post-tuning shift in the model's outputs;\nconversely, truly novel data elicits more modest changes. By comparing the\nmodel's output shifts for a suspicious dataset against those for a known\nnon-member validation set, we statistically determine whether the suspicious\nset is likely to have been part of the model's original training corpus.\nExtensive experiments on both open-source and API-based LLMs validate\nCatShift's effectiveness in logit-inaccessible settings, offering a robust and\npractical solution for safeguarding proprietary data.", "AI": {"tldr": "CatShift\u662f\u4e00\u79cd\u4ec5\u4f9d\u8d56\u6807\u7b7e\u7684\u6570\u636e\u96c6\u63a8\u65ad\u6846\u67b6\uff0c\u5229\u7528\u707e\u96be\u6027\u9057\u5fd8\u73b0\u8c61\u68c0\u6d4b\u6a21\u578b\u662f\u5426\u4f7f\u7528\u8fc7\u53ef\u7591\u6570\u636e\u96c6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u4f46\u672a\u7ecf\u6388\u6743\u4f7f\u7528\u53ef\u80fd\u5bfc\u81f4\u7248\u6743\u4fb5\u6743\u548c\u7ecf\u6d4e\u635f\u5931\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u4fe1\u53f7\uff0c\u4f46\u8bb8\u591aLLMs\u5df2\u9690\u85cf\u8fd9\u4e9b\u4fe1\u53f7\uff0c\u4e9f\u9700\u4e0d\u4f9d\u8d56\u5185\u90e8\u4fe1\u606f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "CatShift\u901a\u8fc7\u5fae\u8c03\u53ef\u7591\u6570\u636e\u96c6\u7684\u4e00\u90e8\u5206\uff0c\u89c2\u5bdf\u6a21\u578b\u8f93\u51fa\u7684\u53d8\u5316\uff08\u707e\u96be\u6027\u9057\u5fd8\u73b0\u8c61\uff09\uff0c\u5e76\u4e0e\u5df2\u77e5\u975e\u6210\u5458\u9a8c\u8bc1\u96c6\u7684\u8f93\u51fa\u53d8\u5316\u5bf9\u6bd4\uff0c\u7edf\u8ba1\u63a8\u65ad\u53ef\u7591\u6570\u636e\u96c6\u662f\u5426\u66fe\u88ab\u7528\u4e8e\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCatShift\u5728\u65e0\u6cd5\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6709\u6548\u68c0\u6d4b\u6570\u636e\u96c6\u6210\u5458\u8d44\u683c\u3002", "conclusion": "CatShift\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4fdd\u62a4\u4e13\u6709\u6570\u636e\u514d\u53d7\u672a\u7ecf\u6388\u6743\u7684\u4f7f\u7528\u3002"}}
{"id": "2506.06058", "pdf": "https://arxiv.org/pdf/2506.06058", "abs": "https://arxiv.org/abs/2506.06058", "authors": ["Viorica Chifu", "Cristina Bianca Pop", "Tudor Cioara", "Ionut Anghel"], "title": "Microgrids Coalitions for Energy Market Balancing", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "With the integration of renewable sources in electricity distribution\nnetworks, the need to develop intelligent mechanisms for balancing the energy\nmarket has arisen. In the absence of such mechanisms, the energy market may\nface imbalances that can lead to power outages, financial losses or instability\nat the grid level. In this context, the grouping of microgrids into optimal\ncoalitions that can absorb energy from the market during periods of surplus or\nsupply energy to the market during periods of is a key aspect in the efficient\nmanagement of distribution networks. In this article, we propose a method that\nidentify an optimal microgrids coalition capable of addressing the dynamics of\nthe energy market. The proposed method models the problem of identifying the\noptimal coalition as an optimization problem that it solves by combining a\nstrategy inspired by cooperative game theory with a memetic algorithm. An\nindividual is represented as a coalition of microgrids and the evolution of\npopulation of individuals over generations is assured by recombination and\nmutation. The fitness function is defined as the difference between the total\nvalue generated by the coalition and a penalty applied to the coalition when\nthe energy traded by coalition exceeds the energy available/demanded on/by the\nenergy market. The value generated by the coalition is calculated based on the\nprofit obtained by the collation if it sells energy on the market during\nperiods of deficit or the savings obtained by the coalition if it buys energy\non the market during periods of surplus and the costs associated with the\ntrading process. This value is divided equitably among the coalition members,\naccording to the Shapley value, which considers the contribution of each one to\nthe formation of collective value.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5408\u4f5c\u535a\u5f08\u8bba\u548c\u6a21\u56e0\u7b97\u6cd5\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u5fae\u7535\u7f51\u6700\u4f18\u8054\u76df\u4ee5\u5e73\u8861\u80fd\u6e90\u5e02\u573a\u52a8\u6001\u3002", "motivation": "\u968f\u7740\u53ef\u518d\u751f\u80fd\u6e90\u5e76\u5165\u7535\u7f51\uff0c\u9700\u667a\u80fd\u673a\u5236\u5e73\u8861\u80fd\u6e90\u5e02\u573a\uff0c\u907f\u514d\u5931\u8861\u5bfc\u81f4\u505c\u7535\u6216\u7ecf\u6d4e\u635f\u5931\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u5408\u4f5c\u535a\u5f08\u8bba\u548c\u6a21\u56e0\u7b97\u6cd5\uff0c\u901a\u8fc7\u91cd\u7ec4\u548c\u7a81\u53d8\u4f18\u5316\u5fae\u7535\u7f51\u8054\u76df\u3002", "result": "\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u6700\u4f18\u8054\u76df\uff0c\u5e73\u8861\u5e02\u573a\u4f9b\u9700\uff0c\u5e76\u901a\u8fc7Shapley\u503c\u516c\u5e73\u5206\u914d\u6536\u76ca\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7535\u7f51\u7ba1\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u5e02\u573a\u7a33\u5b9a\u548c\u6536\u76ca\u516c\u5e73\u5206\u914d\u3002"}}
{"id": "2506.06060", "pdf": "https://arxiv.org/pdf/2506.06060", "abs": "https://arxiv.org/abs/2506.06060", "authors": ["Yingqi Hu", "Zhuo Zhang", "Jingyuan Zhang", "Lizhen Qu", "Zenglin Xu"], "title": "Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Federated fine-tuning of large language models (FedLLMs) presents a promising\napproach for achieving strong model performance while preserving data privacy\nin sensitive domains. However, the inherent memorization ability of LLMs makes\nthem vulnerable to training data extraction attacks. To investigate this risk,\nwe introduce simple yet effective extraction attack algorithms specifically\ndesigned for FedLLMs. In contrast to prior \"verbatim\" extraction attacks, which\nassume access to fragments from all training data, our approach operates under\na more realistic threat model, where the attacker only has access to a single\nclient's data and aims to extract previously unseen personally identifiable\ninformation (PII) from other clients. This requires leveraging contextual\nprefixes held by the attacker to generalize across clients. To evaluate the\neffectiveness of our approaches, we propose two rigorous metrics-coverage rate\nand efficiency-and extend a real-world legal dataset with PII annotations\naligned with CPIS, GDPR, and CCPA standards, achieving 89.9% human-verified\nprecision. Experimental results show that our method can extract up to 56.57%\nof victim-exclusive PII, with \"Address,\" \"Birthday,\" and \"Name\" being the most\nvulnerable categories. Our findings underscore the pressing need for robust\ndefense strategies and contribute a new benchmark and evaluation framework for\nfuture research in privacy-preserving federated learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08FedLLMs\uff09\u8fdb\u884c\u5fae\u8c03\u65f6\u7684\u6570\u636e\u9690\u79c1\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9FedLLMs\u7684\u63d0\u53d6\u653b\u51fb\u7b97\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bb0\u5fc6\u80fd\u529b\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u653b\u51fb\uff0c\u5c24\u5176\u662f\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63d0\u53d6\u653b\u51fb\u7b97\u6cd5\uff0c\u653b\u51fb\u8005\u4ec5\u9700\u8bbf\u95ee\u5355\u4e2a\u5ba2\u6237\u7aef\u7684\u6570\u636e\uff0c\u5e76\u5229\u7528\u4e0a\u4e0b\u6587\u524d\u7f00\u6765\u63d0\u53d6\u5176\u4ed6\u5ba2\u6237\u7aef\u7684\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\uff08PII\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u53d6\u9ad8\u8fbe56.57%\u7684\u53d7\u5bb3\u8005\u4e13\u5c5ePII\uff0c\u5176\u4e2d\u201c\u5730\u5740\u201d\u3001\u201c\u751f\u65e5\u201d\u548c\u201c\u59d3\u540d\u201d\u6700\u6613\u53d7\u653b\u51fb\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5f00\u53d1\u9c81\u68d2\u9632\u5fa1\u7b56\u7565\u7684\u7d27\u8feb\u6027\uff0c\u5e76\u4e3a\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2506.06105", "pdf": "https://arxiv.org/pdf/2506.06105", "abs": "https://arxiv.org/abs/2506.06105", "authors": ["Rujikorn Charakorn", "Edoardo Cetin", "Yujin Tang", "Robert Tjarko Lange"], "title": "Text-to-LoRA: Instant Transformer Adaption", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at ICML 2025", "summary": "While Foundation Models provide a general tool for rapid content creation,\nthey regularly require task-specific adaptation. Traditionally, this exercise\ninvolves careful curation of datasets and repeated fine-tuning of the\nunderlying model. Fine-tuning techniques enable practitioners to adapt\nfoundation models for many new applications but require expensive and lengthy\ntraining while being notably sensitive to hyper-parameter choices. To overcome\nthese limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting\nLarge Language Models on the fly solely based on a natural language description\nof the target task. T2L is a hypernetwork trained to construct LoRAs in a\nsingle inexpensive forward pass. After training T2L on a suite of 9 pre-trained\nLoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA\ninstances match the performance of task-specific adapters across the\ncorresponding test sets. Furthermore, T2L can compress hundreds of LoRA\ninstances and zero-shot generalize to entirely unseen tasks. This approach\nprovides a significant step towards democratizing the specialization of\nfoundation models and enables language-based adaptation with minimal compute\nrequirements. Our code is available at https://github.com/SakanaAI/text-to-lora", "AI": {"tldr": "T2L\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u52a8\u6001\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8d85\u7f51\u7edc\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u751f\u6210LoRA\u9002\u914d\u5668\uff0c\u6027\u80fd\u63a5\u8fd1\u4efb\u52a1\u4e13\u7528\u9002\u914d\u5668\uff0c\u5e76\u80fd\u538b\u7f29\u548c\u96f6\u6837\u672c\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e14\u5bf9\u8d85\u53c2\u6570\u654f\u611f\uff0c\u9650\u5236\u4e86\u57fa\u7840\u6a21\u578b\u7684\u5feb\u901f\u9002\u5e94\u3002", "method": "\u8bad\u7ec3T2L\u8d85\u7f51\u7edc\uff0c\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210LoRA\u9002\u914d\u5668\uff0c\u65e0\u9700\u91cd\u590d\u5fae\u8c03\u3002", "result": "T2L\u751f\u6210\u7684LoRA\u5728\u6d4b\u8bd5\u96c6\u4e0a\u6027\u80fd\u63a5\u8fd1\u4efb\u52a1\u4e13\u7528\u9002\u914d\u5668\uff0c\u5e76\u80fd\u96f6\u6837\u672c\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "conclusion": "T2L\u4e3a\u5feb\u901f\u3001\u4f4e\u6210\u672c\u9002\u5e94\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u63a8\u52a8\u4e86\u6a21\u578b\u6c11\u4e3b\u5316\u3002"}}
{"id": "2506.06112", "pdf": "https://arxiv.org/pdf/2506.06112", "abs": "https://arxiv.org/abs/2506.06112", "authors": ["Cheng-Long Wang", "Qi Li", "Zihang Xiang", "Yinzhi Cao", "Di Wang"], "title": "Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Unlearning Completeness", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "To appear in the Proceedings of USENIX Security Symposium, 2025", "summary": "Growing concerns over data privacy and security highlight the importance of\nmachine unlearning--removing specific data influences from trained models\nwithout full retraining. Techniques like Membership Inference Attacks (MIAs)\nare widely used to externally assess successful unlearning. However, existing\nmethods face two key limitations: (1) maximizing MIA effectiveness (e.g., via\nonline attacks) requires prohibitive computational resources, often exceeding\nretraining costs; (2) MIAs, designed for binary inclusion tests, struggle to\ncapture granular changes in approximate unlearning. To address these\nchallenges, we propose the Interpolated Approximate Measurement (IAM), a\nframework natively designed for unlearning inference. IAM quantifies\nsample-level unlearning completeness by interpolating the model's\ngeneralization-fitting behavior gap on queried samples. IAM achieves strong\nperformance in binary inclusion tests for exact unlearning and high correlation\nfor approximate unlearning--scalable to LLMs using just one pre-trained shadow\nmodel. We theoretically analyze how IAM's scoring mechanism maintains\nperformance efficiently. We then apply IAM to recent approximate unlearning\nalgorithms, revealing general risks of both over-unlearning and\nunder-unlearning, underscoring the need for stronger safeguards in approximate\nunlearning systems. The code is available at\nhttps://github.com/Happy2Git/Unlearning_Inference_IAM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIAM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u6570\u636e\u9057\u5fd8\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u8d44\u6e90\u548c\u7c92\u5ea6\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6570\u636e\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u6709\u6548\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u6570\u636e\u9057\u5fd8\u6548\u679c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982MIA\uff09\u5728\u8ba1\u7b97\u8d44\u6e90\u548c\u7c92\u5ea6\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faIAM\u6846\u67b6\uff0c\u901a\u8fc7\u63d2\u503c\u6a21\u578b\u7684\u6cdb\u5316-\u62df\u5408\u884c\u4e3a\u5dee\u8ddd\u6765\u91cf\u5316\u6837\u672c\u7ea7\u9057\u5fd8\u6548\u679c\uff0c\u9002\u7528\u4e8e\u7cbe\u786e\u548c\u8fd1\u4f3c\u9057\u5fd8\u573a\u666f\u3002", "result": "IAM\u5728\u7cbe\u786e\u9057\u5fd8\u7684\u4e8c\u5143\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u8fd1\u4f3c\u9057\u5fd8\u4e2d\u76f8\u5173\u6027\u9ad8\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4f4e\uff0c\u9002\u7528\u4e8e\u5927\u578b\u6a21\u578b\u3002", "conclusion": "IAM\u4e3a\u6570\u636e\u9057\u5fd8\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63ed\u793a\u4e86\u8fd1\u4f3c\u9057\u5fd8\u7b97\u6cd5\u4e2d\u8fc7\u5ea6\u6216\u4e0d\u8db3\u9057\u5fd8\u7684\u98ce\u9669\uff0c\u5f3a\u8c03\u4e86\u66f4\u5f3a\u7684\u5b89\u5168\u4fdd\u969c\u9700\u6c42\u3002"}}
{"id": "2506.06117", "pdf": "https://arxiv.org/pdf/2506.06117", "abs": "https://arxiv.org/abs/2506.06117", "authors": ["Christophe Van Gysel", "Maggie Wu", "Lyan Verwimp", "Caglar Tirkaz", "Marco Bertola", "Zhihong Lei", "Youssef Oualil"], "title": "Phonetically-Augmented Discriminative Rescoring for Voice Search Error Correction", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "To appear at Interspeech '25", "summary": "End-to-end (E2E) Automatic Speech Recognition (ASR) models are trained using\npaired audio-text samples that are expensive to obtain, since high-quality\nground-truth data requires human annotators. Voice search applications, such as\ndigital media players, leverage ASR to allow users to search by voice as\nopposed to an on-screen keyboard. However, recent or infrequent movie titles\nmay not be sufficiently represented in the E2E ASR system's training data, and\nhence, may suffer poor recognition.\n  In this paper, we propose a phonetic correction system that consists of (a) a\nphonetic search based on the ASR model's output that generates phonetic\nalternatives that may not be considered by the E2E system, and (b) a rescorer\ncomponent that combines the ASR model recognition and the phonetic\nalternatives, and select a final system output.\n  We find that our approach improves word error rate between 4.4 and 7.6%\nrelative on benchmarks of popular movie titles over a series of competitive\nbaselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u97f3\u7ea0\u6b63\u7cfb\u7edf\uff0c\u901a\u8fc7\u751f\u6210\u8bed\u97f3\u66ff\u4ee3\u65b9\u6848\u5e76\u7ed3\u5408\u91cd\u8bc4\u5206\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u8bc6\u522b\u5728\u7535\u5f71\u6807\u9898\u4e0a\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u7aef\u5230\u7aef\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u65b0\u7535\u5f71\u6807\u9898\u53ef\u80fd\u7f3a\u4e4f\u8db3\u591f\u8bad\u7ec3\u6570\u636e\uff0c\u5bfc\u81f4\u8bc6\u522b\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7cfb\u7edf\u5305\u62ec\u8bed\u97f3\u641c\u7d22\u751f\u6210\u66ff\u4ee3\u65b9\u6848\u548c\u91cd\u8bc4\u5206\u7ec4\u4ef6\u7ed3\u5408\u9009\u62e9\u6700\u7ec8\u8f93\u51fa\u3002", "result": "\u5728\u7535\u5f71\u6807\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8bcd\u9519\u8bef\u7387\u76f8\u5bf9\u964d\u4f4e\u4e864.4%\u81f37.6%\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bed\u97f3\u7ea0\u6b63\u7cfb\u7edf\u6709\u6548\u63d0\u5347\u4e86\u8bed\u97f3\u8bc6\u522b\u5728\u7279\u5b9a\u9886\u57df\u7684\u6027\u80fd\u3002"}}
{"id": "2506.06151", "pdf": "https://arxiv.org/pdf/2506.06151", "abs": "https://arxiv.org/abs/2506.06151", "authors": ["Haowei Wang", "Rupeng Zhang", "Junjie Wang", "Mingyang Li", "Yuekai Huang", "Dandan Wang", "Qing Wang"], "title": "Joint-GCG: Unified Gradient-Based Poisoning Attacks on Retrieval-Augmented Generation Systems", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by retrieving relevant documents from external corpora before generating\nresponses. This approach significantly expands LLM capabilities by leveraging\nvast, up-to-date external knowledge. However, this reliance on external\nknowledge makes RAG systems vulnerable to corpus poisoning attacks that\nmanipulate generated outputs via poisoned document injection. Existing\npoisoning attack strategies typically treat the retrieval and generation stages\nas disjointed, limiting their effectiveness. We propose Joint-GCG, the first\nframework to unify gradient-based attacks across both retriever and generator\nmodels through three innovations: (1) Cross-Vocabulary Projection for aligning\nembedding spaces, (2) Gradient Tokenization Alignment for synchronizing\ntoken-level gradient signals, and (3) Adaptive Weighted Fusion for dynamically\nbalancing attacking objectives. Evaluations demonstrate that Joint-GCG achieves\nat most 25% and an average of 5% higher attack success rate than previous\nmethods across multiple retrievers and generators. While optimized under a\nwhite-box assumption, the generated poisons show unprecedented transferability\nto unseen models. Joint-GCG's innovative unification of gradient-based attacks\nacross retrieval and generation stages fundamentally reshapes our understanding\nof vulnerabilities within RAG systems. Our code is available at\nhttps://github.com/NicerWang/Joint-GCG.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faJoint-GCG\u6846\u67b6\uff0c\u9996\u6b21\u7edf\u4e00\u68c0\u7d22\u548c\u751f\u6210\u9636\u6bb5\u7684\u68af\u5ea6\u653b\u51fb\uff0c\u663e\u8457\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "RAG\u7cfb\u7edf\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\uff0c\u6613\u53d7\u6587\u6863\u6ce8\u5165\u653b\u51fb\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u521b\u65b0\uff1a\u8de8\u8bcd\u6c47\u6295\u5f71\u3001\u68af\u5ea6\u6807\u8bb0\u5bf9\u9f50\u548c\u81ea\u9002\u5e94\u52a0\u6743\u878d\u5408\u3002", "result": "Joint-GCG\u653b\u51fb\u6210\u529f\u7387\u5e73\u5747\u63d0\u9ad85%\uff0c\u6700\u9ad825%\uff0c\u4e14\u5177\u6709\u5f3a\u8fc1\u79fb\u6027\u3002", "conclusion": "Joint-GCG\u91cd\u65b0\u5b9a\u4e49\u4e86RAG\u7cfb\u7edf\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u9632\u5fa1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.06162", "pdf": "https://arxiv.org/pdf/2506.06162", "abs": "https://arxiv.org/abs/2506.06162", "authors": ["Zackary Okun Dunivin", "Paul E. Smaldino"], "title": "Recommender systems, stigmergy, and the tyranny of popularity", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.IR"], "comment": null, "summary": "Scientific recommender systems, such as Google Scholar and Web of Science,\nare essential tools for discovery. Search algorithms that power work through\nstigmergy, a collective intelligence mechanism that surfaces useful paths\nthrough repeated engagement. While generally effective, this\n``rich-get-richer'' dynamic results in a small number of high-profile papers\nthat dominate visibility. This essay argues argue that these algorithm\nover-reliance on popularity fosters intellectual homogeneity and exacerbates\nstructural inequities, stifling innovative and diverse perspectives critical\nfor scientific progress. We propose an overhaul of search platforms to\nincorporate user-specific calibration, allowing researchers to manually adjust\nthe weights of factors like popularity, recency, and relevance. We also advise\nplatform developers on how word embeddings and LLMs could be implemented in\nways that increase user autonomy. While our suggestions are particularly\npertinent to aligning recommender systems with scientific values, these ideas\nare broadly applicable to information access systems in general. Designing\nplatforms that increase user autonomy is an important step toward more robust\nand dynamic information", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u79d1\u5b66\u63a8\u8350\u7cfb\u7edf\uff08\u5982Google Scholar\u548cWeb of Science\uff09\u8fc7\u5ea6\u4f9d\u8d56\u6d41\u884c\u5ea6\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u7528\u6237\u81ea\u5b9a\u4e49\u6821\u51c6\u548c\u5f15\u5165\u65b0\u6280\u672f\uff08\u5982\u8bcd\u5d4c\u5165\u548cLLMs\uff09\u6765\u63d0\u5347\u7528\u6237\u81ea\u4e3b\u6027\uff0c\u4ee5\u4fc3\u8fdb\u79d1\u5b66\u591a\u6837\u6027\u3002", "motivation": "\u5f53\u524d\u79d1\u5b66\u63a8\u8350\u7cfb\u7edf\u901a\u8fc7\u96c6\u4f53\u667a\u80fd\u673a\u5236\uff08\u5982stigmergy\uff09\u63a8\u8350\u9ad8\u6d41\u884c\u5ea6\u8bba\u6587\uff0c\u5bfc\u81f4\u5c11\u6570\u9ad8\u5f71\u54cd\u529b\u8bba\u6587\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u6291\u5236\u4e86\u79d1\u5b66\u591a\u6837\u6027\u548c\u521b\u65b0\u3002", "method": "\u63d0\u51fa\u5bf9\u641c\u7d22\u5e73\u53f0\u8fdb\u884c\u6539\u9769\uff0c\u5f15\u5165\u7528\u6237\u81ea\u5b9a\u4e49\u6821\u51c6\u529f\u80fd\uff0c\u5141\u8bb8\u8c03\u6574\u6d41\u884c\u5ea6\u3001\u65f6\u6548\u6027\u548c\u76f8\u5173\u6027\u7b49\u6743\u91cd\uff0c\u5e76\u5efa\u8bae\u5229\u7528\u8bcd\u5d4c\u5165\u548cLLMs\u6280\u672f\u589e\u5f3a\u7528\u6237\u81ea\u4e3b\u6027\u3002", "result": "\u901a\u8fc7\u7528\u6237\u81ea\u4e3b\u8c03\u6574\u63a8\u8350\u6743\u91cd\u548c\u5f15\u5165\u65b0\u6280\u672f\uff0c\u53ef\u4ee5\u7f13\u89e3\u6d41\u884c\u5ea6\u4e3b\u5bfc\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u79d1\u5b66\u591a\u6837\u6027\u548c\u521b\u65b0\u3002", "conclusion": "\u8bbe\u8ba1\u589e\u5f3a\u7528\u6237\u81ea\u4e3b\u6027\u7684\u63a8\u8350\u7cfb\u7edf\u662f\u63a8\u52a8\u79d1\u5b66\u8fdb\u6b65\u7684\u5173\u952e\uff0c\u8fd9\u4e00\u601d\u8def\u4e5f\u9002\u7528\u4e8e\u5176\u4ed6\u4fe1\u606f\u8bbf\u95ee\u7cfb\u7edf\u3002"}}
{"id": "2506.06165", "pdf": "https://arxiv.org/pdf/2506.06165", "abs": "https://arxiv.org/abs/2506.06165", "authors": ["Eunhye Grace Ko", "Soo Hyoung Joo"], "title": "(AI peers) are people learning from the same standpoint: Perception of AI characters in a Collaborative Science Investigation", "categories": ["cs.HC", "cs.AI"], "comment": "14 pages", "summary": "While the complexity of 21st-century demands has promoted pedagogical\napproaches to foster complex competencies, a persistent gap remains between\nin-class learning activities and individualized learning or assessment\npractices. To address this, studies have explored the use of AI-generated\ncharacters in learning and assessment. One attempt is scenario-based assessment\n(SBA), a technique that not only measures but also fosters the development of\ncompetencies throughout the assessment process. SBA introduces simulated agents\nto provide an authentic social-interactional context, allowing for the\nassessment of competency-based constructs while mitigating the unpredictability\nof real-life interactions. Recent advancements in multimodal AI, such as\ntext-to-video technology, allow these agents to be enhanced into AI-generated\ncharacters. This mixed-method study investigates how learners perceive AI\ncharacters taking the role of mentor and teammates in an SBA mirroring the\ncontext of a collaborative science investigation. Specifically, we examined the\nLikert scale responses of 56 high schoolers regarding trust, social presence,\nand effectiveness. We analyzed the relationships between these factors and\ntheir impact on the intention to adopt AI characters through PLS-SEM. Our\nfindings indicated that learners' trust shaped their sense of social presence\nwith the AI characters, enhancing perceived effectiveness. Qualitative analysis\nfurther highlighted factors that foster trust, such as material credibility and\nalignment with learning goals, as well as the pivotal role of social presence\nin creating a collaborative context.\n  This paper was accepted as an full paper for AIED 2025.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u57fa\u4e8e\u60c5\u666f\u7684\u8bc4\u4f30\uff08SBA\uff09\u4e2d\u4f7f\u7528AI\u751f\u6210\u89d2\u8272\u5bf9\u5b66\u4e60\u8005\u4fe1\u4efb\u3001\u793e\u4f1a\u5b58\u5728\u611f\u548c\u6548\u679c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4fe1\u4efb\u589e\u5f3a\u793e\u4f1a\u5b58\u5728\u611f\uff0c\u8fdb\u800c\u63d0\u5347\u6548\u679c\u3002", "motivation": "21\u4e16\u7eaa\u9700\u6c42\u590d\u6742\u5316\u5bfc\u81f4\u6559\u5b66\u4e0e\u4e2a\u6027\u5316\u5b66\u4e60\u8bc4\u4f30\u8131\u8282\uff0c\u9700\u63a2\u7d22AI\u89d2\u8272\u5728\u5b66\u4e60\u548c\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7Likert\u91cf\u8868\u548cPLS-SEM\u5206\u679056\u540d\u9ad8\u4e2d\u751f\u5bf9AI\u89d2\u8272\u7684\u4fe1\u4efb\u3001\u793e\u4f1a\u5b58\u5728\u611f\u548c\u6548\u679c\u7684\u611f\u77e5\u3002", "result": "\u5b66\u4e60\u8005\u4fe1\u4efb\u589e\u5f3a\u793e\u4f1a\u5b58\u5728\u611f\uff0c\u8fdb\u800c\u63d0\u5347AI\u89d2\u8272\u7684\u6548\u679c\u611f\u77e5\uff1b\u6750\u6599\u53ef\u4fe1\u5ea6\u548c\u76ee\u6807\u4e00\u81f4\u6027\u662f\u5173\u952e\u4fe1\u4efb\u56e0\u7d20\u3002", "conclusion": "AI\u751f\u6210\u89d2\u8272\u5728SBA\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4fe1\u4efb\u548c\u793e\u4f1a\u5b58\u5728\u611f\u662f\u6210\u529f\u5e94\u7528\u7684\u5173\u952e\u3002"}}
{"id": "2506.06166", "pdf": "https://arxiv.org/pdf/2506.06166", "abs": "https://arxiv.org/abs/2506.06166", "authors": ["Tianyi Alex Qiu", "Zhonghao He", "Tejasveer Chugh", "Max Kleiman-Weiner"], "title": "The Lock-in Hypothesis: Stagnation by Algorithm", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.HC"], "comment": "ICML 2025, 46 pages", "summary": "The training and deployment of large language models (LLMs) create a feedback\nloop with human users: models learn human beliefs from data, reinforce these\nbeliefs with generated content, reabsorb the reinforced beliefs, and feed them\nback to users again and again. This dynamic resembles an echo chamber. We\nhypothesize that this feedback loop entrenches the existing values and beliefs\nof users, leading to a loss of diversity and potentially the lock-in of false\nbeliefs. We formalize this hypothesis and test it empirically with agent-based\nLLM simulations and real-world GPT usage data. Analysis reveals sudden but\nsustained drops in diversity after the release of new GPT iterations,\nconsistent with the hypothesized human-AI feedback loop. Code and data\navailable at https://thelockinhypothesis.com", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u4eba\u7c7b\u7528\u6237\u7684\u53cd\u9988\u5faa\u73af\uff0c\u53d1\u73b0\u8fd9\u79cd\u5faa\u73af\u53ef\u80fd\u5bfc\u81f4\u591a\u6837\u6027\u4e27\u5931\u548c\u9519\u8bef\u4fe1\u5ff5\u7684\u56fa\u5316\u3002\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u5047\u8bbe\u3002", "motivation": "\u63a2\u8ba8LLMs\u4e0e\u4eba\u7c7b\u4e92\u52a8\u5f62\u6210\u7684\u53cd\u9988\u5faa\u73af\u5982\u4f55\u5f71\u54cd\u4fe1\u5ff5\u591a\u6837\u6027\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u4fe1\u5ff5\u7684\u56fa\u5316\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4ee3\u7406\u7684LLM\u6a21\u62df\u548c\u771f\u5b9eGPT\u4f7f\u7528\u6570\u636e\uff0c\u5f62\u5f0f\u5316\u5e76\u9a8c\u8bc1\u5047\u8bbe\u3002", "result": "\u5206\u6790\u663e\u793a\u65b0GPT\u7248\u672c\u53d1\u5e03\u540e\u591a\u6837\u6027\u7a81\u7136\u4e14\u6301\u7eed\u4e0b\u964d\uff0c\u652f\u6301\u53cd\u9988\u5faa\u73af\u5047\u8bbe\u3002", "conclusion": "\u4eba\u7c7b-AI\u53cd\u9988\u5faa\u73af\u53ef\u80fd\u52a0\u5267\u4fe1\u5ff5\u56fa\u5316\u548c\u591a\u6837\u6027\u4e27\u5931\uff0c\u9700\u5f15\u8d77\u5173\u6ce8\u3002"}}
{"id": "2506.06169", "pdf": "https://arxiv.org/pdf/2506.06169", "abs": "https://arxiv.org/abs/2506.06169", "authors": ["Jwalanthi Ranganathan", "Rohan Jha", "Kanishka Misra", "Kyle Mahowald"], "title": "semantic-features: A User-Friendly Tool for Studying Contextual Word Embeddings in Interpretable Semantic Spaces", "categories": ["cs.CL", "cs.AI"], "comment": "SCiL 2025 Camera Ready Extended Abstract", "summary": "We introduce semantic-features, an extensible, easy-to-use library based on\nChronis et al. (2023) for studying contextualized word embeddings of LMs by\nprojecting them into interpretable spaces. We apply this tool in an experiment\nwhere we measure the contextual effect of the choice of dative construction\n(prepositional or double object) on the semantic interpretation of utterances\n(Bresnan, 2007). Specifically, we test whether \"London\" in \"I sent London the\nletter.\" is more likely to be interpreted as an animate referent (e.g., as the\nname of a person) than in \"I sent the letter to London.\" To this end, we devise\na dataset of 450 sentence pairs, one in each dative construction, with\nrecipients being ambiguous with respect to person-hood vs. place-hood. By\napplying semantic-features, we show that the contextualized word embeddings of\nthree masked language models show the expected sensitivities. This leaves us\noptimistic about the usefulness of our tool.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3asemantic-features\u7684\u5e93\uff0c\u7528\u4e8e\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u8bcd\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u7279\u5b9a\u8bed\u6cd5\u7ed3\u6784\u4e2d\u7684\u8bed\u4e49\u654f\u611f\u6027\u3002", "motivation": "\u7814\u7a76\u4e0a\u4e0b\u6587\u8bcd\u5d4c\u5165\u7684\u8bed\u4e49\u89e3\u91ca\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u8bed\u6cd5\u7ed3\u6784\uff08\u5982\u4e0e\u683c\u7ed3\u6784\uff09\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528semantic-features\u5e93\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b450\u5bf9\u53e5\u5b50\u7684\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u8bed\u8a00\u6a21\u578b\u5bf9\u4e0e\u683c\u7ed3\u6784\u4e2d\u8bed\u4e49\u5dee\u5f02\u7684\u654f\u611f\u6027\u3002", "result": "\u4e09\u4e2a\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u8bcd\u5d4c\u5165\u8868\u73b0\u51fa\u9884\u671f\u7684\u8bed\u4e49\u654f\u611f\u6027\u3002", "conclusion": "semantic-features\u5e93\u5728\u5206\u6790\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u89e3\u91ca\u80fd\u529b\u65b9\u9762\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.06205", "pdf": "https://arxiv.org/pdf/2506.06205", "abs": "https://arxiv.org/abs/2506.06205", "authors": ["Sheng Chen", "Peiyu He", "Jiaxin Hu", "Ziyang Liu", "Yansheng Wang", "Tao Xu", "Chi Zhang", "Chongchong Zhang", "Chao An", "Shiyu Cai", "Duo Cao", "Kangping Chen", "Shuai Chu", "Tianwei Chu", "Mingdi Dan", "Min Du", "Weiwei Fang", "Pengyou Fu", "Junkai Hu", "Xiaowei Jiang", "Zhaodi Jiang", "Fuxuan Li", "Jun Li", "Minghui Li", "Mingyao Li", "Yanchang Li", "Zhibin Li", "Guangming Liu", "Kairui Liu", "Lihao Liu", "Weizhi Liu", "Xiaoshun Liu", "Yufei Liu", "Yunfei Liu", "Qiang Lu", "Yuanfei Luo", "Xiang Lv", "Hongying Ma", "Sai Ma", "Lingxian Mi", "Sha Sa", "Hongxiang Shu", "Lei Tian", "Chengzhi Wang", "Jiayu Wang", "Kaijie Wang", "Qingyi Wang", "Renwen Wang", "Tao Wang", "Wei Wang", "Xirui Wang", "Chao Wei", "Xuguang Wei", "Zijun Xia", "Zhaohao Xiao", "Tingshuai Yan", "Liyan Yang", "Yifan Yang", "Zhikai Yang", "Zhong Yin", "Li Yuan", "Liuchun Yuan", "Chi Zhang", "Jinyang Zhang", "Junhui Zhang", "Linge Zhang", "Zhenyi Zhang", "Zheyu Zhang", "Dongjie Zhu", "Hang Li", "Yangang Zhang"], "title": "Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal Learning", "categories": ["cs.RO", "cs.AI"], "comment": "Astra Technical Report", "summary": "Modern robot navigation systems encounter difficulties in diverse and complex\nindoor environments. Traditional approaches rely on multiple modules with small\nmodels or rule-based systems and thus lack adaptability to new environments. To\naddress this, we developed Astra, a comprehensive dual-model architecture,\nAstra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a\nmultimodal LLM, processes vision and language inputs to perform self and goal\nlocalization using a hybrid topological-semantic graph as the global map, and\noutperforms traditional visual place recognition methods. Astra-Local, a\nmultitask network, handles local path planning and odometry estimation. Its 4D\nspatial-temporal encoder, trained through self-supervised learning, generates\nrobust 4D features for downstream tasks. The planning head utilizes flow\nmatching and a novel masked ESDF loss to minimize collision risks for\ngenerating local trajectories, and the odometry head integrates multi-sensor\ninputs via a transformer encoder to predict the relative pose of the robot.\nDeployed on real in-house mobile robots, Astra achieves high end-to-end mission\nsuccess rate across diverse indoor environments.", "AI": {"tldr": "Astra\u662f\u4e00\u79cd\u53cc\u6a21\u578b\u67b6\u6784\uff08Astra-Global\u548cAstra-Local\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u95ee\u9898\uff0c\u7ed3\u5408\u4e86\u591a\u6a21\u6001LLM\u548c\u591a\u4efb\u52a1\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u7cfb\u7edf\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u5bf9\u65b0\u73af\u5883\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "Astra-Global\u901a\u8fc7\u591a\u6a21\u6001LLM\u5904\u7406\u89c6\u89c9\u548c\u8bed\u8a00\u8f93\u5165\uff0c\u4f7f\u7528\u6df7\u5408\u62d3\u6251-\u8bed\u4e49\u56fe\u4f5c\u4e3a\u5168\u5c40\u5730\u56fe\uff1bAstra-Local\u901a\u8fc74D\u65f6\u7a7a\u7f16\u7801\u5668\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u5904\u7406\u5c40\u90e8\u8def\u5f84\u89c4\u5212\u548c\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u3002", "result": "Astra\u5728\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "Astra\u901a\u8fc7\u53cc\u6a21\u578b\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2506.06208", "pdf": "https://arxiv.org/pdf/2506.06208", "abs": "https://arxiv.org/abs/2506.06208", "authors": ["Henry Watkins"], "title": "Building Models of Neurological Language", "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 6 figures", "summary": "This report documents the development and evaluation of domain-specific\nlanguage models for neurology. Initially focused on building a bespoke model,\nthe project adapted to rapid advances in open-source and commercial medical\nLLMs, shifting toward leveraging retrieval-augmented generation (RAG) and\nrepresentational models for secure, local deployment. Key contributions include\nthe creation of neurology-specific datasets (case reports, QA sets,\ntextbook-derived data), tools for multi-word expression extraction, and\ngraph-based analyses of medical terminology. The project also produced scripts\nand Docker containers for local hosting. Performance metrics and graph\ncommunity results are reported, with future possible work open for multimodal\nmodels using open-source architectures like phi-4.", "AI": {"tldr": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u9488\u5bf9\u795e\u7ecf\u75c5\u5b66\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\uff0c\u8f6c\u5411\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u8868\u5f81\u6a21\u578b\uff0c\u521b\u5efa\u4e86\u795e\u7ecf\u75c5\u5b66\u4e13\u7528\u6570\u636e\u96c6\u548c\u5de5\u5177\uff0c\u5e76\u63d0\u4f9b\u4e86\u672c\u5730\u90e8\u7f72\u7684\u811a\u672c\u548c\u5bb9\u5668\u3002", "motivation": "\u9002\u5e94\u5f00\u6e90\u548c\u5546\u4e1a\u533b\u5b66LLM\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5f00\u53d1\u9002\u7528\u4e8e\u795e\u7ecf\u75c5\u5b66\u7684\u5b89\u5168\u672c\u5730\u90e8\u7f72\u6a21\u578b\u3002", "method": "\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u8868\u5f81\u6a21\u578b\uff0c\u521b\u5efa\u795e\u7ecf\u75c5\u5b66\u4e13\u7528\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u591a\u8bcd\u8868\u8fbe\u63d0\u53d6\u5de5\u5177\u548c\u57fa\u4e8e\u56fe\u7684\u533b\u5b66\u672f\u8bed\u5206\u6790\u3002", "result": "\u751f\u6210\u4e86\u795e\u7ecf\u75c5\u5b66\u6570\u636e\u96c6\u3001\u5de5\u5177\u548c\u672c\u5730\u90e8\u7f72\u65b9\u6848\uff0c\u62a5\u544a\u4e86\u6027\u80fd\u6307\u6807\u548c\u56fe\u793e\u533a\u5206\u6790\u7ed3\u679c\u3002", "conclusion": "\u672a\u6765\u53ef\u63a2\u7d22\u57fa\u4e8e\u5f00\u6e90\u67b6\u6784\uff08\u5982phi-4\uff09\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002"}}
{"id": "2506.06211", "pdf": "https://arxiv.org/pdf/2506.06211", "abs": "https://arxiv.org/abs/2506.06211", "authors": ["Hengzhi Li", "Brendon Jiang", "Alexander Naehu", "Regan Song", "Justin Zhang", "Megan Tjandrasuwita", "Chanakya Ekbote", "Steven-Shine Chen", "Adithya Balachandran", "Wei Dai", "Rebecca Chang", "Paul Pu Liang"], "title": "PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Puzzlehunts are a genre of complex, multi-step puzzles lacking well-defined\nproblem definitions. In contrast to conventional reasoning benchmarks\nconsisting of tasks with clear instructions, puzzlehunts require models to\ndiscover the underlying problem structure from multimodal evidence and\niterative reasoning, mirroring real-world domains such as scientific discovery,\nexploratory data analysis, or investigative problem-solving. Despite recent\nprogress in foundation models, their performance on such open-ended settings\nremains largely untested. In this paper, we introduce PuzzleWorld, a\nlarge-scale benchmark of 667 puzzlehunt-style problems designed to assess\nstep-by-step, open-ended, and creative multimodal reasoning. Each puzzle is\nannotated with the final solution, detailed reasoning traces, and cognitive\nskill labels, enabling holistic benchmarking and fine-grained diagnostic\nanalysis. Most state-of-the-art models achieve only 1-2% final answer accuracy,\nwith the best model solving only 14% of puzzles and reaching 40% stepwise\naccuracy. To demonstrate the value of our reasoning annotations, we show that\nfine-tuning a small model on reasoning traces improves stepwise reasoning from\n4% to 11%, while training on final answers alone degrades performance to near\nzero. Our error analysis reveals that current models exhibit myopic reasoning,\nare bottlenecked by the limitations of language-based inference, and lack\nsketching capabilities crucial for visual and spatial reasoning. We release\nPuzzleWorld at https://github.com/MIT-MI/PuzzleWorld to support future work on\nbuilding more general, open-ended, and creative reasoning systems.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86PuzzleWorld\uff0c\u4e00\u4e2a\u5305\u542b667\u4e2a\u8c1c\u9898\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03\u63a8\u7406\u75d5\u8ff9\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u6d4b\u8bd5\u57fa\u7840\u6a21\u578b\u5728\u5f00\u653e\u6027\u95ee\u9898\uff08\u5982\u8c1c\u9898\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u63d0\u5347\u5176\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u521b\u5efaPuzzleWorld\u57fa\u51c6\uff0c\u6807\u6ce8\u8be6\u7ec6\u63a8\u7406\u75d5\u8ff9\u548c\u8ba4\u77e5\u6280\u80fd\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u5b9e\u9a8c\u9a8c\u8bc1\u63a8\u7406\u75d5\u8ff9\u7684\u4ef7\u503c\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u8c1c\u9898\u89e3\u51b3\u4e2d\u8868\u73b0\u8f83\u5dee\uff081-2%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5fae\u8c03\u63a8\u7406\u75d5\u8ff9\u53ef\u5c06\u6b65\u9aa4\u51c6\u786e\u7387\u4ece4%\u63d0\u5347\u81f311%\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u5f53\u524d\u6a21\u578b\u5b58\u5728\u77ed\u89c6\u63a8\u7406\u548c\u8bed\u8a00\u63a8\u7406\u5c40\u9650\u6027\uff0cPuzzleWorld\u7684\u53d1\u5e03\u65e8\u5728\u652f\u6301\u672a\u6765\u5f00\u653e\u6027\u548c\u521b\u9020\u6027\u63a8\u7406\u7cfb\u7edf\u7684\u7814\u7a76\u3002"}}
{"id": "2506.06214", "pdf": "https://arxiv.org/pdf/2506.06214", "abs": "https://arxiv.org/abs/2506.06214", "authors": ["Sirui Lu", "Zhijing Jin", "Terry Jingchen Zhang", "Pavel Kos", "J. Ignacio Cirac", "Bernhard Sch\u00f6lkopf"], "title": "Can Theoretical Physics Research Benefit from Language Agents?", "categories": ["cs.CL", "cs.AI", "math-ph", "math.MP", "quant-ph"], "comment": "9 pages", "summary": "Large Language Models (LLMs) are rapidly advancing across diverse domains,\nyet their application in theoretical physics research is not yet mature. This\nposition paper argues that LLM agents can potentially help accelerate\ntheoretical, computational, and applied physics when properly integrated with\ndomain knowledge and toolbox. We analyze current LLM capabilities for physics\n-- from mathematical reasoning to code generation -- identifying critical gaps\nin physical intuition, constraint satisfaction, and reliable reasoning. We\nenvision future physics-specialized LLMs that could handle multimodal data,\npropose testable hypotheses, and design experiments. Realizing this vision\nrequires addressing fundamental challenges: ensuring physical consistency, and\ndeveloping robust verification methods. We call for collaborative efforts\nbetween physics and AI communities to help advance scientific discovery in\nphysics.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7406\u8bba\u7269\u7406\u7814\u7a76\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\uff0c\u5206\u6790\u4e86\u5f53\u524d\u80fd\u529b\u4e0e\u6311\u6218\uff0c\u5e76\u547c\u5401\u7269\u7406\u4e0eAI\u793e\u533a\u5408\u4f5c\u63a8\u52a8\u79d1\u5b66\u53d1\u5c55\u3002", "motivation": "LLMs\u5728\u591a\u4e2a\u9886\u57df\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u5728\u7406\u8bba\u7269\u7406\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u6210\u719f\uff0c\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u5176\u6f5c\u529b\u4ee5\u52a0\u901f\u7269\u7406\u7814\u7a76\u3002", "method": "\u5206\u6790\u4e86LLMs\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u7b49\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u8bc6\u522b\u4e86\u7269\u7406\u76f4\u89c9\u3001\u7ea6\u675f\u6ee1\u8db3\u548c\u53ef\u9760\u63a8\u7406\u7b49\u5173\u952e\u5dee\u8ddd\u3002", "result": "\u63d0\u51fa\u4e86\u672a\u6765\u7269\u7406\u4e13\u7528LLMs\u7684\u613f\u666f\uff0c\u5305\u62ec\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\u3001\u63d0\u51fa\u53ef\u6d4b\u8bd5\u5047\u8bbe\u548c\u8bbe\u8ba1\u5b9e\u9a8c\u3002", "conclusion": "\u5b9e\u73b0\u8fd9\u4e00\u613f\u666f\u9700\u89e3\u51b3\u7269\u7406\u4e00\u81f4\u6027\u548c\u9a8c\u8bc1\u65b9\u6cd5\u7b49\u6311\u6218\uff0c\u547c\u5401\u7269\u7406\u4e0eAI\u793e\u533a\u5408\u4f5c\u63a8\u52a8\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2506.06220", "pdf": "https://arxiv.org/pdf/2506.06220", "abs": "https://arxiv.org/abs/2506.06220", "authors": ["Diji Yang", "Minghao Liu", "Chung-Hsiang Lo", "Yi Zhang", "James Davis"], "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) have shown strong performance on text-to-image\nretrieval benchmarks. However, bridging this success to real-world applications\nremains a challenge. In practice, human search behavior is rarely a one-shot\naction. Instead, it is often a multi-round process guided by clues in mind,\nthat is, a mental image ranging from vague recollections to vivid mental\nrepresentations of the target image. Motivated by this gap, we study the task\nof Mental Image Retrieval (MIR), which targets the realistic yet underexplored\nsetting where users refine their search for a mentally envisioned image through\nmulti-round interactions with an image search engine. Central to successful\ninteractive retrieval is the capability of machines to provide users with\nclear, actionable feedback; however, existing methods rely on indirect or\nabstract verbal feedback, which can be ambiguous, misleading, or ineffective\nfor users to refine the query. To overcome this, we propose GenIR, a generative\nmulti-round retrieval paradigm leveraging diffusion-based image generation to\nexplicitly reify the AI system's understanding at each round. These synthetic\nvisual representations provide clear, interpretable feedback, enabling users to\nrefine their queries intuitively and effectively. We further introduce a fully\nautomated pipeline to generate a high-quality multi-round MIR dataset.\nExperimental results demonstrate that GenIR significantly outperforms existing\ninteractive methods in the MIR scenario. This work establishes a new task with\na dataset and an effective generative retrieval method, providing a foundation\nfor future research in this direction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Mental Image Retrieval (MIR)\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u68c0\u7d22\u7528\u6237\u5fc3\u4e2d\u7684\u56fe\u50cf\uff0c\u5e76\u63d0\u51fa\u4e86GenIR\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u89c6\u89c9\u53cd\u9988\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u7528\u6237\u641c\u7d22\u884c\u4e3a\u662f\u591a\u8f6e\u4ea4\u4e92\u7684\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u95f4\u63a5\u6216\u62bd\u8c61\u7684\u53cd\u9988\u3002", "method": "\u63d0\u51faGenIR\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u8f6e\u68c0\u7d22\u8303\u5f0f\uff0c\u751f\u6210\u53ef\u89c6\u5316\u53cd\u9988\u4ee5\u5e2e\u52a9\u7528\u6237\u4f18\u5316\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGenIR\u5728\u591a\u8f6eMIR\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u4e3aMIR\u4efb\u52a1\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u751f\u6210\u5f0f\u68c0\u7d22\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.06225", "pdf": "https://arxiv.org/pdf/2506.06225", "abs": "https://arxiv.org/abs/2506.06225", "authors": ["Eunhye Grace Ko", "Shaini Nanayakkara", "Earl W. Huff Jr"], "title": "\"We need to avail ourselves of GenAI to enhance knowledge distribution\": Empowering Older Adults through GenAI Literacy", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As generative AI (GenAI) becomes increasingly widespread, it is crucial to\nequip users, particularly vulnerable populations such as older adults (65 and\nolder), with the knowledge to understand its benefits and potential risks.\nOlder adults often exhibit greater reservations about adopting emerging\ntechnologies and require tailored literacy support. Using a mixed methods\napproach, this study examines strategies for delivering GenAI literacy to older\nadults through a chatbot named Litti, evaluating its impact on their AI\nliteracy (knowledge, safety, and ethical use). The quantitative data indicated\na trend toward improved AI literacy, though the results were not statistically\nsignificant. However, qualitative interviews revealed diverse levels of\nfamiliarity with generative AI and a strong desire to learn more. Findings also\nshow that while Litti provided a positive learning experience, it did not\nsignificantly enhance participants' trust or sense of safety regarding GenAI.\nThis exploratory case study highlights the challenges and opportunities in\ndesigning AI literacy education for the rapidly growing older adult population.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u804a\u5929\u673a\u5668\u4ebaLitti\u5411\u8001\u5e74\u4eba\u63d0\u4f9b\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u6559\u80b2\uff0c\u53d1\u73b0\u5176\u5bf9AI\u7d20\u517b\u63d0\u5347\u6709\u8d8b\u52bf\u4f46\u672a\u8fbe\u663e\u8457\u6c34\u5e73\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8001\u5e74\u4eba\u5bf9GenAI\u7684\u5b66\u4e60\u9700\u6c42\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u666e\u53ca\uff0c\u8001\u5e74\u4eba\u7b49\u5f31\u52bf\u7fa4\u4f53\u9700\u8981\u7279\u522b\u652f\u6301\u4ee5\u7406\u89e3\u5176\u76ca\u5904\u548c\u98ce\u9669\uff0c\u4f46\u4ed6\u4eec\u5bf9\u65b0\u6280\u672f\u6301\u4fdd\u7559\u6001\u5ea6\uff0c\u9700\u5b9a\u5236\u5316\u6559\u80b2\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u804a\u5929\u673a\u5668\u4ebaLitti\u5411\u8001\u5e74\u4eba\u63d0\u4f9bGenAI\u6559\u80b2\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9AI\u7d20\u517b\uff08\u77e5\u8bc6\u3001\u5b89\u5168\u3001\u4f26\u7406\uff09\u7684\u5f71\u54cd\u3002", "result": "\u5b9a\u91cf\u6570\u636e\u663e\u793aAI\u7d20\u517b\u6709\u63d0\u5347\u8d8b\u52bf\u4f46\u4e0d\u663e\u8457\uff1b\u5b9a\u6027\u8bbf\u8c08\u663e\u793a\u8001\u5e74\u4eba\u5bf9GenAI\u7684\u719f\u6089\u5ea6\u5dee\u5f02\u5927\u4e14\u5b66\u4e60\u610f\u613f\u5f3a\uff0c\u4f46Litti\u672a\u663e\u8457\u63d0\u5347\u4fe1\u4efb\u6216\u5b89\u5168\u611f\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e3a\u8001\u5e74\u4eba\u8bbe\u8ba1AI\u7d20\u517b\u6559\u80b2\u7684\u6311\u6218\u4e0e\u673a\u9047\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6559\u80b2\u5de5\u5177\u3002"}}
{"id": "2506.06231", "pdf": "https://arxiv.org/pdf/2506.06231", "abs": "https://arxiv.org/abs/2506.06231", "authors": ["Mohammad Jalali", "Bahar Dibaei Nia", "Farzan Farnia"], "title": "Towards an Explainable Comparison and Alignment of Feature Embeddings", "categories": ["cs.LG", "cs.AI", "cs.CV", "math.SP"], "comment": null, "summary": "While several feature embedding models have been developed in the literature,\ncomparisons of these embeddings have largely focused on their numerical\nperformance in classification-related downstream applications. However, an\ninterpretable comparison of different embeddings requires identifying and\nanalyzing mismatches between sample groups clustered within the embedding\nspaces. In this work, we propose the \\emph{Spectral Pairwise Embedding\nComparison (SPEC)} framework to compare embeddings and identify their\ndifferences in clustering a reference dataset. Our approach examines the kernel\nmatrices derived from two embeddings and leverages the eigendecomposition of\nthe difference kernel matrix to detect sample clusters that are captured\ndifferently by the two embeddings. We present a scalable implementation of this\nkernel-based approach, with computational complexity that grows linearly with\nthe sample size. Furthermore, we introduce an optimization problem using this\nframework to align two embeddings, ensuring that clusters identified in one\nembedding are also captured in the other model. We provide numerical results\ndemonstrating the SPEC's application to compare and align embeddings on\nlarge-scale datasets such as ImageNet and MS-COCO. The code is available at\n[https://github.com/mjalali/embedding-comparison](github.com/mjalali/embedding-comparison).", "AI": {"tldr": "\u63d0\u51faSPEC\u6846\u67b6\uff0c\u901a\u8fc7\u6838\u77e9\u9635\u7279\u5f81\u5206\u89e3\u6bd4\u8f83\u5d4c\u5165\u6a21\u578b\uff0c\u8bc6\u522b\u805a\u7c7b\u5dee\u5f02\uff0c\u5e76\u63d0\u4f9b\u53ef\u6269\u5c55\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u5d4c\u5165\u6a21\u578b\u6bd4\u8f83\u591a\u5173\u6ce8\u6570\u503c\u6027\u80fd\uff0c\u7f3a\u4e4f\u5bf9\u805a\u7c7b\u5dee\u5f02\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "method": "\u5229\u7528\u6838\u77e9\u9635\u5dee\u5f02\u7684\u7279\u5f81\u5206\u89e3\u68c0\u6d4b\u6837\u672c\u805a\u7c7b\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u5d4c\u5165\u5bf9\u9f50\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5728ImageNet\u548cMS-COCO\u7b49\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86SPEC\u7684\u6709\u6548\u6027\u3002", "conclusion": "SPEC\u4e3a\u5d4c\u5165\u6a21\u578b\u6bd4\u8f83\u548c\u5bf9\u9f50\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06242", "pdf": "https://arxiv.org/pdf/2506.06242", "abs": "https://arxiv.org/abs/2506.06242", "authors": ["Zahra Babaiee", "Peyman M. Kiasari", "Daniela Rus", "Radu Grosu"], "title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in multimodal large language models have driven\nbreakthroughs in visual question answering. Yet, a critical gap persists,\n`conceptualization'-the ability to recognize and reason about the same concept\ndespite variations in visual form, a basic ability of human reasoning. To\naddress this challenge, we introduce the Visual Graph Arena (VGA), a dataset\nfeaturing six graph-based tasks designed to evaluate and improve AI systems'\ncapacity for visual abstraction. VGA uses diverse graph layouts (e.g.,\nKamada-Kawai vs. planar) to test reasoning independent of visual form.\nExperiments with state-of-the-art vision models and multimodal LLMs reveal a\nstriking divide: humans achieved near-perfect accuracy across tasks, while\nmodels totally failed on isomorphism detection and showed limited success in\npath/cycle tasks. We further identify behavioral anomalies suggesting\npseudo-intelligent pattern matching rather than genuine understanding. These\nfindings underscore fundamental limitations in current AI models for visual\nunderstanding. By isolating the challenge of representation-invariant\nreasoning, the VGA provides a framework to drive progress toward human-like\nconceptualization in AI visual models. The Visual Graph Arena is available at:\n\\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Visual Graph Arena\uff08VGA\uff09\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdbAI\u7cfb\u7edf\u5728\u89c6\u89c9\u62bd\u8c61\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u63ed\u793a\u5f53\u524dAI\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u4eba\u7c7b\u63a8\u7406\u4e2d\u7684\u2018\u6982\u5ff5\u5316\u2019\u80fd\u529b\uff0c\u5373\u8bc6\u522b\u548c\u63a8\u7406\u540c\u4e00\u6982\u5ff5\u7684\u80fd\u529b\u3002", "method": "VGA\u6570\u636e\u96c6\u5305\u542b\u516d\u79cd\u57fa\u4e8e\u56fe\u7684\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u56fe\u5e03\u5c40\uff08\u5982Kamada-Kawai\u4e0e\u5e73\u9762\u56fe\uff09\u6d4b\u8bd5\u6a21\u578b\u5bf9\u89c6\u89c9\u5f62\u5f0f\u65e0\u5173\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4eba\u7c7b\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\uff0c\u800cAI\u6a21\u578b\u5728\u56fe\u540c\u6784\u68c0\u6d4b\u4e2d\u5b8c\u5168\u5931\u8d25\uff0c\u5728\u8def\u5f84/\u5faa\u73af\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u8868\u660e\u5176\u4ec5\u4e3a\u4f2a\u667a\u80fd\u6a21\u5f0f\u5339\u914d\u3002", "conclusion": "VGA\u901a\u8fc7\u9694\u79bb\u8868\u793a\u4e0d\u53d8\u63a8\u7406\u7684\u6311\u6218\uff0c\u4e3aAI\u89c6\u89c9\u6a21\u578b\u5b9e\u73b0\u4eba\u7c7b\u6982\u5ff5\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2506.06266", "pdf": "https://arxiv.org/pdf/2506.06266", "abs": "https://arxiv.org/abs/2506.06266", "authors": ["Sabri Eyuboglu", "Ryan Ehrlich", "Simran Arora", "Neel Guha", "Dylan Zinsley", "Emily Liu", "Will Tennien", "Atri Rudra", "James Zou", "Azalia Mirhoseini", "Christopher Re"], "title": "Cartridges: Lightweight and general-purpose long context representations via self-study", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCartridge\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u8bad\u7ec3\u5c0f\u578bKV\u7f13\u5b58\u6765\u66ff\u4ee3\u4f20\u7edf\u7684\u5927\u89c4\u6a21\u4e0a\u4e0b\u6587\u7a97\u53e3\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5927\u89c4\u6a21\u6587\u672c\u8bed\u6599\u65f6\uff0c\u5185\u5b58\u6d88\u8017\u9ad8\u4e14\u6210\u672c\u6602\u8d35\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCartridge\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u5b66\u4e60\uff08self-study\uff09\u8bad\u7ec3\u5c0f\u578bKV\u7f13\u5b58\uff0c\u751f\u6210\u5408\u6210\u5bf9\u8bdd\u5e76\u4f7f\u7528\u4e0a\u4e0b\u6587\u84b8\u998f\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "Cartridge\u5728\u6027\u80fd\u4e0a\u4e0eICL\u76f8\u5f53\uff0c\u4f46\u5185\u5b58\u6d88\u8017\u51cf\u5c1138.6\u500d\uff0c\u541e\u5410\u91cf\u63d0\u9ad826.4\u500d\uff0c\u5e76\u6269\u5c55\u4e86\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "conclusion": "Cartridge\u65b9\u6cd5\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u7ec4\u5408\u4f7f\u7528\u3002"}}
{"id": "2506.06278", "pdf": "https://arxiv.org/pdf/2506.06278", "abs": "https://arxiv.org/abs/2506.06278", "authors": ["Bruce W. Lee", "Addie Foote", "Alex Infanger", "Leni Shor", "Harish Kamath", "Jacob Goldman-Wetzler", "Bryce Woodworth", "Alex Cloud", "Alexander Matt Turner"], "title": "Distillation Robustifies Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Current LLM unlearning methods are not robust: they can be reverted easily\nwith a few steps of finetuning. This is true even for the idealized unlearning\nmethod of training to imitate an oracle model that was never exposed to\nunwanted information, suggesting that output-based finetuning is insufficient\nto achieve robust unlearning. In a similar vein, we find that training a\nrandomly initialized student to imitate an unlearned model transfers desired\nbehaviors while leaving undesired capabilities behind. In other words,\ndistillation robustifies unlearning. Building on this insight, we propose\nUnlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an\nunlearned model into a partially noised copy of itself. UNDO introduces a\ntunable tradeoff between compute cost and robustness, establishing a new Pareto\nfrontier on synthetic language and arithmetic tasks. At its strongest setting,\nUNDO matches the robustness of a model retrained from scratch with perfect data\nfiltering while using only 60-80% of the compute and requiring only 0.01% of\nthe pretraining data to be labeled. We also show that UNDO robustifies\nunlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP)\nbenchmark. Since distillation is widely used in practice, incorporating an\nunlearning step beforehand offers a convenient path to robust capability\nremoval.", "AI": {"tldr": "\u5f53\u524dLLM\u7684\u9057\u5fd8\u65b9\u6cd5\u4e0d\u591f\u9c81\u68d2\uff0c\u5bb9\u6613\u88ab\u5fae\u8c03\u6062\u590d\u3002\u63d0\u51faUNDO\u65b9\u6cd5\uff0c\u901a\u8fc7\u84b8\u998f\u589e\u5f3a\u9057\u5fd8\u9c81\u68d2\u6027\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u6613\u88ab\u5fae\u8c03\u6062\u590d\uff0c\u8f93\u51fa\u5fae\u8c03\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u9c81\u68d2\u9057\u5fd8\u3002", "method": "\u63d0\u51faUNDO\u65b9\u6cd5\uff0c\u901a\u8fc7\u84b8\u998f\u672a\u5b66\u4e60\u6a21\u578b\u5230\u90e8\u5206\u566a\u58f0\u7248\u672c\uff0c\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u4e0e\u9c81\u68d2\u6027\u3002", "result": "UNDO\u5728\u5408\u6210\u4efb\u52a1\u4e2d\u5339\u914d\u4ece\u5934\u8bad\u7ec3\u7684\u9c81\u68d2\u6027\uff0c\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\uff1b\u5728WMDP\u57fa\u51c6\u4e0a\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "UNDO\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u9c81\u68d2\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.06280", "pdf": "https://arxiv.org/pdf/2506.06280", "abs": "https://arxiv.org/abs/2506.06280", "authors": ["Yuanzhe Hu", "Kinshuk Goel", "Vlad Killiakov", "Yaoqing Yang"], "title": "Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias", "categories": ["cs.LG", "cs.AI"], "comment": "30 pages, 14 figures, published to ICML 2025", "summary": "Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight\nmatrices has been an active area of research in recent years. At a high level,\neigenspectrum analysis of DNNs involves measuring the heavytailness of the\nempirical spectral densities (ESD) of weight matrices. It provides insight into\nhow well a model is trained and can guide decisions on assigning better\nlayer-wise training hyperparameters. In this paper, we address a challenge\nassociated with such eigenspectrum methods: the impact of the aspect ratio of\nweight matrices on estimated heavytailness metrics. We demonstrate that\nmatrices of varying sizes (and aspect ratios) introduce a non-negligible bias\nin estimating heavytailness metrics, leading to inaccurate model diagnosis and\nlayer-wise hyperparameter assignment. To overcome this challenge, we propose\nFARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the\nweight matrices by subsampling submatrices with a fixed aspect ratio. Instead\nof measuring the heavytailness of the original ESD, we measure the average ESD\nof these subsampled submatrices. We show that measuring the heavytailness of\nthese submatrices with the fixed aspect ratio can effectively mitigate the\naspect ratio bias. We validate our approach across various optimization\ntechniques and application domains that involve eigenspectrum analysis of\nweights, including image classification in computer vision (CV) models,\nscientific machine learning (SciML) model training, and large language model\n(LLM) pruning. Our results show that despite its simplicity, FARMS uniformly\nimproves the accuracy of eigenspectrum analysis while enabling more effective\nlayer-wise hyperparameter assignment in these application domains. In one of\nthe LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model\nby 17.3% when compared with the state-of-the-art method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFARMS\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fa\u5b9a\u957f\u5bbd\u6bd4\u7684\u5b50\u77e9\u9635\u91c7\u6837\uff0c\u89e3\u51b3\u6743\u91cd\u77e9\u9635\u957f\u5bbd\u6bd4\u5bf9\u7279\u5f81\u8c31\u5206\u6790\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u6a21\u578b\u8bca\u65ad\u548c\u8d85\u53c2\u6570\u5206\u914d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u77e9\u9635\u7279\u5f81\u8c31\u5206\u6790\u4e2d\uff0c\u77e9\u9635\u957f\u5bbd\u6bd4\u5bf9\u91cd\u5c3e\u6027\u5ea6\u91cf\u7684\u5f71\u54cd\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u8bad\u7ec3\u548c\u8d85\u53c2\u6570\u5206\u914d\u3002", "method": "\u63d0\u51faFARMS\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fa\u5b9a\u957f\u5bbd\u6bd4\u7684\u5b50\u77e9\u9635\u91c7\u6837\uff0c\u8ba1\u7b97\u5e73\u5747\u7279\u5f81\u8c31\u5bc6\u5ea6\uff0c\u51cf\u5c11\u957f\u5bbd\u6bd4\u5e26\u6765\u7684\u504f\u5dee\u3002", "result": "\u5728\u591a\u4e2a\u5e94\u7528\u9886\u57df\u9a8c\u8bc1FARMS\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u56fe\u50cf\u5206\u7c7b\u3001\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4fee\u526a\uff0c\u663e\u8457\u63d0\u5347\u5206\u6790\u51c6\u786e\u6027\u3002", "conclusion": "FARMS\u7b80\u5355\u6709\u6548\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u7279\u5f81\u8c31\u5206\u6790\u4e2d\u7684\u957f\u5bbd\u6bd4\u504f\u5dee\uff0c\u63d0\u5347\u6a21\u578b\u8bca\u65ad\u548c\u8d85\u53c2\u6570\u5206\u914d\u7684\u6548\u679c\u3002"}}
